{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"NWM Coastal: Coastal Model Workflow","text":"<p><code>nwm-coastal</code> is a collection of tools designed to implement skill assessments for coastal model runs and execute National Water Model (NWM) hindcast/forecast runs for standalone coastal models utilized in NWMv4 operations. This includes the Semi-implicit Cross-scale Hydroscience Integrated System Model (SCHISM) and Super-Fast INundation of CoastS (SFINCS) as the intended coastal models for implementation in NWMv4 operations. This repository stores symlinks to both coastal modeling development groups, which are used to compile and run the NWMv4 coastal models. The coastal modeling output for NWMv4 operations is the Total Water Level (TWL) fields. These fields are the focus of the coastal tools in this repository for skill assessments across NWM domains, as well as the end result to retrieve from tools in this repository that set up and execute NWMv4 hindcast and operational forecast runs.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Multi-Model Support: SCHISM (multi-node MPI) and SFINCS (single-node OpenMP) via a     polymorphic <code>ModelConfig</code> architecture</li> <li>YAML Configuration: Simple, human-readable configuration files with variable     interpolation</li> <li>Data Download: Automated download of NWM and STOFS boundary data</li> <li>Multiple Domains: Support for Hawaii, Puerto Rico/Virgin Islands, Atlantic/Gulf,     and Pacific</li> <li>Boundary Conditions: TPXO tidal model and STOFS water level support</li> <li>NOAA Observation Stations: Automatic discovery of CO-OPS water level stations     within the model domain, with post-run comparison plots (simulated vs observed)</li> <li>SFINCS Model Creation: Build SFINCS quadtree models from an AOI polygon with     automatic NOAA DEM discovery, elevation/bathymetry, boundary cells, and subgrid     tables</li> <li>Workflow Control: <code>run</code> pipeline with <code>--start-from</code> / <code>--stop-after</code> support for     partial workflows</li> <li>Configuration Inheritance: Share common settings across multiple runs</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<p>On clusters, the recommended approach is to use a heredoc <code>sbatch</code> script:</p> <pre><code>#!/usr/bin/env bash\n#SBATCH --job-name=coastal_schism\n#SBATCH --partition=c5n-18xlarge\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=18\n#SBATCH --exclusive\n#SBATCH --output=slurm-%j.out\n\nCONFIG_FILE=\"/tmp/coastal_config_${SLURM_JOB_ID}.yaml\"\n\ncat &gt; \"${CONFIG_FILE}\" &lt;&lt;'EOF'\nsimulation:\n  start_date: 2021-06-11\n  duration_hours: 24\n  coastal_domain: hawaii\n  meteo_source: nwm_ana\n\nboundary:\n  source: stofs\nEOF\n\ncoastal-calibration run \"${CONFIG_FILE}\"\nrm -f \"${CONFIG_FILE}\"\n</code></pre> <p>Submit with <code>sbatch my_run.sh</code>. The full NFS path ensures the command is found on compute nodes. See the Quick Start for more options.</p>"},{"location":"#supported-models","title":"Supported Models","text":"Model Status Description SCHISM Supported Semi-implicit Cross-scale Hydroscience Integrated System Model SFINCS Supported Super-Fast INundation of CoastS"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install coastal-calibration\n</code></pre> <p>See the Installation Guide for detailed instructions.</p>"},{"location":"hydromt_sfincs_issues/","title":"<code>hydromt-sfincs</code> Bug Reports","text":"<p>Tested against <code>hydromt-sfincs</code> commit <code>41aac0a</code>. All issues were discovered while integrating <code>hydromt-sfincs</code> into an automated coastal calibration pipeline that builds SFINCS models from NWM and STOFS/TPXO forcing data.</p>"},{"location":"hydromt_sfincs_issues/#1-gridded-meteo-reprojection-produces-conus-extent-output-critical-performance-bug","title":"1. Gridded meteo reprojection produces CONUS-extent output (critical performance bug)","text":"<p>Summary:</p> <p><code>SfincsPrecipitation.create()</code>, <code>SfincsWind.create()</code>, and <code>SfincsPressure.create()</code> produce output grids that can be orders of magnitude larger than the model domain when the source data uses a different CRS than the model.</p> <p>Root cause:</p> <p>Each <code>create()</code> method follows a two-step pattern (<code>meteo.py</code>, precipitation example at lines 351-413):</p> <pre><code># Step 1 \u2014 clip in source CRS (correct)\nprecip = self.data_catalog.get_rasterdataset(\n    precip,\n    bbox=self.model.bbox,   # model bbox in geographic coords\n    buffer=buffer,           # 5 km default\n    ...\n)\n\n# Step 2 \u2014 reproject to model CRS (no post-clip)\nprecip_out = precip.raster.reproject(\n    dst_crs=self.model.crs, dst_res=dst_res, **kwargs\n).fillna(0)\n</code></pre> <p>Step 1 correctly extracts a small rectangle in the source CRS (e.g. Lambert Conformal Conic for NWM LDASIN). Step 2 calls <code>rioxarray.reproject()</code>, which computes the output extent by transforming the corners of the input rectangle into the target CRS. For projections with significant non-linearity (LCC \u2192 UTM), a small source-CRS rectangle maps to an enormously inflated bounding box in the target CRS. <code>reproject()</code> faithfully allocates the full output grid, filling unreachable cells with NaN (subsequently replaced by <code>.fillna(0)</code>).</p> <p>There is no clip to the model domain after the reprojection.</p> <p>Observed impact:</p> <p>For a SFINCS model on the Texas Gulf Coast (UTM zone 14N, domain ~143 km x 92 km) with NWM LDASIN forcing (LCC, ~1 km):</p> File Expected size Actual size Grid dimensions <code>sfincs_netampr.nc</code> (precip) ~11 MB 12 GB 4797 x 4085 <code>sfincs_netamuv.nc</code> (wind) ~45 MB 49 GB 4797 x 4085 <code>sfincs_netamp.nc</code> (pressure) ~23 MB 24 GB 4797 x 4085 <p>The output grid covered x = -1,793 km to 3,118 km and y = 2,339 km to 6,581 km in UTM zone 14N \u2014 essentially the entire CONUS. The SFINCS simulation, which must read these files every time step, slowed from ~7 minutes to an estimated 15+ hours.</p> <p>Suggested fix:</p> <p>Add a <code>rio.clip_box</code> call after <code>reproject()</code> in each <code>create()</code> method, clipping to <code>self.model.region.total_bounds</code> with a buffer:</p> <pre><code>precip_out = precip.raster.reproject(dst_crs=self.model.crs, dst_res=dst_res, **kwargs).fillna(0)\n\n# --- add this ---\nregion = self.model.region.total_bounds  # (xmin, ymin, xmax, ymax)\nbuf = buffer  # reuse the same buffer parameter\nprecip_out = precip_out.rio.clip_box(\n    minx=region[0] - buf,\n    miny=region[1] - buf,\n    maxx=region[2] + buf,\n    maxy=region[3] + buf,\n)\n</code></pre> <p>This applies identically to <code>SfincsPrecipitation.create()</code> (line 411), <code>SfincsWind.create()</code> (line 684), and <code>SfincsPressure.create()</code> (line 581).</p> <p>Current workaround:</p> <p>In our pipeline we (a) pass an explicit <code>dst_res</code> derived from the SFINCS quadtree grid base cell size (via <code>_meteo_dst_res()</code>), and (b) clip each meteo component after <code>create()</code> and before <code>write()</code> using <code>_clip_meteo_to_domain()</code> (in <code>sfincs_build.py</code>). This is applied identically to precipitation, wind, and pressure stages. The <code>meteo_res</code> config option allows users to override the automatic resolution.</p>"},{"location":"hydromt_sfincs_issues/#2-write_gridded-loads-entire-dataset-into-memory-oom-on-large-grids","title":"2. <code>write_gridded</code> loads entire dataset into memory (OOM on large grids)","text":"<p>Summary:</p> <p><code>SfincsMeteo.write_gridded()</code> (<code>meteo.py</code>, line 180) calls <code>self.data.load()</code>, which materializes the full lazy <code>dask</code>-backed dataset into RAM. Combined with issue 1 (CONUS-extent grids), this easily exceeds available memory. Even with properly clipped grids, a 7-day NWM setup with precip + wind + pressure can require ~10-15 GB of contiguous memory.</p> <p>Root cause:</p> <pre><code>def write_gridded(self, filename=None, rename=None):\n    ...\n    ds = self.data.load()  # &lt;-- full materialization\n    ...\n    ds.to_netcdf(filename, ...)\n</code></pre> <p>Suggested fix:</p> <p>Stream one time step at a time via the netCDF4 library, keeping peak memory near ~150 MB regardless of the total dataset size. See the <code>_write_gridded_lazy</code> implementation in our monkey-patch for a working reference (writes each <code>ds.isel(time=i).compute()</code> to a pre-created netCDF4 file with an unlimited time dimension).</p> <p>Current workaround:</p> <p>We monkey-patch <code>SfincsMeteo.write_gridded</code> at import time (via <code>patch_meteo_write_gridded()</code> in <code>_hydromt_compat.py</code>, called from <code>apply_all_patches()</code>) to replace it with a streaming writer that writes one time-step at a time via <code>netCDF4.Dataset</code>, keeping peak memory near ~150 MB instead of materializing the full 3-D array.</p>"},{"location":"hydromt_sfincs_issues/#3-write_netcdf_safely-can-crash-the-hdf5-c-library-on-incompatible-files","title":"3. <code>write_netcdf_safely</code> can crash the HDF5 C library on incompatible files","text":"<p>Summary:</p> <p><code>write_netcdf_safely()</code> (<code>utils.py</code>, line 1921) opens an existing netCDF file to check whether the data changed before overwriting. If the existing file has an incompatible schema (e.g. different number of stations, different dimension sizes), the HDF5 library can segfault during the comparison read.</p> <p>Root cause:</p> <pre><code>def write_netcdf_safely(ds, abs_file_path, encoding=None):\n    ds = ds.load()\n    if abs_file_path.exists():\n        try:\n            existing_ds = GeoDataset.from_netcdf(  # &lt;-- crash here\n                abs_file_path, crs=ds.raster.crs, chunks=\"auto\"\n            )\n            changed = not ds.equals(existing_ds)\n            existing_ds.close()\n        except Exception:\n            changed = True  # fail-safe\n</code></pre> <p>The <code>except Exception</code> on line 1952 is intended as a fail-safe, but the crash occurs inside the HDF5 C library (<code>H5VL__native_blob_specific</code> \u2192 <code>H5T__vlen_disk_isnull</code>), which raises <code>SIGTERM</code>/<code>SIGSEGV</code> rather than a Python exception. The process is killed before the except clause can execute.</p>"},{"location":"hydromt_sfincs_issues/#reproduction","title":"Reproduction","text":"<ol> <li>Run a SFINCS pipeline that writes <code>sfincs_netbndbzsbzifile.nc</code> with 119,903 stations     (e.g. from un-interpolated STOFS data).</li> <li>Fix the pipeline to produce the correct 15 stations.</li> <li>Re-run. <code>write_netcdf_safely</code> opens the old 119,903-station file to compare with the     new 15-station dataset. The HDF5 library segfaults (exit code 139) during     <code>GeoDataset.from_netcdf()</code>.</li> </ol> <p>Suggested fix:</p> <p>The comparison-before-write optimization is fragile when files can have entirely different schemas across runs. Options:</p> <ul> <li>Option A (simple): Delete the existing file before writing instead of comparing.     The comparison only saves a write when the data is identical, which is uncommon in a     re-run scenario.</li> <li>Option B (defensive): Compare only metadata (dimensions, shapes, dtypes) before     attempting a full <code>ds.equals()</code>. Skip the comparison entirely if schemas differ.</li> </ul> <p>Current workaround:</p> <p><code>SfincsInitStage._remove_stale_outputs()</code> deletes all generated netCDF files (forcing, output, boundary) at the start of every pipeline run, so <code>write_netcdf_safely</code> never encounters a stale file with an incompatible schema.</p>"},{"location":"hydromt_sfincs_issues/#4-_validate_and_prepare_gdf-hard-codes-index-dimension-name","title":"4. <code>_validate_and_prepare_gdf</code> hard-codes <code>\"index\"</code> dimension name","text":"<p>Summary:</p> <p><code>BoundaryConditionComponent._create_dummy_dataset</code> uses <code>dims=(\"time\", \"index\")</code>, but <code>GeoDataset.from_gdf</code> derives the spatial dimension name from <code>gdf.index.name</code>. When the input GeoDataFrame's index is named something other than <code>\"index\"</code> (e.g. <code>\"node\"</code> from ADCIRC/STOFS data), the dimension names diverge and <code>from_gdf</code> raises:</p> <pre><code>ValueError: Index dimension node not found in data_vars\n</code></pre> <p>Root cause:</p> <p><code>_validate_and_prepare_gdf</code> does not normalize the GDF index name to <code>\"index\"</code> before it is used downstream.</p> <p>Suggested fix:</p> <p>At the end of <code>_validate_and_prepare_gdf</code>, ensure the index name is always <code>\"index\"</code>:</p> <pre><code>def _validate_and_prepare_gdf(self, gdf):\n    ...  # existing validation\n    if gdf.index.name != \"index\":\n        gdf.index.name = \"index\"\n    return gdf\n</code></pre> <p>Current workaround:</p> <p><code>patch_boundary_conditions_index_dim()</code> in <code>_hydromt_compat.py</code> (called from <code>apply_all_patches()</code>) monkey-patches <code>SfincsBoundaryBase._validate_and_prepare_gdf</code> to normalize the GDF index name to <code>\"index\"</code> after validation.</p>"},{"location":"hydromt_sfincs_issues/#5-_serialize_crs-crashes-on-crs-without-an-authority-code","title":"5. <code>_serialize_crs</code> crashes on CRS without an authority code","text":"<p>Summary:</p> <p><code>hydromt.typing.crs._serialize_crs</code> calls <code>list(crs.to_authority())</code> without guarding against <code>to_authority()</code> returning <code>None</code>. This raises <code>TypeError: 'NoneType' object is not iterable</code> for any CRS that has no EPSG code and no recognized authority (e.g. custom <code>proj</code> strings).</p> <p>Suggested fix:</p> <pre><code>def _serialize_crs(crs):\n    epsg = crs.to_epsg()\n    if epsg:\n        return epsg\n    auth = crs.to_authority()\n    if auth is not None:\n        return list(auth)\n    return crs.to_wkt()\n</code></pre> <p>Note: this is in <code>hydromt</code> core, not <code>hydromt-sfincs</code>.</p> <p>Current workaround:</p> <p><code>patch_serialize_crs()</code> in <code>_hydromt_compat.py</code> (called from <code>apply_all_patches()</code>) swaps the function's <code>__code__</code> in-place at import time so that existing <code>Pydantic</code> <code>PlainSerializer</code> references pick up the fix.</p>"},{"location":"hydromt_sfincs_issues/#6-nwm-ldasin-coordinate-rounding-errors-rejected-as-irregular-grid","title":"6. NWM LDASIN coordinate rounding errors rejected as irregular grid","text":"<p>Summary:</p> <p>NWM LDASIN files store projected coordinates (LCC, in meters) with floating-point rounding errors up to ~0.125 m. <code>hydromt</code>'s raster accessor rejects them with:</p> <pre><code>ValueError: not a regular grid\n</code></pre> <p>because its internal tolerance (<code>atol=5e-4</code>) is too tight for meter-scale coordinates.</p> <p>Suggested fix:</p> <p>Either increase the tolerance or provide a built-in <code>round_coords</code> preprocessor that users can reference in their data catalog YAML:</p> <pre><code>nwm_ana_meteo:\n  ...\n  driver_kwargs:\n    preprocess: round_coords\n</code></pre> <p>Note: this is in <code>hydromt</code> core.</p> <p>Current workaround:</p> <p><code>register_round_coords_preprocessor()</code> in <code>_hydromt_compat.py</code> (called from <code>apply_all_patches()</code>) registers a custom <code>round_coords</code> preprocessor in <code>hydromt</code>'s <code>PREPROCESSORS</code> dictionary that rounds x/y coordinates to the nearest integer before the regularity check.</p>"},{"location":"hydromt_sfincs_issues/#7-water_levelcreategeodataset-passes-all-source-stations-to-netcdf-output","title":"7. <code>water_level.create(geodataset=...)</code> passes all source stations to netCDF output","text":"<p>Summary:</p> <p><code>SfincsWaterLevel.create(geodataset=...)</code> passes all source stations found within the model region to the boundary netCDF file. When the model has a <code>.bnd</code> file that defines explicit boundary points (the normal SFINCS setup), the station count in the netCDF must exactly match the boundary point count. The mismatch causes SFINCS to either crash or silently produce incorrect boundary forcing.</p> <p>Root cause:</p> <p>The <code>create()</code> code path loads the <code>geodataset</code>, clips it to the model region, and writes all matching stations to <code>sfincs_netbndbzsbzifile.nc</code> without checking whether a <code>.bnd</code> file exists or how many boundary points it defines. For a STOFS <code>geodataset</code> over the Texas Gulf Coast domain, this can yield ~119,903 source stations, far more than the 15 boundary points in <code>sfincs.bnd</code>.</p> <p>Suggested fix:</p> <p>When a <code>.bnd</code> file exists, <code>water_level.create()</code> should spatially interpolate the <code>geodataset</code> to the boundary point locations (e.g. via IDW from the nearest source nodes) and write only those interpolated time-series, matching the <code>.bnd</code> point count.</p> <p>Current workaround:</p> <p><code>SfincsForcingStage._create_geodataset_forcing()</code> in <code>sfincs_build.py</code> bypasses <code>model.water_level.create(geodataset=...)</code> entirely. It reads the <code>.bnd</code> file, loads the <code>geodataset</code>, spatially interpolates to each boundary point using inverse-distance weighting (IDW) from the nearest source nodes, then injects the result via <code>model.water_level.set(df=..., gdf=...)</code>.</p>"},{"location":"hydromt_sfincs_issues/#8-missing-bzi-infragravity-variable-in-boundary-netcdf","title":"8. Missing <code>bzi</code> (infragravity) variable in boundary netCDF","text":"<p>Summary:</p> <p>SFINCS unconditionally reads a <code>zi</code> (infragravity water level) variable from the boundary netCDF file (<code>sfincs_ncinput.F90:118</code>). When <code>hydromt-sfincs</code> writes <code>sfincs_netbndbzsbzifile.nc</code> it only writes <code>bzs</code> (surface water level), omitting <code>bzi</code>. SFINCS crashes with a netCDF read error on startup.</p> <p>Root cause:</p> <p>The boundary writer in <code>hydromt-sfincs</code> does not generate the <code>bzi</code> variable. SFINCS's Fortran I/O code does not check whether <code>zi</code> exists before attempting to read it, so the missing variable causes a hard crash rather than a graceful fallback to zero.</p> <p>Suggested fix:</p> <p><code>hydromt-sfincs</code> should always write a <code>bzi</code> variable alongside <code>bzs</code>. If no infragravity data is available, it should be zero-filled with the same shape as <code>bzs</code>.</p> <p>Current workaround:</p> <p><code>SfincsForcingStage._inject_water_level()</code> in <code>sfincs_build.py</code> manually adds a zero-filled <code>bzi</code> variable after calling <code>model.water_level.set()</code>:</p> <pre><code>ds = model.water_level.data\nif \"bzi\" not in ds.data_vars:\n    ds[\"bzi\"] = xr.zeros_like(ds[\"bzs\"])\n</code></pre>"},{"location":"hydromt_sfincs_issues/#9-discharge-source-points-on-inactive-grid-cells-cause-segfault","title":"9. Discharge source points on inactive grid cells cause segfault","text":"<p>Summary:</p> <p>When a discharge source point (from the <code>.src</code> file) falls on an inactive grid cell (<code>mask != 1</code>), SFINCS segfaults during the simulation. This is a Fortran-side bug but <code>hydromt-sfincs</code> should validate source locations against the grid mask before writing.</p> <p>Root cause:</p> <p>The SFINCS Fortran binary maps each source point to the nearest grid cell. If that cell is inactive, the cell index is left at its default value of 0. The code later accesses <code>zs(0)</code> (water level at cell 0) without a bounds check, causing a segfault (exit code 139) or out-of-bounds array access.</p> <p>Suggested fix:</p> <p><code>hydromt-sfincs</code> should validate discharge source points against the grid mask when they are added and either:</p> <ul> <li>Option A (strict): Raise an error listing the offending points.</li> <li>Option B (lenient): Drop inactive-cell points with a warning, keeping only those     on active cells.</li> </ul> <p>Current workaround:</p> <p><code>SfincsDischargeStage._filter_active_cells()</code> in <code>sfincs_build.py</code> uses <code>scipy.spatial.cKDTree</code> to map each source point to its nearest quadtree face, checks the face mask value, and drops points on inactive cells. The names of dropped points are logged as a warning.</p>"},{"location":"hydromt_sfincs_issues/#10-_parse_river_list-crashes-on-freshly-created-models-keyerror-geoms","title":"10. <code>_parse_river_list</code> crashes on freshly created models (<code>KeyError: 'geoms'</code>)","text":"<p>Summary:</p> <p><code>SfincsModel._parse_river_list()</code> crashes with <code>KeyError: 'geoms'</code> when called on a model created in write mode (<code>mode=\"w+\"</code>). This makes it impossible to use river burning (<code>burn_river_rect</code>) during subgrid table creation on new models \u2014 i.e. whenever <code>subgrid.create(river_list=...)</code> is called as part of a model-building workflow rather than on a model read from disk.</p> <p>Root cause:</p> <p><code>_parse_river_list</code> (<code>sfincs.py</code>, line 744) checks whether a river centerline dataset name already exists as a loaded model geometry:</p> <pre><code>if isinstance(rivers, str) and rivers in self.geoms:\n    gdf_riv = self.geoms[rivers].copy()\nelse:\n    gdf_riv = self.data_catalog.get_geodataframe(rivers, ...)\n</code></pre> <p>In <code>hydromt</code> v1+, <code>self.geoms</code> is resolved via <code>Model.__getattr__</code> \u2192 <code>Model.get_component</code>, which looks up <code>\"geoms\"</code> in the component registry (<code>self.components[\"geoms\"]</code>). On freshly created models the <code>geoms</code> component is never registered, so the lookup raises:</p> <pre><code>KeyError: \"geoms\"\n</code></pre> <p>Python's <code>and</code> operator should short-circuit, but the <code>KeyError</code> is raised by the attribute access itself (<code>self.geoms</code>), not by the <code>in</code> operator. The exception propagates before the <code>in</code> check can execute, and the <code>else</code> branch (<code>data_catalog.get_geodataframe</code>) \u2014 which is the correct code path for new models \u2014 is never reached.</p>"},{"location":"hydromt_sfincs_issues/#reproduction_1","title":"Reproduction","text":"<pre><code>from hydromt_sfincs import SfincsModel\n\nsf = SfincsModel(root=\"./test_model\", mode=\"w+\", data_libs=[\"my_catalog.yml\"])\n\n# ... create grid, elevation, mask, roughness ...\n\nsf.subgrid.create(\n    elevation_list=[{\"elevation\": \"dem\"}],\n    roughness_list=[{\"lulc\": \"esa_worldcover\"}],\n    river_list=[{\"centerlines\": \"my_rivers\"}],  # &lt;-- KeyError: 'geoms'\n    nr_subgrid_pixels=5,\n)\n</code></pre> <p>Suggested fix:</p> <p>Guard the <code>self.geoms</code> access in <code>_parse_river_list</code> so that a missing component is treated the same as an empty one:</p> <pre><code>try:\n    model_geoms = self.geoms\nexcept KeyError:\n    model_geoms = {}\n\nif isinstance(rivers, str) and rivers in model_geoms:\n    gdf_riv = model_geoms[rivers].copy()\nelse:\n    gdf_riv = self.data_catalog.get_geodataframe(rivers, ...)\n</code></pre> <p>Current workaround:</p> <p><code>patch_parse_river_list_geoms()</code> in <code>_hydromt_compat.py</code> (called from <code>apply_all_patches()</code>) wraps <code>_parse_river_list</code> to temporarily inject an empty dict into the model instance's <code>__dict__</code> under the key <code>\"geoms\"</code> when the component is missing. Python's normal attribute lookup finds the instance dict entry before falling through to <code>__getattr__</code>, so <code>self.geoms</code> resolves to <code>{}</code> and the <code>in</code> check evaluates to <code>False</code>.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.11</li> <li>Access to an HPC cluster with Singularity</li> <li>NFS mount point (default: <code>/ngen-test</code>)</li> <li>Singularity image with SCHISM and/or SFINCS and dependencies pre-compiled</li> </ul> <p>Model Executables</p> <p>This package orchestrates SCHISM and SFINCS workflows on HPC clusters where the models are already compiled and available (typically inside a Singularity container). You do not need to install SCHISM or SFINCS locally to use this package.</p>"},{"location":"getting-started/installation/#install-from-pypi","title":"Install from PyPI","text":"<pre><code>pip install coastal-calibration\n</code></pre> <p>This installs the core package with CLI and workflow orchestration capabilities.</p>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<p>For development or to get the latest features, install from source:</p> <pre><code>git clone https://github.com/NGWPC/nwm-coastal\ncd nwm-coastal\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#development-installation-with-pixi","title":"Development Installation with Pixi","text":"<p>For development, we recommend using Pixi for environment management:</p> <pre><code># Install Pixi (Linux/macOS)\ncurl -fsSL https://pixi.sh/install.sh | sh\n</code></pre> <p>Restart Terminal</p> <p>After installing Pixi, restart your terminal or run <code>source ~/.bashrc</code> (or <code>source ~/.zshrc</code> for Zsh) to make the <code>pixi</code> command available.</p> <pre><code># Clone and install\ngit clone https://github.com/NGWPC/nwm-coastal\ncd nwm-coastal\npixi install -e dev\n</code></pre>"},{"location":"getting-started/installation/#available-environments","title":"Available Environments","text":"Environment Description Command <code>dev</code> Development with all tools <code>pixi r -e dev &lt;cmd&gt;</code> <code>test311</code> Testing with Python 3.11 <code>pixi r -e test311 test</code> <code>test314</code> Testing with Python 3.14 <code>pixi r -e test314 test</code> <code>schism</code> Local development with SCHISM I/O libraries <code>pixi r -e schism &lt;cmd&gt;</code> <code>sfincs</code> Local development with HydroMT-SFINCS <code>pixi r -e sfincs &lt;cmd&gt;</code> <code>typecheck</code> Type checking with ty <code>pixi r -e typecheck typecheck</code> <code>lint</code> Linting with pre-commit <code>pixi r lint</code> <code>docs</code> Documentation building <code>pixi r -e docs docs-serve</code>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Optional dependencies are available for local development purposes only. They are useful for:</p> <ul> <li>Reading and analyzing model output files</li> <li>Debugging and testing workflow components locally</li> <li>Building SFINCS models with HydroMT</li> </ul> <p>Not Required for Cluster Execution</p> <p>These optional dependencies are not required to submit and run jobs on the cluster. The actual SCHISM and SFINCS executables must be pre-compiled and available on the HPC cluster (inside the Singularity container).</p> <pre><code># SCHISM I/O dependencies (netCDF, numpy, etc.) - for local development\npip install coastal-calibration[schism]\n\n# SFINCS/HydroMT dependencies - for local model building and analysis\npip install coastal-calibration[sfincs]\n\n# Development dependencies (Jupyter, etc.)\npip install coastal-calibration[dev]\n\n# Documentation dependencies\npip install coastal-calibration[docs]\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>After installation, verify by running:</p> <pre><code>coastal-calibration --help\n</code></pre> <p>You should see the CLI help output with available commands:</p> <pre><code>Usage: coastal-calibration [OPTIONS] COMMAND [ARGS]...\n\n  Coastal calibration workflow manager (SCHISM, SFINCS).\n\nCommands:\n  create             Create a SFINCS model from an AOI polygon.\n  init               Create a minimal configuration file.\n  prepare-topobathy  Download NWS topobathy DEM clipped to an AOI bounding box.\n  run                Run the calibration workflow.\n  stages             List available workflow stages.\n  update-dem-index   Rebuild the NOAA DEM spatial index from S3 STAC metadata.\n  validate           Validate a configuration file.\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide walks you through running your first coastal simulation using the <code>coastal-calibration</code> CLI.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li><code>coastal-calibration</code> installed (see Installation)</li> <li>Access to an HPC cluster with SLURM</li> <li>The Singularity image at <code>/ngencerf-app/singularity/ngen-coastal.sif</code></li> <li>Access to the <code>/ngen-test</code> NFS mount</li> </ul>"},{"location":"getting-started/quickstart/#schism-quick-start","title":"SCHISM Quick Start","text":""},{"location":"getting-started/quickstart/#step-1-generate-a-configuration-file","title":"Step 1: Generate a Configuration File","text":"<p>Create a new configuration file for your simulation:</p> <pre><code>coastal-calibration init config.yaml --domain hawaii\n</code></pre> <p>This generates a template configuration file with sensible defaults.</p>"},{"location":"getting-started/quickstart/#step-2-edit-the-configuration","title":"Step 2: Edit the Configuration","text":"<p>Open <code>config.yaml</code> and set your simulation parameters:</p> <pre><code>simulation:\n  start_date: 2021-06-11\n  duration_hours: 24\n  coastal_domain: hawaii\n  meteo_source: nwm_ana\n\nboundary:\n  source: stofs\n</code></pre> <p>Minimal Configuration</p> <p>The configuration above is all you need! Paths are automatically generated based on your username, domain, and data sources.</p>"},{"location":"getting-started/quickstart/#step-3-validate-the-configuration","title":"Step 3: Validate the Configuration","text":"<p>Before running, validate your configuration:</p> <pre><code>coastal-calibration validate config.yaml\n</code></pre> <p>This checks for:</p> <ul> <li>Required fields</li> <li>Valid date ranges for data sources</li> <li>File and directory existence</li> <li>Model-specific configuration validity</li> </ul>"},{"location":"getting-started/quickstart/#step-4-run-the-workflow","title":"Step 4: Run the Workflow","text":""},{"location":"getting-started/quickstart/#heredoc-sbatch-script-recommended","title":"Heredoc sbatch Script (Recommended)","text":"<p>The preferred approach on clusters is to write an <code>sbatch</code> script with an inline YAML configuration using a heredoc. This keeps the SLURM directives and workflow configuration in a single, self-contained file:</p> <pre><code>#!/usr/bin/env bash\n#SBATCH --job-name=coastal_schism\n#SBATCH --partition=c5n-18xlarge\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=18\n#SBATCH --exclusive\n#SBATCH --output=slurm-%j.out\n\nCONFIG_FILE=\"/tmp/coastal_config_${SLURM_JOB_ID}.yaml\"\n\ncat &gt; \"${CONFIG_FILE}\" &lt;&lt;'EOF'\nmodel: schism\n\nsimulation:\n  start_date: 2021-01-01\n  duration_hours: 12\n  coastal_domain: hawaii\n  meteo_source: nwm_retro\n\nboundary:\n  source: tpxo\n\nmodel_config:\n  include_noaa_gages: true\nEOF\n\n/ngen-test/coastal-calibration/coastal-calibration run \"${CONFIG_FILE}\"\nrm -f \"${CONFIG_FILE}\"\n</code></pre> <p>Save this as <code>my_run.sh</code> and submit with <code>sbatch my_run.sh</code>.</p> <p>Use the full NFS path</p> <p>Compute nodes may not have <code>coastal-calibration</code> in their <code>PATH</code>. Using the full path to the wrapper on the shared filesystem (<code>/ngen-test/coastal-calibration/coastal-calibration</code>) ensures the command is always found.</p> <p>Unique config filenames</p> <p>The config filename uses <code>$SLURM_JOB_ID</code> to avoid collisions when multiple jobs run concurrently.</p> <p>Single-quoted heredoc</p> <p>Use <code>&lt;&lt;'EOF'</code> (single-quoted) to prevent the shell from expanding <code>$</code> variables inside the YAML content. This ensures the YAML is written exactly as written.</p> <p>Complete sbatch examples are provided for SCHISM and SFINCS.</p>"},{"location":"getting-started/quickstart/#step-5-check-results","title":"Step 5: Check Results","text":"<p>After the job completes, find your outputs in the work directory:</p> <pre><code>ls /ngen-test/coastal/your_username/schism_hawaii_stofs_nwm_ana/schism_2021-06-11/\n</code></pre> <p>Compare with NOAA observations</p> <p>Add <code>include_noaa_gages: true</code> under <code>model_config</code> to automatically discover NOAA CO-OPS water level stations, collect station time-series during the simulation, and generate comparison plots after the run completes. See Configuration for details.</p>"},{"location":"getting-started/quickstart/#sfincs-quick-start","title":"SFINCS Quick Start","text":""},{"location":"getting-started/quickstart/#step-1-generate-a-sfincs-configuration","title":"Step 1: Generate a SFINCS Configuration","text":"<pre><code>coastal-calibration init sfincs_config.yaml --domain atlgulf --model sfincs\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-edit-the-configuration_1","title":"Step 2: Edit the Configuration","text":"<p>Set the path to a pre-built SFINCS model:</p> <pre><code>model: sfincs\n\nsimulation:\n  start_date: 2025-06-01\n  duration_hours: 168\n  coastal_domain: atlgulf\n  meteo_source: nwm_ana\n\nboundary:\n  source: stofs\n\nmodel_config:\n  prebuilt_dir: /path/to/prebuilt/sfincs/model\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-validate-and-run","title":"Step 3: Validate and Run","text":"<pre><code>coastal-calibration validate sfincs_config.yaml\n</code></pre> <p>Then write an sbatch script using <code>coastal-calibration run sfincs_config.yaml</code>.</p>"},{"location":"getting-started/quickstart/#sfincs-model-creation-quick-start","title":"SFINCS Model Creation Quick Start","text":"<p>The <code>create</code> command builds a new SFINCS quadtree model from an AOI polygon, handling grid generation, DEM download, elevation, masking, boundary cells, and subgrid tables.</p>"},{"location":"getting-started/quickstart/#step-1-prepare-a-topobathy-dem","title":"Step 1: Prepare a Topobathy DEM","text":"<p>Download the NWS 30 m topobathymetric DEM clipped to your area of interest:</p> <pre><code>coastal-calibration prepare-topobathy aoi.geojson --domain atlgulf --output-dir ./dem\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-write-a-creation-config","title":"Step 2: Write a Creation Config","text":"<pre><code>aoi: aoi.geojson\noutput_dir: ./my_sfincs_model\n\ngrid:\n  crs: EPSG:32617\n\nelevation:\n  datasets:\n    - name: nws_topobathy\n      zmin: -20000\n\ndata_catalog:\n  data_libs:\n    - ./dem/data_catalog.yml\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-run-the-creation-workflow","title":"Step 3: Run the Creation Workflow","text":"<pre><code>coastal-calibration create create_config.yaml\n</code></pre> <p>The output directory will contain a ready-to-run SFINCS model that can be used as the <code>prebuilt_dir</code> in a simulation config.</p>"},{"location":"getting-started/quickstart/#using-the-python-api","title":"Using the Python API","text":"<p>You can also run workflows programmatically:</p> <pre><code>from coastal_calibration import CoastalCalibConfig, CoastalCalibRunner\n\n# Load configuration\nconfig = CoastalCalibConfig.from_yaml(\"config.yaml\")\n\n# Create runner and execute\nrunner = CoastalCalibRunner(config)\nresult = runner.run()\n\nif result.success:\n    print(f\"Completed in {result.duration_seconds:.1f}s\")\nelse:\n    print(f\"Failed: {result.errors}\")\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Configuration Options</li> <li>Explore Workflow Stages</li> <li>See the CLI Reference</li> </ul>"},{"location":"reference/api/","title":"API Reference","text":"<p>This page provides detailed documentation for the NWM Coastal Python API.</p>"},{"location":"reference/api/#configuration-classes","title":"Configuration Classes","text":""},{"location":"reference/api/#coastalcalibconfig","title":"CoastalCalibConfig","text":""},{"location":"reference/api/#coastal_calibration.config.schema.CoastalCalibConfig","title":"CoastalCalibConfig  <code>dataclass</code>","text":"<pre><code>CoastalCalibConfig(\n    simulation,\n    boundary,\n    paths,\n    model_config,\n    monitoring=MonitoringConfig(),\n    download=DownloadConfig(),\n    _base_config=None,\n)\n</code></pre> <p>Complete coastal calibration workflow configuration.</p> <p>Supports both SCHISM and SFINCS models via the polymorphic :attr:<code>model_config</code> field.  The concrete type is selected by the <code>model</code> key in the YAML file and resolved through :data:<code>MODEL_REGISTRY</code>.</p>"},{"location":"reference/api/#coastal_calibration.config.schema.CoastalCalibConfig.model","title":"model  <code>property</code>","text":"<pre><code>model\n</code></pre> <p>Model identifier string (convenience accessor).</p>"},{"location":"reference/api/#coastal_calibration.config.schema.CoastalCalibConfig.from_yaml","title":"from_yaml  <code>classmethod</code>","text":"<pre><code>from_yaml(config_path)\n</code></pre> <p>Load configuration from YAML file with optional inheritance.</p> <p>Supports variable interpolation using ${section.key} syntax. Variables are resolved from other config values, e.g.:</p> <ul> <li><code>${user}</code> -&gt; value of <code>$USER</code> environment variable</li> <li><code>${simulation.coastal_domain}</code> -&gt; value of <code>simulation.coastal_domain</code></li> <li><code>${model}</code> -&gt; the model type string (<code>\"schism\"</code> or <code>\"sfincs\"</code>)</li> </ul> PARAMETER DESCRIPTION <code>config_path</code> <p>Path to YAML configuration file.</p> <p> TYPE: <code>Path or str</code> </p> RETURNS DESCRIPTION <code>CoastalCalibConfig</code> <p>Loaded configuration.</p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the configuration file does not exist.</p> <code>YAMLError</code> <p>If the YAML file is malformed.</p> Source code in <code>src/coastal_calibration/config/schema.py</code> <pre><code>@classmethod\ndef from_yaml(cls, config_path: Path | str) -&gt; CoastalCalibConfig:\n    \"\"\"Load configuration from YAML file with optional inheritance.\n\n    Supports variable interpolation using ${section.key} syntax.\n    Variables are resolved from other config values, e.g.:\n\n    - ``${user}`` -&gt; value of ``$USER`` environment variable\n    - ``${simulation.coastal_domain}`` -&gt; value of ``simulation.coastal_domain``\n    - ``${model}`` -&gt; the model type string (``\"schism\"`` or ``\"sfincs\"``)\n\n    Parameters\n    ----------\n    config_path : Path or str\n        Path to YAML configuration file.\n\n    Returns\n    -------\n    CoastalCalibConfig\n        Loaded configuration.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the configuration file does not exist.\n    yaml.YAMLError\n        If the YAML file is malformed.\n    \"\"\"\n    config_path = Path(config_path)\n    if not config_path.exists():\n        raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n\n    try:\n        data = yaml.safe_load(config_path.read_text())\n    except yaml.YAMLError as e:\n        raise yaml.YAMLError(f\"Invalid YAML in {config_path}: {e}\") from e\n\n    if data is None:\n        raise ValueError(f\"Configuration file is empty: {config_path}\")\n\n    base_config = None\n    if \"_base\" in data:\n        base_path = Path(data.pop(\"_base\"))\n        if not base_path.is_absolute():\n            base_path = config_path.parent / base_path\n        base_config = cls.from_yaml(base_path)\n        data = _deep_merge(base_config.to_dict(), data)\n\n    # Ensure model key has a default before interpolation\n    data.setdefault(\"model\", \"schism\")\n\n    # Inject default path templates if not provided (before interpolation)\n    if \"paths\" not in data:\n        data[\"paths\"] = {}\n    if \"work_dir\" not in data[\"paths\"]:\n        data[\"paths\"][\"work_dir\"] = DEFAULT_WORK_DIR_TEMPLATE\n    if \"raw_download_dir\" not in data[\"paths\"]:\n        data[\"paths\"][\"raw_download_dir\"] = DEFAULT_RAW_DOWNLOAD_DIR_TEMPLATE\n\n    # Interpolate variables after merging\n    data = _interpolate_config(data)\n\n    return cls._from_dict(data, base_config_path=config_path if base_config else None)\n</code></pre>"},{"location":"reference/api/#coastal_calibration.config.schema.CoastalCalibConfig.to_yaml","title":"to_yaml","text":"<pre><code>to_yaml(path)\n</code></pre> <p>Write configuration to YAML file.</p> PARAMETER DESCRIPTION <code>path</code> <p>Path to YAML output file. Parent directories will be created if they don't exist.</p> <p> TYPE: <code>Path or str</code> </p> Source code in <code>src/coastal_calibration/config/schema.py</code> <pre><code>def to_yaml(self, path: Path | str) -&gt; None:\n    \"\"\"Write configuration to YAML file.\n\n    Parameters\n    ----------\n    path : Path or str\n        Path to YAML output file. Parent directories will be created\n        if they don't exist.\n    \"\"\"\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(yaml.dump(self.to_dict(), default_flow_style=False, sort_keys=False))\n</code></pre>"},{"location":"reference/api/#coastal_calibration.config.schema.CoastalCalibConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Convert config to dictionary.</p> Source code in <code>src/coastal_calibration/config/schema.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert config to dictionary.\"\"\"\n    return {\n        \"model\": self.model,\n        \"simulation\": {\n            \"start_date\": self.simulation.start_date.isoformat(),\n            \"duration_hours\": self.simulation.duration_hours,\n            \"coastal_domain\": self.simulation.coastal_domain,\n            \"meteo_source\": self.simulation.meteo_source,\n            \"timestep_seconds\": self.simulation.timestep_seconds,\n        },\n        \"boundary\": {\n            \"source\": self.boundary.source,\n            \"stofs_file\": (str(self.boundary.stofs_file) if self.boundary.stofs_file else None),\n        },\n        \"paths\": {\n            \"work_dir\": str(self.paths.work_dir),\n            \"raw_download_dir\": (\n                str(self.paths.raw_download_dir) if self.paths.raw_download_dir else None\n            ),\n            \"nfs_mount\": str(self.paths.nfs_mount),\n            \"nwm_dir\": str(self.paths.nwm_dir),\n            \"hot_start_file\": (\n                str(self.paths.hot_start_file) if self.paths.hot_start_file else None\n            ),\n            \"conda_env_name\": self.paths.conda_env_name,\n            \"parm_dir\": str(self.paths.parm_dir),\n        },\n        \"model_config\": self.model_config.to_dict(),\n        \"monitoring\": {\n            \"log_level\": self.monitoring.log_level,\n            \"log_file\": (str(self.monitoring.log_file) if self.monitoring.log_file else None),\n            \"enable_progress_tracking\": self.monitoring.enable_progress_tracking,\n            \"enable_timing\": self.monitoring.enable_timing,\n        },\n        \"download\": {\n            \"enabled\": self.download.enabled,\n            \"timeout\": self.download.timeout,\n            \"raise_on_error\": self.download.raise_on_error,\n            \"limit_per_host\": self.download.limit_per_host,\n        },\n    }\n</code></pre>"},{"location":"reference/api/#coastal_calibration.config.schema.CoastalCalibConfig.validate","title":"validate","text":"<pre><code>validate()\n</code></pre> <p>Validate configuration and return list of errors.</p> Source code in <code>src/coastal_calibration/config/schema.py</code> <pre><code>def validate(self) -&gt; list[str]:\n    \"\"\"Validate configuration and return list of errors.\"\"\"\n    from coastal_calibration.downloader import validate_date_ranges\n\n    errors: list[str] = []\n\n    if self.simulation.duration_hours &lt;= 0:\n        errors.append(\"simulation.duration_hours must be positive\")\n\n    # Model-specific validation\n    errors.extend(self.model_config.validate(self))\n\n    # Shared boundary validation\n    errors.extend(self._validate_boundary_source())\n\n    # Date range validation\n    if self.download.enabled:\n        sim = self.simulation\n        start_time = sim.start_date\n        end_time = start_time + timedelta(hours=sim.duration_hours)\n        date_errors = validate_date_ranges(\n            start_time,\n            end_time,\n            sim.meteo_source,\n            self.boundary.source,\n            sim.coastal_domain,\n        )\n        errors.extend(date_errors)\n\n    return errors\n</code></pre>"},{"location":"reference/api/#simulationconfig","title":"SimulationConfig","text":""},{"location":"reference/api/#coastal_calibration.config.schema.SimulationConfig","title":"SimulationConfig  <code>dataclass</code>","text":"<pre><code>SimulationConfig(\n    start_date,\n    duration_hours,\n    coastal_domain,\n    meteo_source,\n    timestep_seconds=3600,\n)\n</code></pre> <p>Simulation time and domain configuration.</p>"},{"location":"reference/api/#coastal_calibration.config.schema.SimulationConfig.start_pdy","title":"start_pdy  <code>property</code>","text":"<pre><code>start_pdy\n</code></pre> <p>Return start date as YYYYMMDD string.</p>"},{"location":"reference/api/#coastal_calibration.config.schema.SimulationConfig.start_cyc","title":"start_cyc  <code>property</code>","text":"<pre><code>start_cyc\n</code></pre> <p>Return start cycle (hour) as HH string.</p>"},{"location":"reference/api/#coastal_calibration.config.schema.SimulationConfig.inland_domain","title":"inland_domain  <code>property</code>","text":"<pre><code>inland_domain\n</code></pre> <p>Inland domain directory name for this coastal domain.</p>"},{"location":"reference/api/#coastal_calibration.config.schema.SimulationConfig.nwm_domain","title":"nwm_domain  <code>property</code>","text":"<pre><code>nwm_domain\n</code></pre> <p>NWM domain identifier for this coastal domain.</p>"},{"location":"reference/api/#coastal_calibration.config.schema.SimulationConfig.geo_grid","title":"geo_grid  <code>property</code>","text":"<pre><code>geo_grid\n</code></pre> <p>Geogrid filename for this coastal domain.</p>"},{"location":"reference/api/#boundaryconfig","title":"BoundaryConfig","text":""},{"location":"reference/api/#coastal_calibration.config.schema.BoundaryConfig","title":"BoundaryConfig  <code>dataclass</code>","text":"<pre><code>BoundaryConfig(source='tpxo', stofs_file=None)\n</code></pre> <p>Boundary condition configuration.</p>"},{"location":"reference/api/#pathconfig","title":"PathConfig","text":""},{"location":"reference/api/#coastal_calibration.config.schema.PathConfig","title":"PathConfig  <code>dataclass</code>","text":"<pre><code>PathConfig(\n    work_dir,\n    raw_download_dir=None,\n    nfs_mount=(lambda: DEFAULT_NFS_MOUNT)(),\n    nwm_dir=(lambda: DEFAULT_NWM_DIR)(),\n    hot_start_file=None,\n    conda_env_name=DEFAULT_CONDA_ENV_NAME,\n    parm_dir=(lambda: DEFAULT_PARM_DIR)(),\n)\n</code></pre> <p>Path configuration for data and executables.</p>"},{"location":"reference/api/#coastal_calibration.config.schema.PathConfig.otps_dir","title":"otps_dir  <code>property</code>","text":"<pre><code>otps_dir\n</code></pre> <p>TPXO binary directory (inside Singularity container, not configurable).</p>"},{"location":"reference/api/#coastal_calibration.config.schema.PathConfig.tpxo_data_dir","title":"tpxo_data_dir  <code>property</code>","text":"<pre><code>tpxo_data_dir\n</code></pre> <p>TPXO tidal atlas data directory.</p>"},{"location":"reference/api/#coastal_calibration.config.schema.PathConfig.ush_nwm","title":"ush_nwm  <code>property</code>","text":"<pre><code>ush_nwm\n</code></pre> <p>USH scripts directory.</p>"},{"location":"reference/api/#coastal_calibration.config.schema.PathConfig.exec_nwm","title":"exec_nwm  <code>property</code>","text":"<pre><code>exec_nwm\n</code></pre> <p>Executables directory.</p>"},{"location":"reference/api/#coastal_calibration.config.schema.PathConfig.parm_nwm","title":"parm_nwm  <code>property</code>","text":"<pre><code>parm_nwm\n</code></pre> <p>Parameter files directory.</p>"},{"location":"reference/api/#coastal_calibration.config.schema.PathConfig.conda_envs_path","title":"conda_envs_path  <code>property</code>","text":"<pre><code>conda_envs_path\n</code></pre> <p>Conda environments directory.</p>"},{"location":"reference/api/#coastal_calibration.config.schema.PathConfig.download_dir","title":"download_dir  <code>property</code>","text":"<pre><code>download_dir\n</code></pre> <p>Effective download directory (fallback to work_dir/downloads).</p>"},{"location":"reference/api/#coastal_calibration.config.schema.PathConfig.meteo_dir","title":"meteo_dir","text":"<pre><code>meteo_dir(meteo_source)\n</code></pre> <p>Directory for meteorological data.</p> Source code in <code>src/coastal_calibration/config/schema.py</code> <pre><code>def meteo_dir(self, meteo_source: str) -&gt; Path:\n    \"\"\"Directory for meteorological data.\"\"\"\n    return self.download_dir / self.METEO_SUBDIR / meteo_source\n</code></pre>"},{"location":"reference/api/#coastal_calibration.config.schema.PathConfig.streamflow_dir","title":"streamflow_dir","text":"<pre><code>streamflow_dir(meteo_source)\n</code></pre> <p>Directory for streamflow/hydro data.</p> Source code in <code>src/coastal_calibration/config/schema.py</code> <pre><code>def streamflow_dir(self, meteo_source: str) -&gt; Path:\n    \"\"\"Directory for streamflow/hydro data.\"\"\"\n    if meteo_source == \"nwm_retro\":\n        return self.download_dir / self.STREAMFLOW_SUBDIR / \"nwm_retro\"\n    return self.download_dir / self.HYDRO_SUBDIR / \"nwm\"\n</code></pre>"},{"location":"reference/api/#coastal_calibration.config.schema.PathConfig.coastal_dir","title":"coastal_dir","text":"<pre><code>coastal_dir(coastal_source)\n</code></pre> <p>Directory for coastal boundary data.</p> Source code in <code>src/coastal_calibration/config/schema.py</code> <pre><code>def coastal_dir(self, coastal_source: str) -&gt; Path:\n    \"\"\"Directory for coastal boundary data.\"\"\"\n    return self.download_dir / self.COASTAL_SUBDIR / coastal_source\n</code></pre>"},{"location":"reference/api/#coastal_calibration.config.schema.PathConfig.geogrid_file","title":"geogrid_file","text":"<pre><code>geogrid_file(sim)\n</code></pre> <p>Geogrid file path for the given domain.</p> Source code in <code>src/coastal_calibration/config/schema.py</code> <pre><code>def geogrid_file(self, sim: SimulationConfig) -&gt; Path:\n    \"\"\"Geogrid file path for the given domain.\"\"\"\n    return self.parm_nwm / sim.inland_domain / sim.geo_grid\n</code></pre>"},{"location":"reference/api/#modelconfig","title":"ModelConfig","text":""},{"location":"reference/api/#coastal_calibration.config.schema.ModelConfig","title":"ModelConfig","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for model-specific configuration.</p> <p>Each concrete subclass owns its compute parameters, environment variable construction, stage ordering, validation, and SLURM script generation. This keeps model-specific concerns out of the shared configuration and makes adding new models straightforward: create a new subclass, implement the abstract methods, and register it in :data:<code>MODEL_REGISTRY</code>.</p>"},{"location":"reference/api/#coastal_calibration.config.schema.ModelConfig.model_name","title":"model_name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>model_name\n</code></pre> <p>Return the model identifier string (e.g. <code>'schism'</code>, <code>'sfincs'</code>).</p>"},{"location":"reference/api/#coastal_calibration.config.schema.ModelConfig.stage_order","title":"stage_order  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>stage_order\n</code></pre> <p>Ordered list of stage names for this model's pipeline.</p>"},{"location":"reference/api/#coastal_calibration.config.schema.ModelConfig.build_environment","title":"build_environment  <code>abstractmethod</code>","text":"<pre><code>build_environment(env, config)\n</code></pre> <p>Add model-specific environment variables to env (mutating).</p> <p>Called by :meth:<code>WorkflowStage.build_environment</code> after shared variables have been populated.</p> Source code in <code>src/coastal_calibration/config/schema.py</code> <pre><code>@abstractmethod\ndef build_environment(self, env: dict[str, str], config: CoastalCalibConfig) -&gt; dict[str, str]:\n    \"\"\"Add model-specific environment variables to *env* (mutating).\n\n    Called by :meth:`WorkflowStage.build_environment` after shared\n    variables have been populated.\n    \"\"\"\n</code></pre>"},{"location":"reference/api/#coastal_calibration.config.schema.ModelConfig.validate","title":"validate  <code>abstractmethod</code>","text":"<pre><code>validate(config)\n</code></pre> <p>Return model-specific validation errors.</p> Source code in <code>src/coastal_calibration/config/schema.py</code> <pre><code>@abstractmethod\ndef validate(self, config: CoastalCalibConfig) -&gt; list[str]:\n    \"\"\"Return model-specific validation errors.\"\"\"\n</code></pre>"},{"location":"reference/api/#coastal_calibration.config.schema.ModelConfig.create_stages","title":"create_stages  <code>abstractmethod</code>","text":"<pre><code>create_stages(config, monitor)\n</code></pre> <p>Construct and return the <code>{name: stage}</code> dictionary.</p> Source code in <code>src/coastal_calibration/config/schema.py</code> <pre><code>@abstractmethod\ndef create_stages(self, config: CoastalCalibConfig, monitor: Any) -&gt; dict[str, Any]:\n    \"\"\"Construct and return the ``{name: stage}`` dictionary.\"\"\"\n</code></pre>"},{"location":"reference/api/#coastal_calibration.config.schema.ModelConfig.to_dict","title":"to_dict  <code>abstractmethod</code>","text":"<pre><code>to_dict()\n</code></pre> <p>Serialize model-specific fields to a dictionary.</p> Source code in <code>src/coastal_calibration/config/schema.py</code> <pre><code>@abstractmethod\ndef to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Serialize model-specific fields to a dictionary.\"\"\"\n</code></pre>"},{"location":"reference/api/#schismmodelconfig","title":"SchismModelConfig","text":""},{"location":"reference/api/#coastal_calibration.config.schema.SchismModelConfig","title":"SchismModelConfig  <code>dataclass</code>","text":"<pre><code>SchismModelConfig(\n    singularity_image=(lambda: DEFAULT_SING_IMAGE_PATH)(),\n    nodes=2,\n    ntasks_per_node=18,\n    exclusive=True,\n    nscribes=2,\n    omp_num_threads=2,\n    oversubscribe=False,\n    binary=_DEFAULT_SCHISM_BINARY,\n    include_noaa_gages=False,\n)\n</code></pre> <p>               Bases: <code>ModelConfig</code></p> <p>SCHISM model configuration.</p> <p>Contains compute parameters (MPI layout, SCHISM binary).</p> PARAMETER DESCRIPTION <code>singularity_image</code> <p>Path to the Singularity/Apptainer SIF image used to run SCHISM and its pre-/post-processing scripts inside a container.  SFINCS manages its own container independently (see :attr:<code>SfincsModelConfig.container_tag</code>).</p> <p> TYPE: <code>Path</code> DEFAULT: <code>(lambda: DEFAULT_SING_IMAGE_PATH)()</code> </p> <code>nodes</code> <p>Number of SLURM nodes.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>ntasks_per_node</code> <p>MPI tasks per node.</p> <p> TYPE: <code>int</code> DEFAULT: <code>18</code> </p> <code>exclusive</code> <p>Request exclusive node access.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>nscribes</code> <p>Number of SCHISM scribe processes.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>omp_num_threads</code> <p>OpenMP threads per MPI rank.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>oversubscribe</code> <p>Allow MPI oversubscription.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>binary</code> <p>SCHISM executable name.</p> <p> TYPE: <code>str</code> DEFAULT: <code>_DEFAULT_SCHISM_BINARY</code> </p> <code>include_noaa_gages</code> <p>When True, automatically query NOAA CO-OPS for water level stations within the model domain (computed from the concave hull of open boundary nodes in <code>hgrid.gr3</code>), write a <code>station.in</code> file, set <code>iout_sta = 1</code> in <code>param.nml</code>, and generate sim-vs-obs comparison plots after the run. Requires the <code>plot</code> optional dependencies.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/api/#coastal_calibration.config.schema.SchismModelConfig.total_tasks","title":"total_tasks  <code>property</code>","text":"<pre><code>total_tasks\n</code></pre> <p>Total number of MPI tasks (nodes * ntasks_per_node).</p>"},{"location":"reference/api/#coastal_calibration.config.schema.SchismModelConfig.schism_mesh","title":"schism_mesh","text":"<pre><code>schism_mesh(sim, paths)\n</code></pre> <p>SCHISM ESMF mesh file path for the given domain.</p> Source code in <code>src/coastal_calibration/config/schema.py</code> <pre><code>def schism_mesh(self, sim: SimulationConfig, paths: PathConfig) -&gt; Path:\n    \"\"\"SCHISM ESMF mesh file path for the given domain.\"\"\"\n    return paths.parm_nwm / \"coastal\" / sim.coastal_domain / \"hgrid.nc\"\n</code></pre>"},{"location":"reference/api/#sfincsmodelconfig","title":"SfincsModelConfig","text":""},{"location":"reference/api/#coastal_calibration.config.schema.SfincsModelConfig","title":"SfincsModelConfig  <code>dataclass</code>","text":"<pre><code>SfincsModelConfig(\n    prebuilt_dir,\n    model_root=None,\n    include_noaa_gages=False,\n    observation_points=list(),\n    observation_locations_file=None,\n    merge_observations=False,\n    discharge_locations_file=None,\n    merge_discharge=False,\n    include_precip=False,\n    include_wind=False,\n    include_pressure=False,\n    meteo_res=None,\n    forcing_to_mesh_offset_m=0.0,\n    vdatum_mesh_to_msl_m=0.0,\n    sfincs_exe=None,\n    container_tag=\"latest\",\n    container_image=None,\n    omp_num_threads=0,\n    inp_overrides=dict(),\n)\n</code></pre> <p>               Bases: <code>ModelConfig</code></p> <p>SFINCS model configuration.</p> <p>SFINCS runs on a single node using OpenMP (all available cores). There is no MPI or multi-node support.</p> PARAMETER DESCRIPTION <code>prebuilt_dir</code> <p>Path to the directory containing the pre-built model files (<code>sfincs.inp</code>, <code>sfincs.nc</code>, <code>region.geojson</code>, etc.).</p> <p> TYPE: <code>Path</code> </p> <code>model_root</code> <p>Output directory for the built model.  Defaults to <code>{work_dir}/sfincs_model</code>.</p> <p> TYPE: <code>Path</code> DEFAULT: <code>None</code> </p> <code>include_noaa_gages</code> <p>When True, automatically query NOAA CO-OPS for water level stations within the model domain and add them as observation points.  Requires the <code>plot</code> optional dependencies.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>observation_points</code> <p>Observation point specifications as list of dicts with <code>x</code>, <code>y</code>, <code>name</code> keys (coordinates in model CRS).</p> <p> TYPE: <code>list</code> DEFAULT: <code>list()</code> </p> <code>observation_locations_file</code> <p>Path to a GeoJSON file with observation point locations.</p> <p> TYPE: <code>Path</code> DEFAULT: <code>None</code> </p> <code>merge_observations</code> <p>Whether to merge with pre-existing observation points.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>discharge_locations_file</code> <p>Path to a SFINCS <code>.src</code> or GeoJSON with discharge source point locations.</p> <p> TYPE: <code>Path</code> DEFAULT: <code>None</code> </p> <code>merge_discharge</code> <p>Whether to merge with pre-existing discharge source points.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>include_precip</code> <p>When True, add precipitation forcing from the meteorological data catalog entry (derived from <code>simulation.meteo_source</code>).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>include_wind</code> <p>When True, add spatially-varying wind forcing (<code>wind10_u</code>, <code>wind10_v</code>) from the meteorological data catalog entry.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>include_pressure</code> <p>When True, add spatially-varying atmospheric pressure forcing (<code>press_msl</code>) and enable barometric correction (<code>baro=1</code>).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>meteo_res</code> <p>Output resolution (m) for gridded meteorological forcing (precipitation, wind, pressure).  When None (default) the resolution is determined from the SFINCS quadtree grid \u2014 it equals the base cell size (coarsest level) so that the meteo grid is never finer than needed.  Setting an explicit value (e.g. <code>2000</code>) overrides the automatic calculation.</p> <p>.. note::</p> <p>Without this parameter the HydroMT <code>reproject</code> call    retains the source-data resolution (\u2248 1 km for NWM), and    the LCC \u2192 UTM reprojection can inflate the output to the    full CONUS extent, producing multi-GB files and very slow    simulations.</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>forcing_to_mesh_offset_m</code> <p>Vertical offset in metres added to the boundary-condition water levels before they enter SFINCS.</p> <p>Tidal-only sources such as TPXO provide oscillations centred on zero (MSL) but carry no information about where MSL sits on the mesh's vertical datum.  This parameter anchors the forcing signal to the correct geodetic height on the mesh.  Set it to the elevation of MSL in the mesh datum obtained from VDatum (e.g. <code>0.171</code> for a NAVD88 mesh on the Texas Gulf coast, where MSL is 0.171 m above NAVD88).</p> <p>For sources that already report water levels in the mesh datum (e.g. STOFS on a NAVD88 mesh) set this to <code>0.0</code>.</p> <p>Defaults to <code>0.0</code>.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>vdatum_mesh_to_msl_m</code> <p>Vertical offset in metres added to the simulated water level before comparison with NOAA CO-OPS observations (which are in MSL).  The model output inherits the mesh vertical datum, so this converts it to MSL (e.g. <code>0.171</code> for a NAVD88 mesh on the Texas Gulf coast).</p> <p>Defaults to <code>0.0</code>.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>sfincs_exe</code> <p>Path to a locally compiled SFINCS executable.  When set, the <code>sfincs_run</code> stage invokes this binary directly instead of using a Singularity container, making it possible to run on systems where Singularity is unavailable (e.g. macOS laptops). The container-related options (<code>container_tag</code>, <code>container_image</code>) are ignored when <code>sfincs_exe</code> is set.</p> <p> TYPE: <code>Path</code> DEFAULT: <code>None</code> </p> <code>container_tag</code> <p>Tag for the <code>deltares/sfincs-cpu</code> Docker/Singularity image.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'latest'</code> </p> <code>container_image</code> <p>Path to a pre-pulled Singularity SIF file.</p> <p> TYPE: <code>Path</code> DEFAULT: <code>None</code> </p> <code>omp_num_threads</code> <p>Number of OpenMP threads.  Defaults to the number of physical CPU cores on the current machine (see :func:<code>~coastal_calibration.utils.system.get_cpu_count</code>). On HPC nodes this auto-detects correctly; on a local laptop it avoids over-subscribing the system.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>inp_overrides</code> <p>Arbitrary key/value pairs written to <code>sfincs.inp</code> just before the model is written to disk.  Use this to override physics parameters that HydroMT-SFINCS sets by default (e.g. <code>advection: 0</code>, <code>nuvisc: 0.01</code>).  Keys must be valid <code>sfincs.inp</code> parameter names.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>dict()</code> </p>"},{"location":"reference/api/#monitoringconfig","title":"MonitoringConfig","text":""},{"location":"reference/api/#coastal_calibration.config.schema.MonitoringConfig","title":"MonitoringConfig  <code>dataclass</code>","text":"<pre><code>MonitoringConfig(\n    log_level=\"INFO\",\n    log_file=None,\n    enable_progress_tracking=True,\n    enable_timing=True,\n)\n</code></pre> <p>Workflow monitoring configuration.</p>"},{"location":"reference/api/#downloadconfig","title":"DownloadConfig","text":""},{"location":"reference/api/#coastal_calibration.config.schema.DownloadConfig","title":"DownloadConfig  <code>dataclass</code>","text":"<pre><code>DownloadConfig(\n    enabled=True,\n    timeout=600,\n    raise_on_error=True,\n    limit_per_host=4,\n)\n</code></pre> <p>Data download configuration.</p>"},{"location":"reference/api/#sfincs-creation-configuration","title":"SFINCS Creation Configuration","text":""},{"location":"reference/api/#sfincscreateconfig","title":"SfincsCreateConfig","text":""},{"location":"reference/api/#coastal_calibration.config.create_schema.SfincsCreateConfig","title":"SfincsCreateConfig  <code>dataclass</code>","text":"<pre><code>SfincsCreateConfig(\n    aoi,\n    output_dir,\n    download_dir=None,\n    grid=GridConfig(),\n    elevation=ElevationConfig(),\n    mask=MaskConfig(),\n    subgrid=SubgridConfig(),\n    data_catalog=DataCatalogConfig(),\n    monitoring=MonitoringConfig(),\n    nwm_discharge=None,\n)\n</code></pre> <p>Root configuration for SFINCS model creation workflow.</p> <p>Loaded from YAML via :meth:<code>from_yaml</code>.  All paths are resolved to absolute paths during construction.</p>"},{"location":"reference/api/#coastal_calibration.config.create_schema.SfincsCreateConfig.stage_order","title":"stage_order  <code>property</code>","text":"<pre><code>stage_order\n</code></pre> <p>Ordered list of creation stages to execute.</p> <p>Roughness is embedded in the quadtree subgrid tables, so there is no separate roughness stage.  The <code>create_discharge</code> stage is included only when :attr:<code>nwm_discharge</code> is configured.</p>"},{"location":"reference/api/#coastal_calibration.config.create_schema.SfincsCreateConfig.from_yaml","title":"from_yaml  <code>classmethod</code>","text":"<pre><code>from_yaml(config_path)\n</code></pre> <p>Load configuration from a YAML file.</p> PARAMETER DESCRIPTION <code>config_path</code> <p>Path to YAML configuration file.</p> <p> TYPE: <code>Path or str</code> </p> RETURNS DESCRIPTION <code>SfincsCreateConfig</code> <p>Loaded configuration.</p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the configuration file does not exist.</p> <code>YAMLError</code> <p>If the YAML file is malformed.</p> Source code in <code>src/coastal_calibration/config/create_schema.py</code> <pre><code>@classmethod\ndef from_yaml(cls, config_path: Path | str) -&gt; SfincsCreateConfig:\n    \"\"\"Load configuration from a YAML file.\n\n    Parameters\n    ----------\n    config_path : Path or str\n        Path to YAML configuration file.\n\n    Returns\n    -------\n    SfincsCreateConfig\n        Loaded configuration.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the configuration file does not exist.\n    yaml.YAMLError\n        If the YAML file is malformed.\n    \"\"\"\n    config_path = Path(config_path)\n    if not config_path.exists():\n        raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n\n    try:\n        data = yaml.safe_load(config_path.read_text())\n    except yaml.YAMLError as e:\n        raise yaml.YAMLError(f\"Invalid YAML in {config_path}: {e}\") from e\n\n    if data is None:\n        raise ValueError(f\"Configuration file is empty: {config_path}\")\n\n    cls._resolve_relative_paths(data, config_path.parent)\n    return cls._from_dict(data)\n</code></pre>"},{"location":"reference/api/#coastal_calibration.config.create_schema.SfincsCreateConfig.to_yaml","title":"to_yaml","text":"<pre><code>to_yaml(path)\n</code></pre> <p>Write configuration to a YAML file.</p> PARAMETER DESCRIPTION <code>path</code> <p>Path to YAML output file.  Parent directories are created automatically.</p> <p> TYPE: <code>Path or str</code> </p> Source code in <code>src/coastal_calibration/config/create_schema.py</code> <pre><code>def to_yaml(self, path: Path | str) -&gt; None:\n    \"\"\"Write configuration to a YAML file.\n\n    Parameters\n    ----------\n    path : Path or str\n        Path to YAML output file.  Parent directories are created\n        automatically.\n    \"\"\"\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(yaml.dump(self.to_dict(), default_flow_style=False, sort_keys=False))\n</code></pre>"},{"location":"reference/api/#coastal_calibration.config.create_schema.SfincsCreateConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Convert configuration to a plain dictionary.</p> Source code in <code>src/coastal_calibration/config/create_schema.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert configuration to a plain dictionary.\"\"\"\n    return {\n        \"aoi\": str(self.aoi),\n        \"output_dir\": str(self.output_dir),\n        **({\"download_dir\": str(self.download_dir)} if self.download_dir else {}),\n        \"grid\": {\n            \"resolution\": self.grid.resolution,\n            \"crs\": self.grid.crs,\n            \"rotated\": self.grid.rotated,\n            \"refinement\": [\n                {\n                    \"polygon\": str(r.polygon),\n                    \"level\": r.level,\n                    **({\"buffer_m\": r.buffer_m} if r.buffer_m != 0.0 else {}),\n                }\n                for r in self.grid.refinement\n            ],\n        },\n        \"elevation\": {\n            \"datasets\": [\n                {\n                    \"name\": d.name,\n                    \"zmin\": d.zmin,\n                    **({\"source\": d.source} if d.source else {}),\n                    **({\"noaa_dataset\": d.noaa_dataset} if d.noaa_dataset else {}),\n                }\n                for d in self.elevation.datasets\n            ],\n            \"buffer_cells\": self.elevation.buffer_cells,\n        },\n        \"mask\": {\n            \"zmin\": self.mask.zmin,\n            \"boundary_zmax\": self.mask.boundary_zmax,\n            \"reset_bounds\": self.mask.reset_bounds,\n        },\n        \"subgrid\": {\n            \"nr_subgrid_pixels\": self.subgrid.nr_subgrid_pixels,\n            \"lulc_dataset\": self.subgrid.lulc_dataset,\n            \"reclass_table\": (\n                str(self.subgrid.reclass_table) if self.subgrid.reclass_table else None\n            ),\n            \"manning_land\": self.subgrid.manning_land,\n            \"manning_sea\": self.subgrid.manning_sea,\n        },\n        \"data_catalog\": {\n            \"data_libs\": self.data_catalog.data_libs,\n        },\n        \"monitoring\": {\n            \"log_level\": self.monitoring.log_level,\n            \"log_file\": (str(self.monitoring.log_file) if self.monitoring.log_file else None),\n            \"enable_progress_tracking\": self.monitoring.enable_progress_tracking,\n            \"enable_timing\": self.monitoring.enable_timing,\n        },\n        \"nwm_discharge\": (\n            {\n                \"hydrofabric_gpkg\": str(self.nwm_discharge.hydrofabric_gpkg),\n                \"flowpaths_layer\": self.nwm_discharge.flowpaths_layer,\n                \"flowpath_id_column\": self.nwm_discharge.flowpath_id_column,\n                \"flowpath_ids\": self.nwm_discharge.flowpath_ids,\n                \"coastal_domain\": self.nwm_discharge.coastal_domain,\n            }\n            if self.nwm_discharge is not None\n            else None\n        ),\n    }\n</code></pre>"},{"location":"reference/api/#coastal_calibration.config.create_schema.SfincsCreateConfig.validate","title":"validate","text":"<pre><code>validate()\n</code></pre> <p>Validate configuration and return a list of error messages.</p> RETURNS DESCRIPTION <code>list of str</code> <p>Validation errors (empty when the config is valid).</p> Source code in <code>src/coastal_calibration/config/create_schema.py</code> <pre><code>def validate(self) -&gt; list[str]:\n    \"\"\"Validate configuration and return a list of error messages.\n\n    Returns\n    -------\n    list of str\n        Validation errors (empty when the config is valid).\n    \"\"\"\n    errors: list[str] = []\n\n    if not self.aoi.exists():\n        errors.append(f\"AOI file not found: {self.aoi}\")\n\n    if self.grid.resolution &lt;= 0:\n        errors.append(f\"grid.resolution must be positive, got {self.grid.resolution}\")\n\n    errors.extend(self._validate_elevation())\n    errors.extend(self._validate_subgrid())\n\n    for ref in self.grid.refinement:\n        if not ref.polygon.exists():\n            errors.append(f\"refinement polygon not found: {ref.polygon}\")\n        if ref.level &lt; 1:\n            errors.append(f\"refinement level must be &gt;= 1, got {ref.level}\")\n\n    if self.nwm_discharge is not None:\n        nd = self.nwm_discharge\n        if not nd.hydrofabric_gpkg.exists():\n            errors.append(f\"nwm_discharge.hydrofabric_gpkg not found: {nd.hydrofabric_gpkg}\")\n        if not nd.flowpath_ids:\n            errors.append(\"nwm_discharge.flowpath_ids must contain at least one ID\")\n        if nd.coastal_domain not in _VALID_NWM_DOMAINS:\n            errors.append(\n                f\"nwm_discharge.coastal_domain must be one of \"\n                f\"{sorted(_VALID_NWM_DOMAINS)}, got '{nd.coastal_domain}'\"\n            )\n\n    return errors\n</code></pre>"},{"location":"reference/api/#gridconfig","title":"GridConfig","text":""},{"location":"reference/api/#coastal_calibration.config.create_schema.GridConfig","title":"GridConfig  <code>dataclass</code>","text":"<pre><code>GridConfig(\n    resolution=50.0,\n    crs=\"utm\",\n    rotated=True,\n    refinement=list(),\n)\n</code></pre> <p>Grid generation configuration.</p>"},{"location":"reference/api/#elevationconfig","title":"ElevationConfig","text":""},{"location":"reference/api/#coastal_calibration.config.create_schema.ElevationConfig","title":"ElevationConfig  <code>dataclass</code>","text":"<pre><code>ElevationConfig(\n    datasets=(\n        lambda: [\n            ElevationDataset(name=\"copdem30\", zmin=0.001),\n            ElevationDataset(name=\"gebco\", zmin=-20000),\n        ]\n    )(),\n    buffer_cells=1,\n)\n</code></pre> <p>Elevation and bathymetry configuration.</p>"},{"location":"reference/api/#maskconfig","title":"MaskConfig","text":""},{"location":"reference/api/#coastal_calibration.config.create_schema.MaskConfig","title":"MaskConfig  <code>dataclass</code>","text":"<pre><code>MaskConfig(\n    zmin=-5.0, boundary_zmax=-5.0, reset_bounds=True\n)\n</code></pre> <p>Active-cell mask and boundary configuration.</p>"},{"location":"reference/api/#subgridconfig","title":"SubgridConfig","text":""},{"location":"reference/api/#coastal_calibration.config.create_schema.SubgridConfig","title":"SubgridConfig  <code>dataclass</code>","text":"<pre><code>SubgridConfig(\n    nr_subgrid_pixels=5,\n    lulc_dataset=\"esa_worldcover_2021\",\n    reclass_table=None,\n    manning_land=0.04,\n    manning_sea=0.02,\n)\n</code></pre> <p>Subgrid table configuration.</p> <p>Roughness parameters are included here because for quadtree grids the Manning coefficients are embedded directly in the subgrid tables.</p>"},{"location":"reference/api/#nwmdischargeconfig","title":"NWMDischargeConfig","text":""},{"location":"reference/api/#coastal_calibration.config.create_schema.NWMDischargeConfig","title":"NWMDischargeConfig  <code>dataclass</code>","text":"<pre><code>NWMDischargeConfig(\n    hydrofabric_gpkg,\n    flowpaths_layer,\n    flowpath_id_column,\n    flowpath_ids=list(),\n    coastal_domain=\"conus\",\n)\n</code></pre> <p>NWM discharge source point configuration.</p> <p>Derives discharge source points by intersecting NWM hydrofabric flowpaths with the model AOI boundary.  The intersection points are added as SFINCS discharge source locations.</p>"},{"location":"reference/api/#workflow-runners","title":"Workflow Runners","text":""},{"location":"reference/api/#coastalcalibrunner","title":"CoastalCalibRunner","text":""},{"location":"reference/api/#coastal_calibration.runner.CoastalCalibRunner","title":"CoastalCalibRunner","text":"<pre><code>CoastalCalibRunner(config)\n</code></pre> <p>Main workflow runner for coastal model calibration.</p> <p>This class orchestrates the entire calibration workflow, managing stage execution and progress monitoring.</p> <p>Supports both SCHISM (<code>model=\"schism\"</code>, default) and SFINCS (<code>model=\"sfincs\"</code>) pipelines.  The model type is selected via <code>config.model</code>.</p> <p>Initialize the workflow runner.</p> PARAMETER DESCRIPTION <code>config</code> <p>Coastal calibration configuration.</p> <p> TYPE: <code>CoastalCalibConfig</code> </p> Source code in <code>src/coastal_calibration/runner.py</code> <pre><code>def __init__(self, config: CoastalCalibConfig) -&gt; None:\n    \"\"\"Initialize the workflow runner.\n\n    Parameters\n    ----------\n    config : CoastalCalibConfig\n        Coastal calibration configuration.\n    \"\"\"\n    self.config = config\n\n    # Ensure log directory exists early so file logging can start.\n    config.paths.work_dir.mkdir(parents=True, exist_ok=True)\n\n    # Set up file logging *before* creating the monitor so that\n    # every message (including third-party) is captured on disk.\n    if not config.monitoring.log_file:\n        log_path = generate_log_path(config.paths.work_dir)\n        configure_logger(file=str(log_path), file_level=\"DEBUG\")\n\n    # Silence noisy third-party loggers (HydroMT, xarray, ...)\n    silence_third_party_loggers()\n\n    self.monitor = WorkflowMonitor(config.monitoring)\n    self._stages: dict[str, WorkflowStage] = {}\n    self._results: dict[str, Any] = {}\n</code></pre>"},{"location":"reference/api/#coastal_calibration.runner.CoastalCalibRunner.validate","title":"validate","text":"<pre><code>validate()\n</code></pre> <p>Validate configuration and prerequisites.</p> RETURNS DESCRIPTION <code>list of str</code> <p>List of validation error messages (empty if valid).</p> Source code in <code>src/coastal_calibration/runner.py</code> <pre><code>def validate(self) -&gt; list[str]:\n    \"\"\"Validate configuration and prerequisites.\n\n    Returns\n    -------\n    list of str\n        List of validation error messages (empty if valid).\n    \"\"\"\n    errors = []\n\n    config_errors = self.config.validate()\n    errors.extend(config_errors)\n\n    self._init_stages()\n    for name, stage in self._stages.items():\n        stage_errors = stage.validate()\n        errors.extend(f\"[{name}] {error}\" for error in stage_errors)\n\n    return errors\n</code></pre>"},{"location":"reference/api/#coastal_calibration.runner.CoastalCalibRunner.run","title":"run","text":"<pre><code>run(start_from=None, stop_after=None, dry_run=False)\n</code></pre> <p>Execute the calibration workflow.</p> PARAMETER DESCRIPTION <code>start_from</code> <p>Stage name to start from (skip earlier stages).</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>stop_after</code> <p>Stage name to stop after (skip later stages).</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>dry_run</code> <p>If True, validate but don't execute.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>WorkflowResult</code> <p>Result with execution details.</p> Source code in <code>src/coastal_calibration/runner.py</code> <pre><code>def run(\n    self,\n    start_from: str | None = None,\n    stop_after: str | None = None,\n    dry_run: bool = False,\n) -&gt; WorkflowResult:\n    \"\"\"Execute the calibration workflow.\n\n    Parameters\n    ----------\n    start_from : str, optional\n        Stage name to start from (skip earlier stages).\n    stop_after : str, optional\n        Stage name to stop after (skip later stages).\n    dry_run : bool, default False\n        If True, validate but don't execute.\n\n    Returns\n    -------\n    WorkflowResult\n        Result with execution details.\n    \"\"\"\n    start_time = datetime.now()\n    stages_completed: list[str] = []\n    stages_failed: list[str] = []\n    outputs: dict[str, Any] = {}\n    errors: list[str] = []\n\n    validation_errors = self.validate()\n    # When resuming mid-pipeline, verify that earlier stages completed.\n    if not validation_errors and start_from:\n        validation_errors = self._check_prerequisites(start_from)\n    if validation_errors:\n        return WorkflowResult(\n            success=False,\n            job_id=None,\n            start_time=start_time,\n            end_time=datetime.now(),\n            stages_completed=[],\n            stages_failed=[],\n            outputs={},\n            errors=validation_errors,\n        )\n\n    if dry_run:\n        self.monitor.info(\"Dry run mode - validation passed, no execution\")\n        return WorkflowResult(\n            success=True,\n            job_id=None,\n            start_time=start_time,\n            end_time=datetime.now(),\n            stages_completed=[],\n            stages_failed=[],\n            outputs={\"dry_run\": True},\n            errors=[],\n        )\n\n    self.monitor.register_stages(self.STAGE_ORDER)\n    self.monitor.start_workflow()\n    self.monitor.info(\"-\" * 40)\n\n    stages_to_run = self._get_stages_to_run(start_from, stop_after)\n\n    current_stage = \"\"\n    try:\n        for current_stage in stages_to_run:\n            stage = self._stages[current_stage]\n\n            with self.monitor.stage_context(current_stage, stage.description):\n                result = stage.run()\n                self._results[current_stage] = result\n                outputs[current_stage] = result\n                stages_completed.append(current_stage)\n                self._save_stage_status(current_stage)\n\n        self.monitor.end_workflow(success=True)\n        success = True\n\n    except Exception as e:\n        self.monitor.error(f\"Workflow failed: {e}\")\n        self.monitor.end_workflow(success=False)\n        errors.append(str(e))\n        stages_failed.append(current_stage)\n        success = False\n\n    result = WorkflowResult(\n        success=success,\n        job_id=None,\n        start_time=start_time,\n        end_time=datetime.now(),\n        stages_completed=stages_completed,\n        stages_failed=stages_failed,\n        outputs=outputs,\n        errors=errors,\n    )\n\n    result_file = self.config.paths.work_dir / \"workflow_result.json\"\n    result.save(result_file)\n    self.monitor.save_progress(self.config.paths.work_dir / \"workflow_progress.json\")\n\n    return result\n</code></pre>"},{"location":"reference/api/#sfincscreator","title":"SfincsCreator","text":""},{"location":"reference/api/#coastal_calibration.creator.SfincsCreator","title":"SfincsCreator","text":"<pre><code>SfincsCreator(config)\n</code></pre> <p>Runner that orchestrates the SFINCS model creation pipeline.</p> <p>Mirrors :class:<code>~coastal_calibration.runner.CoastalCalibRunner</code> but operates on a :class:<code>SfincsCreateConfig</code> and delegates to :class:<code>~coastal_calibration.stages.sfincs_create.CreateStage</code> instances.</p> Source code in <code>src/coastal_calibration/creator.py</code> <pre><code>def __init__(self, config: SfincsCreateConfig) -&gt; None:\n    self.config = config\n\n    # Ensure the output directory exists early so file logging can start.\n    config.output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Set up file logging before creating the monitor.\n    if not config.monitoring.log_file:\n        log_path = generate_log_path(config.output_dir, prefix=\"sfincs-create\")\n        configure_logger(file=str(log_path), file_level=\"DEBUG\")\n\n    silence_third_party_loggers()\n\n    self.monitor = WorkflowMonitor(config.monitoring)\n    self._stages: dict[str, CreateStage] = {}\n    self._results: dict[str, Any] = {}\n</code></pre>"},{"location":"reference/api/#coastal_calibration.creator.SfincsCreator.run","title":"run","text":"<pre><code>run(start_from=None, stop_after=None, dry_run=False)\n</code></pre> <p>Execute the SFINCS model creation workflow.</p> PARAMETER DESCRIPTION <code>start_from</code> <p>Stage name to start from (skip earlier stages).</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>stop_after</code> <p>Stage name to stop after (skip later stages).</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>dry_run</code> <p>If True, validate but don't execute.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>WorkflowResult</code> <p>Result with execution details.</p> Source code in <code>src/coastal_calibration/creator.py</code> <pre><code>def run(\n    self,\n    start_from: str | None = None,\n    stop_after: str | None = None,\n    dry_run: bool = False,\n) -&gt; WorkflowResult:\n    \"\"\"Execute the SFINCS model creation workflow.\n\n    Parameters\n    ----------\n    start_from : str, optional\n        Stage name to start from (skip earlier stages).\n    stop_after : str, optional\n        Stage name to stop after (skip later stages).\n    dry_run : bool, default False\n        If True, validate but don't execute.\n\n    Returns\n    -------\n    WorkflowResult\n        Result with execution details.\n    \"\"\"\n    start_time = datetime.now()\n    stages_completed: list[str] = []\n    stages_failed: list[str] = []\n    outputs: dict[str, Any] = {}\n    errors: list[str] = []\n\n    validation_errors = self.validate()\n    if not validation_errors and start_from:\n        validation_errors = self._check_prerequisites(start_from)\n    if validation_errors:\n        return WorkflowResult(\n            success=False,\n            job_id=None,\n            start_time=start_time,\n            end_time=datetime.now(),\n            stages_completed=[],\n            stages_failed=[],\n            outputs={},\n            errors=validation_errors,\n        )\n\n    if dry_run:\n        self.monitor.info(\"Dry run mode - validation passed, no execution\")\n        return WorkflowResult(\n            success=True,\n            job_id=None,\n            start_time=start_time,\n            end_time=datetime.now(),\n            stages_completed=[],\n            stages_failed=[],\n            outputs={\"dry_run\": True},\n            errors=[],\n        )\n\n    self.monitor.register_stages(self.stage_order)\n    self.monitor.start_workflow()\n    self.monitor.info(\"-\" * 40)\n\n    stages_to_run = self._get_stages_to_run(start_from, stop_after)\n\n    # When resuming from a later stage, load the existing model so\n    # that stages which reference ``self.sfincs`` can find it.\n    if start_from and \"create_grid\" not in stages_to_run:\n        _load_existing_model(self.config)\n\n    current_stage = \"\"\n    try:\n        for current_stage in stages_to_run:\n            stage = self._stages[current_stage]\n\n            with self.monitor.stage_context(current_stage, stage.description):\n                result = stage.run()\n                self._results[current_stage] = result\n                outputs[current_stage] = result\n                stages_completed.append(current_stage)\n                self._save_stage_status(current_stage)\n\n        self.monitor.end_workflow(success=True)\n        success = True\n\n    except Exception as e:\n        self.monitor.error(f\"Workflow failed: {e}\")\n        self.monitor.end_workflow(success=False)\n        errors.append(str(e))\n        stages_failed.append(current_stage)\n        success = False\n\n    result = WorkflowResult(\n        success=success,\n        job_id=None,\n        start_time=start_time,\n        end_time=datetime.now(),\n        stages_completed=stages_completed,\n        stages_failed=stages_failed,\n        outputs=outputs,\n        errors=errors,\n    )\n\n    result_file = self.config.output_dir / \"create_result.json\"\n    result.save(result_file)\n    self.monitor.save_progress(self.config.output_dir / \"create_progress.json\")\n\n    return result\n</code></pre>"},{"location":"reference/api/#workflowresult","title":"WorkflowResult","text":""},{"location":"reference/api/#coastal_calibration.runner.WorkflowResult","title":"WorkflowResult  <code>dataclass</code>","text":"<pre><code>WorkflowResult(\n    success,\n    job_id,\n    start_time,\n    end_time,\n    stages_completed,\n    stages_failed,\n    outputs,\n    errors,\n)\n</code></pre> <p>Result of a workflow execution.</p>"},{"location":"reference/api/#coastal_calibration.runner.WorkflowResult.duration_seconds","title":"duration_seconds  <code>property</code>","text":"<pre><code>duration_seconds\n</code></pre> <p>Get workflow duration in seconds.</p>"},{"location":"reference/api/#coastal_calibration.runner.WorkflowResult.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Convert to dictionary.</p> Source code in <code>src/coastal_calibration/runner.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary.\"\"\"\n    return {\n        \"success\": self.success,\n        \"job_id\": self.job_id,\n        \"start_time\": self.start_time.isoformat(),\n        \"end_time\": self.end_time.isoformat() if self.end_time else None,\n        \"duration_seconds\": self.duration_seconds,\n        \"stages_completed\": self.stages_completed,\n        \"stages_failed\": self.stages_failed,\n        \"outputs\": self.outputs,\n        \"errors\": self.errors,\n    }\n</code></pre>"},{"location":"reference/api/#coastal_calibration.runner.WorkflowResult.save","title":"save","text":"<pre><code>save(path)\n</code></pre> <p>Save result to JSON file.</p> PARAMETER DESCRIPTION <code>path</code> <p>Path to output JSON file. Parent directories will be created if they don't exist.</p> <p> TYPE: <code>Path or str</code> </p> Source code in <code>src/coastal_calibration/runner.py</code> <pre><code>def save(self, path: Path | str) -&gt; None:\n    \"\"\"Save result to JSON file.\n\n    Parameters\n    ----------\n    path : Path or str\n        Path to output JSON file. Parent directories will be created\n        if they don't exist.\n    \"\"\"\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(json.dumps(self.to_dict(), indent=2))\n</code></pre>"},{"location":"reference/api/#downloader","title":"Downloader","text":""},{"location":"reference/api/#validate_date_ranges","title":"validate_date_ranges","text":""},{"location":"reference/api/#coastal_calibration.downloader.validate_date_ranges","title":"validate_date_ranges","text":"<pre><code>validate_date_ranges(\n    start_time,\n    end_time,\n    meteo_source,\n    coastal_source,\n    domain,\n)\n</code></pre> <p>Validate that requested dates are within available ranges.</p> Source code in <code>src/coastal_calibration/downloader.py</code> <pre><code>def validate_date_ranges(\n    start_time: datetime,\n    end_time: datetime,\n    meteo_source: str,\n    coastal_source: str,\n    domain: str,\n) -&gt; list[str]:\n    \"\"\"Validate that requested dates are within available ranges.\"\"\"\n    errors: list[str] = []\n\n    meteo_range = get_date_range(meteo_source, domain)\n    if meteo_range:\n        error = meteo_range.validate(start_time, end_time)\n        if error:\n            errors.append(error)\n\n    if coastal_source != \"tpxo\":\n        coastal_range = get_date_range(coastal_source, domain)\n        if coastal_range:\n            error = coastal_range.validate(start_time, end_time)\n            if error:\n                errors.append(error)\n\n    return errors\n</code></pre>"},{"location":"reference/api/#noaa-co-ops-api","title":"NOAA CO-OPS API","text":""},{"location":"reference/api/#coopsapiclient","title":"COOPSAPIClient","text":""},{"location":"reference/api/#coastal_calibration.coops_api.COOPSAPIClient","title":"COOPSAPIClient","text":"<pre><code>COOPSAPIClient(timeout=120)\n</code></pre> <p>Client for interacting with NOAA CO-OPS API.</p> <p>Initialize COOPS API client.</p> PARAMETER DESCRIPTION <code>timeout</code> <p>Request timeout in seconds, by default 120</p> <p> TYPE: <code>int</code> DEFAULT: <code>120</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If plot optional dependencies are not installed.</p> Source code in <code>src/coastal_calibration/coops_api.py</code> <pre><code>def __init__(self, timeout: int = 120) -&gt; None:\n    \"\"\"Initialize COOPS API client.\n\n    Parameters\n    ----------\n    timeout : int, optional\n        Request timeout in seconds, by default 120\n\n    Raises\n    ------\n    ImportError\n        If plot optional dependencies are not installed.\n    \"\"\"\n    _check_plot_deps()\n    self.timeout = timeout\n    self._stations_metadata = self._get_stations_metadata()\n</code></pre>"},{"location":"reference/api/#coastal_calibration.coops_api.COOPSAPIClient.stations_metadata","title":"stations_metadata  <code>property</code>","text":"<pre><code>stations_metadata\n</code></pre> <p>Get metadata for all water level stations as a GeoDataFrame.</p> RETURNS DESCRIPTION <code>GeoDataFrame</code> <p>GeoDataFrame with station metadata and Point geometries.</p>"},{"location":"reference/api/#coastal_calibration.coops_api.COOPSAPIClient.validate_parameters","title":"validate_parameters","text":"<pre><code>validate_parameters(\n    product, datum, units, time_zone, interval\n)\n</code></pre> <p>Validate API parameters.</p> PARAMETER DESCRIPTION <code>product</code> <p>Data product type</p> <p> TYPE: <code>str</code> </p> <code>datum</code> <p>Vertical datum</p> <p> TYPE: <code>str</code> </p> <code>units</code> <p>Unit system</p> <p> TYPE: <code>str</code> </p> <code>time_zone</code> <p>Time zone</p> <p> TYPE: <code>str</code> </p> <code>interval</code> <p>Time interval for predictions</p> <p> TYPE: <code>str | int | None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If any parameter is invalid</p> Source code in <code>src/coastal_calibration/coops_api.py</code> <pre><code>def validate_parameters(\n    self,\n    product: str,\n    datum: str,\n    units: str,\n    time_zone: str,\n    interval: str | int | None,\n) -&gt; None:\n    \"\"\"Validate API parameters.\n\n    Parameters\n    ----------\n    product : str\n        Data product type\n    datum : str\n        Vertical datum\n    units : str\n        Unit system\n    time_zone : str\n        Time zone\n    interval : str | int | None\n        Time interval for predictions\n\n    Raises\n    ------\n    ValueError\n        If any parameter is invalid\n    \"\"\"\n    if product not in self.valid_products:\n        raise ValueError(\n            f\"Invalid product '{product}'. Must be one of: {', '.join(self.valid_products)}\"\n        )\n\n    if datum.upper() not in self.valid_datums:\n        raise ValueError(\n            f\"Invalid datum '{datum}'. Must be one of: {', '.join(self.valid_datums)}\"\n        )\n\n    if units not in self.valid_units:\n        raise ValueError(\n            f\"Invalid units '{units}'. Must be one of: {', '.join(self.valid_units)}\"\n        )\n\n    if time_zone not in self.valid_timezones:\n        raise ValueError(\n            f\"Invalid time_zone '{time_zone}'. Must be one of: {', '.join(self.valid_timezones)}\"\n        )\n\n    if (\n        product == \"predictions\"\n        and interval is not None\n        and str(interval) not in self.valid_intervals\n    ):\n        raise ValueError(\n            f\"Invalid interval '{interval}' for predictions. \"\n            f\"Must be one of: {', '.join(self.valid_intervals)}\"\n        )\n</code></pre>"},{"location":"reference/api/#coastal_calibration.coops_api.COOPSAPIClient.build_url","title":"build_url","text":"<pre><code>build_url(\n    station_id,\n    begin_date,\n    end_date,\n    product,\n    datum,\n    units,\n    time_zone,\n    interval,\n)\n</code></pre> <p>Build API request URL for a station.</p> PARAMETER DESCRIPTION <code>station_id</code> <p>Station ID</p> <p> TYPE: <code>str</code> </p> <code>begin_date</code> <p>Start date</p> <p> TYPE: <code>str</code> </p> <code>end_date</code> <p>End date</p> <p> TYPE: <code>str</code> </p> <code>product</code> <p>Data product</p> <p> TYPE: <code>str</code> </p> <code>datum</code> <p>Vertical datum</p> <p> TYPE: <code>str</code> </p> <code>units</code> <p>Unit system</p> <p> TYPE: <code>str</code> </p> <code>time_zone</code> <p>Time zone</p> <p> TYPE: <code>str</code> </p> <code>interval</code> <p>Time interval for predictions</p> <p> TYPE: <code>str | int | None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Complete API request URL</p> Source code in <code>src/coastal_calibration/coops_api.py</code> <pre><code>def build_url(\n    self,\n    station_id: str,\n    begin_date: str,\n    end_date: str,\n    product: str,\n    datum: str,\n    units: str,\n    time_zone: str,\n    interval: str | int | None,\n) -&gt; str:\n    \"\"\"Build API request URL for a station.\n\n    Parameters\n    ----------\n    station_id : str\n        Station ID\n    begin_date : str\n        Start date\n    end_date : str\n        End date\n    product : str\n        Data product\n    datum : str\n        Vertical datum\n    units : str\n        Unit system\n    time_zone : str\n        Time zone\n    interval : str | int | None, optional\n        Time interval for predictions\n\n    Returns\n    -------\n    str\n        Complete API request URL\n    \"\"\"\n    params = {\n        \"begin_date\": begin_date,\n        \"end_date\": end_date,\n        \"station\": station_id,\n        \"product\": product,\n        \"datum\": datum,\n        \"units\": units,\n        \"time_zone\": time_zone,\n        \"format\": \"json\",\n        \"application\": \"coastal_calibration_coops\",\n    }\n\n    if product == \"predictions\" and interval is not None:\n        params[\"interval\"] = str(interval)\n\n    query_parts = [f\"{k}={v}\" for k, v in params.items()]\n    return f\"{self.base_url}?{'&amp;'.join(query_parts)}\"\n</code></pre>"},{"location":"reference/api/#coastal_calibration.coops_api.COOPSAPIClient.fetch_data","title":"fetch_data","text":"<pre><code>fetch_data(urls)\n</code></pre> <p>Fetch data from API for multiple URLs.</p> PARAMETER DESCRIPTION <code>urls</code> <p>List of API request URLs</p> <p> TYPE: <code>list[str]</code> </p> RETURNS DESCRIPTION <code>list[dict | None]</code> <p>List of JSON responses (None for failed requests)</p> Source code in <code>src/coastal_calibration/coops_api.py</code> <pre><code>def fetch_data(self, urls: list[str]) -&gt; list[dict[str, Any] | None]:\n    \"\"\"Fetch data from API for multiple URLs.\n\n    Parameters\n    ----------\n    urls : list[str]\n        List of API request URLs\n\n    Returns\n    -------\n    list[dict | None]\n        List of JSON responses (None for failed requests)\n    \"\"\"\n    logger.info(\"  Fetching data from %d station(s)\", len(urls))\n\n    return fetch(\n        urls,\n        \"json\",\n        request_method=\"get\",\n        timeout=self.timeout,\n        raise_status=False,\n    )\n</code></pre>"},{"location":"reference/api/#coastal_calibration.coops_api.COOPSAPIClient.get_datums","title":"get_datums","text":"<pre><code>get_datums(station_ids: str) -&gt; StationDatum\n</code></pre><pre><code>get_datums(station_ids: list[str]) -&gt; list[StationDatum]\n</code></pre> <pre><code>get_datums(station_ids)\n</code></pre> <p>Retrieve datum information for one or more stations.</p> PARAMETER DESCRIPTION <code>station_ids</code> <p>Single station ID or list of station IDs</p> <p> TYPE: <code>str | list[str]</code> </p> RETURNS DESCRIPTION <code>StationDatum | list[StationDatum]</code> <p>Single StationDatum object if input is str, list of StationDatum if input is list</p> RAISES DESCRIPTION <code>ValueError</code> <p>If no valid datum data is returned for any station.</p> Source code in <code>src/coastal_calibration/coops_api.py</code> <pre><code>def get_datums(self, station_ids: str | list[str]) -&gt; StationDatum | list[StationDatum]:\n    \"\"\"Retrieve datum information for one or more stations.\n\n    Parameters\n    ----------\n    station_ids : str | list[str]\n        Single station ID or list of station IDs\n\n    Returns\n    -------\n    StationDatum | list[StationDatum]\n        Single StationDatum object if input is str,\n        list of StationDatum if input is list\n\n    Raises\n    ------\n    ValueError\n        If no valid datum data is returned for any station.\n    \"\"\"\n    import numpy as np\n\n    single_input = isinstance(station_ids, str)\n    ids: list[str] = [station_ids] if isinstance(station_ids, str) else list(station_ids)\n\n    datum_base_url = \"https://api.tidesandcurrents.noaa.gov/mdapi/prod/webapi/stations\"\n    urls = [f\"{datum_base_url}/{sid}/datums.json\" for sid in ids]\n    logger.info(\"  Fetching datum information for %d station(s)\", len(ids))\n    responses = self.fetch_data(urls)\n    datum_objects = []\n    for station_id, response in zip(ids, responses, strict=False):\n        if response is None:\n            logger.warning(\"  No datum data returned for station %s\", station_id)\n            continue\n\n        if \"error\" in response:\n            logger.warning(\n                \"  Datum API error for station %s: %s\",\n                station_id,\n                response[\"error\"].get(\"message\", \"Unknown error\"),\n            )\n            continue\n\n        raw_datums = response.get(\"datums\") or []\n        datum_values = [\n            DatumValue(\n                name=datum_dict.get(\"name\", \"\"),\n                description=datum_dict.get(\"description\", \"\"),\n                value=np.float64(datum_dict.get(\"value\", np.nan)),\n            )\n            for datum_dict in raw_datums\n        ]\n\n        station_datum = StationDatum(\n            station_id=station_id,\n            accepted=response.get(\"accepted\", \"\"),\n            superseded=response.get(\"superseded\", \"\"),\n            epoch=response.get(\"epoch\", \"\"),\n            units=response.get(\"units\", \"\"),\n            orthometric_datum=response.get(\"OrthometricDatum\", \"\"),\n            datums=datum_values,\n            lat=np.float64(response.get(\"LAT\", np.nan)),\n            lat_date=response.get(\"LATdate\", \"\"),\n            lat_time=response.get(\"LATtime\", \"\"),\n            hat=np.float64(response.get(\"HAT\", np.nan)),\n            hat_date=response.get(\"HATdate\", \"\"),\n            hat_time=response.get(\"HATtime\", \"\"),\n            min_value=np.float64(response.get(\"min\", np.nan)),\n            min_date=response.get(\"mindate\", \"\"),\n            min_time=response.get(\"mintime\", \"\"),\n            max_value=np.float64(response.get(\"max\", np.nan)),\n            max_date=response.get(\"maxdate\", \"\"),\n            max_time=response.get(\"maxtime\", \"\"),\n            datum_analysis_period=response.get(\"DatumAnalysisPeriod\") or [],\n            ngs_link=response.get(\"NGSLink\", \"\"),\n            ctrl_station=response.get(\"ctrlStation\", \"\"),\n        )\n\n        datum_objects.append(station_datum)\n\n    if not datum_objects:\n        raise ValueError(\"No valid datum data returned for any station\")\n\n    if single_input:\n        return datum_objects[0]\n    return datum_objects\n</code></pre>"},{"location":"reference/api/#query_coops_byids","title":"query_coops_byids","text":""},{"location":"reference/api/#coastal_calibration.coops_api.query_coops_byids","title":"query_coops_byids","text":"<pre><code>query_coops_byids(\n    station_ids,\n    begin_date,\n    end_date,\n    *,\n    product=\"water_level\",\n    datum=\"MLLW\",\n    units=\"metric\",\n    time_zone=\"gmt\",\n    interval=None,\n)\n</code></pre> <p>Fetch water level data from NOAA CO-OPS API for multiple stations.</p> PARAMETER DESCRIPTION <code>station_ids</code> <p>List of station IDs to retrieve data for.</p> <p> TYPE: <code>list[str]</code> </p> <code>begin_date</code> <p>Start date in format: yyyyMMdd, yyyyMMdd HH:mm, MM/dd/yyyy, or MM/dd/yyyy HH:mm</p> <p> TYPE: <code>str</code> </p> <code>end_date</code> <p>End date in same format as begin_date.</p> <p> TYPE: <code>str</code> </p> <code>product</code> <p>Data product to retrieve, by default <code>water_level</code>.</p> <p> TYPE: <code>('water_level', 'hourly_height', 'high_low', 'predictions')</code> DEFAULT: <code>\"water_level\"</code> </p> <code>datum</code> <p>Vertical datum for water levels, by default \"MLLW\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'MLLW'</code> </p> <code>units</code> <p>Units for data, by default \"metric\".</p> <p> TYPE: <code>('metric', 'english')</code> DEFAULT: <code>\"metric\"</code> </p> <code>time_zone</code> <p>Time zone for returned data, by default \"gmt\".</p> <p> TYPE: <code>('gmt', 'lst', 'lst_ldt')</code> DEFAULT: <code>\"gmt\"</code> </p> <code>interval</code> <p>Time interval for predictions product only, by default None.</p> <p> TYPE: <code>str | int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Dataset</code> <p>Dataset containing water level data with dimensions (time, station).</p> RAISES DESCRIPTION <code>ValueError</code> <p>If invalid parameters are provided or if API returns errors.</p> Source code in <code>src/coastal_calibration/coops_api.py</code> <pre><code>def query_coops_byids(\n    station_ids: list[str],\n    begin_date: str,\n    end_date: str,\n    *,\n    product: Literal[\n        \"water_level\",\n        \"hourly_height\",\n        \"high_low\",\n        \"predictions\",\n    ] = \"water_level\",\n    datum: str = \"MLLW\",\n    units: Literal[\"metric\", \"english\"] = \"metric\",\n    time_zone: Literal[\"gmt\", \"lst\", \"lst_ldt\"] = \"gmt\",\n    interval: str | int | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Fetch water level data from NOAA CO-OPS API for multiple stations.\n\n    Parameters\n    ----------\n    station_ids : list[str]\n        List of station IDs to retrieve data for.\n    begin_date : str\n        Start date in format: yyyyMMdd, yyyyMMdd HH:mm, MM/dd/yyyy, or MM/dd/yyyy HH:mm\n    end_date : str\n        End date in same format as begin_date.\n    product : {\"water_level\", \"hourly_height\", \"high_low\", \"predictions\"}, optional\n        Data product to retrieve, by default ``water_level``.\n    datum : str, optional\n        Vertical datum for water levels, by default \"MLLW\".\n    units : {\"metric\", \"english\"}, optional\n        Units for data, by default \"metric\".\n    time_zone : {\"gmt\", \"lst\", \"lst_ldt\"}, optional\n        Time zone for returned data, by default \"gmt\".\n    interval : str | int | None, optional\n        Time interval for predictions product only, by default None.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset containing water level data with dimensions (time, station).\n\n    Raises\n    ------\n    ValueError\n        If invalid parameters are provided or if API returns errors.\n    \"\"\"\n    client = COOPSAPIClient()\n    client.validate_parameters(product, datum, units, time_zone, interval)\n    begin_dt = client.parse_date(begin_date)\n    end_dt = client.parse_date(end_date)\n\n    if end_dt &lt;= begin_dt:\n        raise ValueError(\"end_date must be after begin_date\")\n\n    begin_str = begin_dt.strftime(\"%Y%m%d %H:%M\")\n    end_str = end_dt.strftime(\"%Y%m%d %H:%M\")\n\n    logger.info(\n        \"  Requesting %s data for %d station(s) from %s to %s\",\n        product,\n        len(station_ids),\n        begin_str,\n        end_str,\n    )\n\n    urls = [\n        client.build_url(\n            station_id=station_id,\n            begin_date=begin_str,\n            end_date=end_str,\n            product=product,\n            datum=datum,\n            units=units,\n            time_zone=time_zone,\n            interval=interval,\n        )\n        for station_id in station_ids\n    ]\n\n    return _process_responses(\n        responses=client.fetch_data(urls),\n        station_ids=station_ids,\n        product=product,\n        datum=datum,\n        units=units,\n        time_zone=time_zone,\n    )\n</code></pre>"},{"location":"reference/api/#query_coops_bygeometry","title":"query_coops_bygeometry","text":""},{"location":"reference/api/#coastal_calibration.coops_api.query_coops_bygeometry","title":"query_coops_bygeometry","text":"<pre><code>query_coops_bygeometry(\n    geometry,\n    begin_date,\n    end_date,\n    *,\n    product=\"water_level\",\n    datum=\"MLLW\",\n    units=\"metric\",\n    time_zone=\"gmt\",\n    interval=None,\n)\n</code></pre> <p>Fetch water level data from NOAA CO-OPS API for stations within a geometry.</p> PARAMETER DESCRIPTION <code>geometry</code> <p>Geometry to select stations within (Point, Polygon, etc.)</p> <p> TYPE: <code>BaseGeometry</code> </p> <code>begin_date</code> <p>Start date in format: yyyyMMdd, yyyyMMdd HH:mm, MM/dd/yyyy, or MM/dd/yyyy HH:mm</p> <p> TYPE: <code>str</code> </p> <code>end_date</code> <p>End date in same format as begin_date.</p> <p> TYPE: <code>str</code> </p> <code>product</code> <p>Data product to retrieve, by default <code>water_level</code>.</p> <p> TYPE: <code>('water_level', 'hourly_height', 'high_low', 'predictions')</code> DEFAULT: <code>\"water_level\"</code> </p> <code>datum</code> <p>Vertical datum for water levels, by default \"MLLW\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'MLLW'</code> </p> <code>units</code> <p>Units for data, by default \"metric\".</p> <p> TYPE: <code>('metric', 'english')</code> DEFAULT: <code>\"metric\"</code> </p> <code>time_zone</code> <p>Time zone for returned data, by default \"gmt\".</p> <p> TYPE: <code>('gmt', 'lst', 'lst_ldt')</code> DEFAULT: <code>\"gmt\"</code> </p> <code>interval</code> <p>Time interval for predictions product only, by default None.</p> <p> TYPE: <code>str | int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Dataset</code> <p>Dataset containing water level data for stations within the geometry.</p> Source code in <code>src/coastal_calibration/coops_api.py</code> <pre><code>def query_coops_bygeometry(\n    geometry: shapely.geometry.base.BaseGeometry,\n    begin_date: str,\n    end_date: str,\n    *,\n    product: Literal[\n        \"water_level\",\n        \"hourly_height\",\n        \"high_low\",\n        \"predictions\",\n    ] = \"water_level\",\n    datum: str = \"MLLW\",\n    units: Literal[\"metric\", \"english\"] = \"metric\",\n    time_zone: Literal[\"gmt\", \"lst\", \"lst_ldt\"] = \"gmt\",\n    interval: str | int | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Fetch water level data from NOAA CO-OPS API for stations within a geometry.\n\n    Parameters\n    ----------\n    geometry : shapely.geometry.base.BaseGeometry\n        Geometry to select stations within (Point, Polygon, etc.)\n    begin_date : str\n        Start date in format: yyyyMMdd, yyyyMMdd HH:mm, MM/dd/yyyy, or MM/dd/yyyy HH:mm\n    end_date : str\n        End date in same format as begin_date.\n    product : {\"water_level\", \"hourly_height\", \"high_low\", \"predictions\"}, optional\n        Data product to retrieve, by default ``water_level``.\n    datum : str, optional\n        Vertical datum for water levels, by default \"MLLW\".\n    units : {\"metric\", \"english\"}, optional\n        Units for data, by default \"metric\".\n    time_zone : {\"gmt\", \"lst\", \"lst_ldt\"}, optional\n        Time zone for returned data, by default \"gmt\".\n    interval : str | int | None, optional\n        Time interval for predictions product only, by default None.\n\n    Returns\n    -------\n    xr.Dataset\n        Dataset containing water level data for stations within the geometry.\n    \"\"\"\n    import numpy as np\n    import shapely\n\n    client = COOPSAPIClient()\n    if not all(shapely.is_valid(np.atleast_1d(geometry))):\n        raise ValueError(\"Invalid geometry provided.\")\n\n    stations_gdf = client.stations_metadata\n    selected_stations = stations_gdf[stations_gdf.intersects(geometry)]\n\n    if selected_stations.empty:\n        raise ValueError(\"No stations found within the specified geometry and buffer.\")\n\n    station_ids = selected_stations[\"station_id\"].tolist()\n    return query_coops_byids(\n        station_ids,\n        begin_date,\n        end_date,\n        product=product,\n        datum=datum,\n        units=units,\n        time_zone=time_zone,\n        interval=interval,\n    )\n</code></pre>"},{"location":"reference/api/#type-aliases","title":"Type Aliases","text":"<pre><code># Model type\nModelType = Literal[\"schism\", \"sfincs\"]\n\n# Meteorological data source\nMeteoSource = Literal[\"nwm_retro\", \"nwm_ana\"]\n\n# Coastal domain identifier\nCoastalDomain = Literal[\"prvi\", \"hawaii\", \"atlgulf\", \"pacific\"]\n\n# Boundary condition source\nBoundarySource = Literal[\"tpxo\", \"stofs\"]\n\n# Logging level\nLogLevel = Literal[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"]\n</code></pre>"},{"location":"reference/api/#constants","title":"Constants","text":""},{"location":"reference/api/#default-paths","title":"Default Paths","text":"<pre><code>DEFAULT_SING_IMAGE_PATH = Path(\"/ngencerf-app/singularity/ngen-coastal.sif\")\nDEFAULT_PARM_DIR = Path(\"/ngen-test/coastal/ngwpc-coastal\")\nDEFAULT_NFS_MOUNT = Path(\"/ngen-test\")\nDEFAULT_CONDA_ENV_NAME = \"ngen_forcing_coastal\"\nDEFAULT_NWM_DIR = Path(\"/ngen-app/nwm.v3.0.6\")\nDEFAULT_OTPS_DIR = Path(\"/ngen-app/OTPSnc\")\n</code></pre>"},{"location":"reference/api/#default-path-templates","title":"Default Path Templates","text":"<pre><code>DEFAULT_WORK_DIR_TEMPLATE = (\n    \"/ngen-test/coastal/${user}/\"\n    \"${model}_${simulation.coastal_domain}_${boundary.source}_${simulation.meteo_source}/\"\n    \"${model}_${simulation.start_date}\"\n)\n\nDEFAULT_RAW_DOWNLOAD_DIR_TEMPLATE = (\n    \"/ngen-test/coastal/${user}/\"\n    \"${model}_${simulation.coastal_domain}_${boundary.source}_${simulation.meteo_source}/\"\n    \"raw_data\"\n)\n</code></pre>"},{"location":"reference/api/#model-registry","title":"Model Registry","text":"<pre><code>MODEL_REGISTRY: dict[str, type[ModelConfig]] = {\n    \"schism\": SchismModelConfig,\n    \"sfincs\": SfincsModelConfig,\n}\n</code></pre>"},{"location":"user-guide/cli/","title":"CLI Reference","text":"<p>The <code>coastal-calibration</code> command-line interface provides commands for managing SCHISM and SFINCS coastal model workflows.</p>"},{"location":"user-guide/cli/#global-options","title":"Global Options","text":"<pre><code>coastal-calibration --help\ncoastal-calibration --version\n</code></pre>"},{"location":"user-guide/cli/#commands","title":"Commands","text":""},{"location":"user-guide/cli/#init","title":"init","text":"<p>Create a minimal configuration file.</p> <pre><code>coastal-calibration init OUTPUT [OPTIONS]\n</code></pre> <p>Arguments:</p> Argument Description <code>OUTPUT</code> Path where the configuration will be written <p>Options:</p> Option Description Default <code>--domain</code> Coastal domain to use <code>pacific</code> <code>--force</code>, <code>-f</code> Overwrite existing file without prompt False <code>--model</code> Model type (<code>schism</code> or <code>sfincs</code>) <code>schism</code> <p>Examples:</p> <pre><code># Generate default SCHISM configuration\ncoastal-calibration init config.yaml\n\n# Generate configuration for a specific domain\ncoastal-calibration init pacific_config.yaml --domain pacific\n\n# Generate SFINCS configuration\ncoastal-calibration init sfincs_config.yaml --domain atlgulf --model sfincs\n\n# Overwrite existing file\ncoastal-calibration init config.yaml --force\n</code></pre>"},{"location":"user-guide/cli/#validate","title":"validate","text":"<p>Validate a configuration file for errors and warnings.</p> <pre><code>coastal-calibration validate &lt;config&gt;\n</code></pre> <p>Arguments:</p> Argument Description <code>config</code> Path to the configuration file <p>Examples:</p> <pre><code>coastal-calibration validate config.yaml\n</code></pre> <p>Output:</p> <pre><code>\u2713 Configuration is valid\n</code></pre> <p>Or with errors:</p> <pre><code>\u2717 Configuration has errors:\n  - simulation.duration_hours must be positive\n  - Simulation dates outside nwm_ana range (2018-09-17 to present)\n</code></pre>"},{"location":"user-guide/cli/#run","title":"run","text":"<p>Run the workflow directly (inside a SLURM job or for local testing).</p> <pre><code>coastal-calibration run &lt;config&gt; [OPTIONS]\n</code></pre> <p>Arguments:</p> Argument Description <code>config</code> Path to the configuration file <p>Options:</p> Option Description Default <code>--start-from</code> Stage to start from First <code>--stop-after</code> Stage to stop after Last <code>--dry-run</code> Validate configuration without executing False <p>Available Stages (SCHISM):</p> <ul> <li><code>download</code></li> <li><code>pre_forcing</code></li> <li><code>nwm_forcing</code></li> <li><code>post_forcing</code></li> <li><code>schism_obs</code></li> <li><code>update_params</code></li> <li><code>boundary_conditions</code></li> <li><code>pre_schism</code></li> <li><code>schism_run</code></li> <li><code>post_schism</code></li> <li><code>schism_plot</code></li> </ul> <p>Available Stages (SFINCS):</p> <ul> <li><code>download</code></li> <li><code>sfincs_symlinks</code></li> <li><code>sfincs_data_catalog</code></li> <li><code>sfincs_init</code></li> <li><code>sfincs_timing</code></li> <li><code>sfincs_forcing</code></li> <li><code>sfincs_obs</code></li> <li><code>sfincs_discharge</code></li> <li><code>sfincs_precip</code></li> <li><code>sfincs_wind</code></li> <li><code>sfincs_pressure</code></li> <li><code>sfincs_write</code></li> <li><code>sfincs_run</code></li> <li><code>sfincs_plot</code></li> </ul> <p>Examples:</p> <pre><code># Run entire workflow\ncoastal-calibration run config.yaml\n\n# Run only forcing stages (SCHISM)\ncoastal-calibration run config.yaml --start-from pre_forcing --stop-after post_forcing\n\n# Run only the model build (SFINCS)\ncoastal-calibration run config.yaml --stop-after sfincs_write\n\n# Run only the model execution (SFINCS)\ncoastal-calibration run config.yaml --start-from sfincs_run\n</code></pre>"},{"location":"user-guide/cli/#using-run-inside-a-slurm-job-heredoc-recommended","title":"Using <code>run</code> Inside a SLURM Job \u2014 Heredoc (Recommended)","text":"<p>The recommended way to run workflows on a cluster is to write an <code>sbatch</code> script with an inline YAML configuration using a heredoc. This keeps everything in a single, self-contained file and gives you full control over SLURM resource allocation:</p> <pre><code>#!/usr/bin/env bash\n#SBATCH --job-name=coastal_schism\n#SBATCH --partition=c5n-18xlarge\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=18\n#SBATCH --exclusive\n#SBATCH --output=slurm-%j.out\n\nCONFIG_FILE=\"/tmp/coastal_config_${SLURM_JOB_ID}.yaml\"\n\ncat &gt; \"${CONFIG_FILE}\" &lt;&lt;'EOF'\nmodel: schism\n\nsimulation:\n  start_date: 2021-01-01\n  duration_hours: 12\n  coastal_domain: hawaii\n  meteo_source: nwm_retro\n\nboundary:\n  source: tpxo\n\nmodel_config:\n  include_noaa_gages: true\nEOF\n\n/ngen-test/coastal-calibration/coastal-calibration run \"${CONFIG_FILE}\"\nrm -f \"${CONFIG_FILE}\"\n</code></pre> <p>Key points:</p> <ul> <li>Use the full NFS path: Compute nodes may not have <code>coastal-calibration</code> in their     <code>PATH</code>. Using the full path to the wrapper on the shared filesystem ensures the     command is always found.</li> <li><code>run</code> executes all stages sequentially: All stages execute locally on the     allocated nodes.</li> <li>Use <code>$SLURM_JOB_ID</code> in the config filename: Ensures uniqueness when multiple jobs     run concurrently.</li> <li>Use <code>&lt;&lt;'EOF'</code> (single-quoted heredoc): Prevents shell variable expansion inside     the YAML content.</li> <li>SCHISM uses multi-node MPI: <code>-N 2 --ntasks-per-node=18</code> matches the default     <code>model_config</code> values (2 nodes, 18 tasks/node).</li> <li>SFINCS uses single-node OpenMP: <code>-N 1 --ntasks=1</code> is sufficient since SFINCS is     OpenMP-only (parallelism is controlled by <code>model_config.omp_num_threads</code>).</li> </ul> <p>Complete examples for both models are available in <code>docs/examples/</code>:</p> <ul> <li><code>schism.sh</code>     \u2014 SCHISM multi-node MPI</li> <li><code>sfincs.sh</code>     \u2014 SFINCS single-node OpenMP</li> </ul>"},{"location":"user-guide/cli/#create","title":"create","text":"<p>Create a SFINCS quadtree model from an AOI polygon.</p> <pre><code>coastal-calibration create &lt;config&gt; [OPTIONS]\n</code></pre> <p>Arguments:</p> Argument Description <code>config</code> Path to the configuration file <p>Options:</p> Option Description Default <code>--start-from</code> Stage to start from First <code>--stop-after</code> Stage to stop after Last <code>--dry-run</code> Validate configuration without executing False <p>Available Stages:</p> <ul> <li><code>create_grid</code> \u2014 Create SFINCS grid from AOI polygon</li> <li><code>create_fetch_elevation</code> \u2014 Fetch NOAA topobathy DEM for AOI</li> <li><code>create_elevation</code> \u2014 Add elevation and bathymetry data</li> <li><code>create_mask</code> \u2014 Create active cell mask</li> <li><code>create_boundary</code> \u2014 Create water level boundary cells</li> <li><code>create_subgrid</code> \u2014 Create subgrid tables</li> <li><code>create_write</code> \u2014 Write SFINCS model to disk</li> </ul> <p>Examples:</p> <pre><code># Run entire creation workflow\ncoastal-calibration create create_config.yaml\n\n# Run only up to grid generation\ncoastal-calibration create create_config.yaml --stop-after create_grid\n\n# Resume from elevation stage\ncoastal-calibration create create_config.yaml --start-from create_elevation\n\n# Dry run to validate config\ncoastal-calibration create create_config.yaml --dry-run\n</code></pre>"},{"location":"user-guide/cli/#prepare-topobathy","title":"prepare-topobathy","text":"<p>Download a NWS 30 m topobathymetric DEM clipped to an AOI bounding box.</p> <pre><code>coastal-calibration prepare-topobathy &lt;aoi&gt; [OPTIONS]\n</code></pre> <p>Arguments:</p> Argument Description <code>aoi</code> Path to an AOI polygon file (GeoJSON, Shapefile, etc.) <p>Options:</p> Option Description Default <code>--domain</code> Coastal domain (<code>atlgulf</code>, <code>hi</code>, <code>prvi</code>, <code>pacific</code>, <code>ak</code>) required <code>--output-dir</code> Output directory for GeoTIFF + catalog Same as AOI location <code>--buffer-deg</code> BBox buffer in degrees 0.1 <p>Examples:</p> <pre><code># Download DEM for Atlantic/Gulf domain\ncoastal-calibration prepare-topobathy aoi.geojson --domain atlgulf\n\n# Download to a specific directory\ncoastal-calibration prepare-topobathy aoi.geojson --domain prvi --output-dir ./dem_data\n</code></pre>"},{"location":"user-guide/cli/#update-dem-index","title":"update-dem-index","text":"<p>Rebuild the NOAA DEM spatial index from S3 STAC metadata.</p> <pre><code>coastal-calibration update-dem-index [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--output</code> Write index to this path instead of the packaged location Packaged location <code>--max-datasets</code> Limit S3 scan to N datasets (for testing) All <p>Examples:</p> <pre><code># Rebuild the packaged index\ncoastal-calibration update-dem-index\n\n# Write to a custom path\ncoastal-calibration update-dem-index --output ./my_index.json\n</code></pre>"},{"location":"user-guide/cli/#stages","title":"stages","text":"<p>List all available workflow stages.</p> <pre><code>coastal-calibration stages [OPTIONS]\n</code></pre> <p>Options:</p> Option Description Default <code>--model</code> Show stages for a specific model/workflow Show all <p>Valid <code>--model</code> values: <code>schism</code>, <code>sfincs</code>, <code>create</code>.</p> <p>Examples:</p> <pre><code># List all stages for all workflows\ncoastal-calibration stages\n\n# List only SCHISM stages\ncoastal-calibration stages --model schism\n\n# List only SFINCS stages\ncoastal-calibration stages --model sfincs\n\n# List only creation stages\ncoastal-calibration stages --model create\n</code></pre> <p>Output (all):</p> <pre><code>SCHISM workflow stages:\n  1. download: Download NWM/STOFS data (optional)\n  2. pre_forcing: Prepare NWM forcing data\n  3. nwm_forcing: Generate atmospheric forcing (MPI)\n  4. post_forcing: Post-process forcing data\n  5. schism_obs: Add NOAA observation stations\n  6. update_params: Create SCHISM param.nml\n  7. boundary_conditions: Generate boundary conditions (TPXO/STOFS)\n  8. pre_schism: Prepare SCHISM inputs\n  9. schism_run: Run SCHISM model (MPI)\n  10. post_schism: Post-process SCHISM outputs\n  11. schism_plot: Plot simulated vs observed water levels\n\nSFINCS workflow stages:\n  1. download: Download NWM/STOFS data (optional)\n  2. sfincs_symlinks: Create .nc symlinks for NWM data\n  3. sfincs_data_catalog: Generate HydroMT data catalog\n  4. sfincs_init: Initialise SFINCS model (pre-built)\n  5. sfincs_timing: Set SFINCS timing\n  6. sfincs_forcing: Add water level forcing\n  7. sfincs_obs: Add observation points\n  8. sfincs_discharge: Add discharge sources\n  9. sfincs_precip: Add precipitation forcing\n  10. sfincs_wind: Add wind forcing\n  11. sfincs_pressure: Add atmospheric pressure forcing\n  12. sfincs_write: Write SFINCS model\n  13. sfincs_run: Run SFINCS model (Singularity)\n  14. sfincs_plot: Plot simulated vs observed water levels\n\nSFINCS creation stages (create subcommand):\n  1. create_grid: Create SFINCS grid from AOI polygon\n  2. create_fetch_elevation: Fetch NOAA topobathy DEM for AOI\n  3. create_elevation: Add elevation and bathymetry data\n  4. create_mask: Create active cell mask\n  5. create_boundary: Create water level boundary cells\n  6. create_subgrid: Create subgrid tables\n  7. create_write: Write SFINCS model to disk\n</code></pre>"},{"location":"user-guide/cli/#exit-codes","title":"Exit Codes","text":"Code Description 0 Success 1 Configuration validation error 2 Runtime error 3 Runtime error (stage failure)"},{"location":"user-guide/cli/#environment-variables","title":"Environment Variables","text":"<p>The CLI respects these environment variables:</p> Variable Description <code>COASTAL_LOG_LEVEL</code> Override default log level <code>SLURM_JOB_ID</code> Detected when running in SLURM"},{"location":"user-guide/cli/#shell-completion","title":"Shell Completion","text":"<p>To enable shell completion (bash/zsh):</p> <pre><code># Bash\neval \"$(_COASTAL_CALIBRATION_COMPLETE=bash_source coastal-calibration)\"\n\n# Zsh\neval \"$(_COASTAL_CALIBRATION_COMPLETE=zsh_source coastal-calibration)\"\n</code></pre> <p>Add this to your shell profile for persistent completion.</p>"},{"location":"user-guide/configuration/","title":"Configuration","text":"<p>NWM Coastal uses YAML configuration files to define simulation parameters. This page documents all available configuration options.</p>"},{"location":"user-guide/configuration/#minimal-configuration","title":"Minimal Configuration","text":""},{"location":"user-guide/configuration/#schism-default","title":"SCHISM (default)","text":"<p>The simplest valid SCHISM configuration only requires:</p> <pre><code>simulation:\n  start_date: 2021-06-11\n  duration_hours: 24\n  coastal_domain: hawaii\n  meteo_source: nwm_ana\n\nboundary:\n  source: stofs\n</code></pre> <p>All other parameters have sensible defaults. When no <code>model</code> key is present, SCHISM is assumed.</p>"},{"location":"user-guide/configuration/#sfincs","title":"SFINCS","text":"<p>A minimal SFINCS configuration requires a <code>model</code> key and a <code>model_config</code> section:</p> <pre><code>model: sfincs\n\nsimulation:\n  start_date: 2025-06-01\n  duration_hours: 168\n  coastal_domain: atlgulf\n  meteo_source: nwm_ana\n\nboundary:\n  source: stofs\n\nmodel_config:\n  prebuilt_dir: /path/to/prebuilt/sfincs/model\n</code></pre> <p>Recommended: heredoc sbatch script</p> <p>The preferred approach on clusters is to embed the YAML configuration directly in a sbatch script using a heredoc and use <code>coastal-calibration run</code> to execute it. This keeps the SLURM directives and workflow configuration in a single, self-contained file. See the CLI reference for details and complete examples in <code>docs/examples/</code>.</p>"},{"location":"user-guide/configuration/#variable-interpolation","title":"Variable Interpolation","text":"<p>Configuration values support variable interpolation using <code>${section.key}</code> syntax:</p> <pre><code>simulation:\n  coastal_domain: hawaii\n\npaths:\n  work_dir: /data/${user}/${simulation.coastal_domain}\n  # Resolves to: /data/&lt;your_username&gt;/hawaii\n</code></pre> <p>The <code>${user}</code> variable is automatically resolved from the <code>$USER</code> environment variable.</p>"},{"location":"user-guide/configuration/#default-path-templates","title":"Default Path Templates","text":"<p>If not specified, paths are automatically generated using model-aware templates:</p> Path Default Template <code>work_dir</code> <code>/ngen-test/coastal/${user}/${model}_${simulation.coastal_domain}_${boundary.source}_${simulation.meteo_source}/${model}_${simulation.start_date}</code> <code>raw_download_dir</code> <code>/ngen-test/coastal/${user}/${model}_${simulation.coastal_domain}_${boundary.source}_${simulation.meteo_source}/raw_data</code> <p>The <code>${model}</code> variable resolves to <code>schism</code> or <code>sfincs</code> based on the <code>model</code> key.</p>"},{"location":"user-guide/configuration/#configuration-sections","title":"Configuration Sections","text":""},{"location":"user-guide/configuration/#model-selection","title":"Model Selection","text":"<p>The top-level <code>model</code> key selects the model type. It defaults to <code>schism</code> if omitted.</p> <pre><code>model: sfincs  # or \"schism\" (default)\n</code></pre>"},{"location":"user-guide/configuration/#simulation-settings","title":"Simulation Settings","text":"<p>Define the simulation time period and domain:</p> <pre><code>simulation:\n  start_date: 2021-06-11         # Start date (ISO format)\n  duration_hours: 24             # Simulation length\n  coastal_domain: hawaii         # Domain name\n  meteo_source: nwm_ana          # Meteorological data source\n  timestep_seconds: 3600         # Forcing time step\n</code></pre> Parameter Type Default Options <code>start_date</code> datetime required ISO format date/datetime <code>duration_hours</code> int required Positive integer <code>coastal_domain</code> string required <code>hawaii</code>, <code>prvi</code>, <code>atlgulf</code>, <code>pacific</code> <code>meteo_source</code> string required <code>nwm_ana</code>, <code>nwm_retro</code> <code>timestep_seconds</code> int 3600 Forcing time step in seconds"},{"location":"user-guide/configuration/#supported-date-formats","title":"Supported Date Formats","text":"<p>The <code>start_date</code> field accepts multiple formats:</p> <pre><code>start_date: 2021-06-11              # Date only (midnight)\nstart_date: 2021-06-11T00:00:00     # ISO format with time\nstart_date: \"2021-06-11 00:00:00\"   # Date with space separator\nstart_date: 20210611                # Compact format\n</code></pre>"},{"location":"user-guide/configuration/#boundary-settings","title":"Boundary Settings","text":"<p>Configure boundary conditions:</p> <pre><code>boundary:\n  source: stofs          # Boundary condition source\n  stofs_file:            # Optional: explicit STOFS file path\n</code></pre> Parameter Type Default Options Description <code>source</code> string <code>tpxo</code> <code>tpxo</code>, <code>stofs</code> Boundary condition source <code>stofs_file</code> path null - Override STOFS file path"},{"location":"user-guide/configuration/#path-settings","title":"Path Settings","text":"<p>Configure file system paths:</p> <p>All path fields support <code>~</code> (tilde) expansion, so you can write <code>~/my_data</code> instead of the full home directory path.</p> <pre><code>paths:\n  work_dir: /path/to/work         # Working directory (auto-generated if not set)\n  raw_download_dir: /path/to/data # Download directory (auto-generated if not set)\n  nfs_mount: /ngen-test           # NFS mount point\n  nwm_dir: /ngen-app/nwm.v3.0.6\n  hot_start_file:                 # Hot restart file for warm start\n  conda_env_name: ngen_forcing_coastal\n  parm_dir: /ngen-test/coastal/ngwpc-coastal\n</code></pre> Parameter Type Default <code>work_dir</code> path Auto-generated from template <code>raw_download_dir</code> path Auto-generated from template <code>nfs_mount</code> path <code>/ngen-test</code> <code>nwm_dir</code> path <code>/ngen-app/nwm.v3.0.6</code> <code>hot_start_file</code> path null <code>conda_env_name</code> string <code>ngen_forcing_coastal</code> <code>parm_dir</code> path <code>/ngen-test/coastal/ngwpc-coastal</code>"},{"location":"user-guide/configuration/#model-configuration","title":"Model Configuration","text":"<p>Model-specific parameters live in the <code>model_config</code> section. The contents depend on the <code>model</code> key.</p>"},{"location":"user-guide/configuration/#schism-model-configuration","title":"SCHISM Model Configuration","text":"<pre><code># model: schism (default, can be omitted)\nmodel_config:\n  singularity_image: /ngencerf-app/singularity/ngen-coastal.sif\n  nodes: 2                        # Number of compute nodes\n  ntasks_per_node: 18             # MPI tasks per node\n  exclusive: true                 # Request exclusive node access\n  nscribes: 2                     # SCHISM I/O scribes\n  omp_num_threads: 2              # OpenMP threads\n  oversubscribe: false            # Allow MPI oversubscription\n  binary: pschism_wcoss2_NO_PARMETIS_TVD-VL.openmpi\n  include_noaa_gages: true        # Enable NOAA observation stations &amp; comparison plots\n</code></pre> Parameter Type Default Description <code>singularity_image</code> path <code>/ngencerf-app/singularity/ngen-coastal.sif</code> Singularity/Apptainer SIF image for SCHISM <code>nodes</code> int 2 Number of compute nodes <code>ntasks_per_node</code> int 18 MPI tasks per node <code>exclusive</code> bool true Request exclusive node access <code>nscribes</code> int 2 SCHISM I/O scribes <code>omp_num_threads</code> int 2 OpenMP threads <code>oversubscribe</code> bool false Allow MPI oversubscription <code>binary</code> string <code>pschism_wcoss2_NO_PARMETIS_TVD-VL.openmpi</code> SCHISM executable name <code>include_noaa_gages</code> bool false Enable NOAA station discovery and comparison"},{"location":"user-guide/configuration/#noaa-observation-stations-include_noaa_gages","title":"NOAA Observation Stations (<code>include_noaa_gages</code>)","text":"<p>When set to <code>true</code>, two additional stages are activated in the SCHISM pipeline:</p> <ul> <li><code>schism_obs</code>: Automatically discovers NOAA CO-OPS water level stations within the     model domain by computing a concave hull around the open boundary nodes in     <code>hgrid.gr3</code> and querying the CO-OPS API. Writes a <code>station.in</code> file so SCHISM     outputs time-series at those locations, and a <code>station_noaa_ids.txt</code> companion that     maps station indices to NOAA station IDs. The <code>pre_schism</code> stage then patches     <code>param.nml</code> to enable station output (<code>iout_sta = 1</code>, <code>nspool_sta = 18</code>).</li> <li><code>schism_plot</code>: After the SCHISM run completes, reads the station output     (<code>staout_1</code>), fetches the corresponding NOAA CO-OPS observations (converting from     MLLW to MSL datum), and generates 2\u00d72 comparison plots saved to <code>figs/</code>.</li> </ul> <p>Both stages require network access for NOAA CO-OPS API calls and are classified as Python-only stages.</p>"},{"location":"user-guide/configuration/#sfincs-model-configuration","title":"SFINCS Model Configuration","text":"<pre><code>model: sfincs\n\nmodel_config:\n  prebuilt_dir: /path/to/model    # Required: pre-built SFINCS model directory\n  model_root:                     # Output directory (defaults to {work_dir}/sfincs_model)\n  observation_points: []          # Observation point coordinates\n  observation_locations_file:     # Observation locations file\n  merge_observations: false       # Merge observations into model\n  discharge_locations_file:       # Discharge source locations file\n  merge_discharge: false          # Merge discharge into model\n  include_noaa_gages: true        # Enable NOAA station discovery &amp; comparison plots\n  include_precip: true            # Add precipitation forcing\n  include_wind: true              # Add wind forcing\n  include_pressure: true          # Add atmospheric pressure forcing\n  forcing_to_mesh_offset_m: 0.0  # Vertical offset added to boundary forcing\n  vdatum_mesh_to_msl_m: 0.0      # Vertical offset converting model output to MSL\n  meteo_res:                      # Meteo forcing resolution in metres (auto if null)\n  sfincs_exe:                     # Local SFINCS executable (bypasses container)\n  omp_num_threads: 36             # OpenMP threads (defaults to CPU count)\n  container_tag: latest           # SFINCS container tag\n  container_image:                # Singularity image path\n</code></pre> Parameter Type Default Description <code>prebuilt_dir</code> path required Path to pre-built SFINCS model <code>model_root</code> path null Output directory (defaults to <code>{work_dir}/sfincs_model</code>) <code>observation_points</code> list <code>[]</code> Observation point coordinates <code>observation_locations_file</code> path null Observation locations file <code>merge_observations</code> bool false Merge observations into model <code>discharge_locations_file</code> path null Discharge source locations file <code>merge_discharge</code> bool false Merge discharge into model <code>include_noaa_gages</code> bool false Enable NOAA station discovery and comparison plots <code>include_precip</code> bool false Add precipitation forcing from meteo data source <code>include_wind</code> bool false Add spatially-varying wind forcing <code>include_pressure</code> bool false Add atmospheric pressure forcing with barometric correction <code>forcing_to_mesh_offset_m</code> float 0.0 Vertical offset (m) added to boundary forcing before simulation <code>vdatum_mesh_to_msl_m</code> float 0.0 Vertical offset (m) added to model output for MSL comparison <code>meteo_res</code> float null Meteo output resolution (m). Auto-derived from quadtree grid if null <code>sfincs_exe</code> path null Local SFINCS executable (bypasses Singularity container) <code>omp_num_threads</code> int auto OpenMP threads (defaults to CPU count) <code>container_tag</code> string latest SFINCS container tag <code>container_image</code> path null Singularity image path <p>Meteorological forcing resolution</p> <p>When <code>meteo_res</code> is not set, the resolution is automatically derived from the base cell size of the SFINCS quadtree grid. This prevents the LCC \u2192 UTM reprojection of NWM data from inflating the meteo grid to the full CONUS extent, which can produce multi-GB forcing files and slow SFINCS runtime from minutes to hours.</p> <p>Vertical datum offsets</p> <p>SFINCS operates in the vertical datum of the mesh (e.g. NAVD88). When the boundary forcing is in a different datum, <code>forcing_to_mesh_offset_m</code> anchors the forcing signal to the correct height on the mesh. For tidal-only sources like TPXO, whose oscillations are centred on zero (MSL), this places the mean water level at the geodetic height of MSL on the mesh. For sources that already report water levels in the mesh datum (e.g. STOFS on a NAVD88 mesh), set this to <code>0.0</code>.</p> <p>After the simulation, <code>vdatum_mesh_to_msl_m</code> converts the model output from the mesh datum to MSL for comparison with NOAA CO-OPS observations. Both values can be obtained from the NOAA VDatum API.</p> <p>For example, on the Texas Gulf coast (NAVD88 mesh), VDatum reports MSL is 0.171 m above NAVD88 at the domain centroid, so both parameters are set to <code>0.171</code>.</p>"},{"location":"user-guide/configuration/#monitoring-settings","title":"Monitoring Settings","text":"<p>Configure logging and monitoring:</p> <pre><code>monitoring:\n  log_level: INFO                 # Logging verbosity\n  log_file:                       # Optional log file path\n  enable_progress_tracking: true  # Show progress bars\n  enable_timing: true             # Track stage timing\n</code></pre> Parameter Type Default Options <code>log_level</code> string <code>INFO</code> <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code> <code>log_file</code> path null Path to log file <code>enable_progress_tracking</code> bool true Show progress bars <code>enable_timing</code> bool true Track and report stage timing"},{"location":"user-guide/configuration/#download-settings","title":"Download Settings","text":"<p>Configure data download behavior:</p> <pre><code>download:\n  enabled: true           # Enable automatic downloads\n  skip_existing: true     # Skip already downloaded files\n  timeout: 600            # Download timeout in seconds\n  raise_on_error: true    # Fail on download errors\n  limit_per_host: 4       # Concurrent downloads per host\n</code></pre> Parameter Type Default Description <code>enabled</code> bool true Enable automatic downloads <code>skip_existing</code> bool true Skip already downloaded files <code>timeout</code> int 600 Download timeout (seconds) <code>raise_on_error</code> bool true Fail workflow on download errors <code>limit_per_host</code> int 4 Concurrent downloads per host"},{"location":"user-guide/configuration/#configuration-inheritance","title":"Configuration Inheritance","text":"<p>Use <code>_base</code> to inherit settings from another configuration file:</p> <pre><code># base.yaml - shared settings\nsimulation:\n  duration_hours: 24\n  meteo_source: nwm_ana\n\nboundary:\n  source: stofs\n</code></pre> <pre><code># hawaii_run.yaml - inherits from base.yaml\n_base: base.yaml\n\nsimulation:\n  start_date: 2021-06-11\n  coastal_domain: hawaii\n</code></pre> <pre><code># prvi_run.yaml - different domain, same settings\n_base: base.yaml\n\nsimulation:\n  start_date: 2022-09-18\n  coastal_domain: prvi\n</code></pre> <p>This allows you to:</p> <ul> <li>Share common settings across multiple runs</li> <li>Override only the parameters that differ</li> <li>Maintain consistency across related simulations</li> </ul>"},{"location":"user-guide/configuration/#sfincs-creation-configuration","title":"SFINCS Creation Configuration","text":"<p>The <code>create</code> command uses a separate configuration schema (<code>SfincsCreateConfig</code>) to build a new SFINCS quadtree model from an AOI polygon. This config is independent of the run/simulation config above.</p>"},{"location":"user-guide/configuration/#minimal-create-config","title":"Minimal Create Config","text":"<pre><code>aoi: ./texas_aoi.geojson\noutput_dir: ./my_sfincs_model\n\nelevation:\n  datasets:\n    - name: nws_topobathy\n      zmin: -20000\n\ndata_catalog:\n  data_libs:\n    - ./dem/data_catalog.yml\n</code></pre>"},{"location":"user-guide/configuration/#create-configuration-sections","title":"Create Configuration Sections","text":""},{"location":"user-guide/configuration/#top-level-fields","title":"Top-Level Fields","text":"Parameter Type Default Description <code>aoi</code> path required Path to AOI polygon (GeoJSON, Shapefile, etc.) <code>output_dir</code> path required Directory where the model will be written <code>download_dir</code> path <code>{output_dir}/downloads</code> Directory for downloaded data (NOAA DEMs)"},{"location":"user-guide/configuration/#grid-settings-grid","title":"Grid Settings (<code>grid</code>)","text":"<pre><code>grid:\n  resolution: 50.0\n  crs: utm\n  rotated: true\n  refinement:\n    - polygon: ./aoi.geojson\n      level: 4\n      buffer_m: -3072\n</code></pre> Parameter Type Default Description <code>resolution</code> float 50.0 Base grid cell resolution in metres <code>crs</code> str <code>utm</code> CRS (<code>\"utm\"</code> for auto-detection, or <code>\"EPSG:xxxxx\"</code>) <code>rotated</code> bool true Allow grid rotation for tighter bounding-box fit <code>refinement</code> list <code>[]</code> Quadtree refinement levels (see below) <p>Each refinement entry:</p> Field Type Description <code>polygon</code> path Polygon defining the area to refine <code>level</code> int Refinement level (1 = base, 2 = base/2, 3 = base/4, \u2026) <code>buffer_m</code> float Inward buffer in metres (negative shrinks the polygon)"},{"location":"user-guide/configuration/#elevation-settings-elevation","title":"Elevation Settings (<code>elevation</code>)","text":"<pre><code>elevation:\n  datasets:\n    - name: nws_topobathy\n      zmin: -20000\n    - name: gebco\n      zmin: -20000\n  buffer_cells: 1\n</code></pre> Parameter Type Default Description <code>datasets</code> list copdem30 + gebco Ordered list of elevation datasets <code>buffer_cells</code> int 1 Buffer cells around grid boundary <p>Each dataset entry:</p> Field Type Description <code>name</code> str HydroMT data-catalog dataset name <code>zmin</code> float Minimum elevation threshold <code>source</code> str Auto-fetch source (<code>\"noaa\"</code> or null) <code>noaa_dataset</code> str Explicit NOAA dataset name (auto-discovered if null)"},{"location":"user-guide/configuration/#mask-settings-mask","title":"Mask Settings (<code>mask</code>)","text":"Parameter Type Default Description <code>zmin</code> float -5.0 Minimum elevation for active cells <code>boundary_zmax</code> float -5.0 Maximum elevation for boundary cells <code>reset_bounds</code> bool true Reset existing boundary conditions"},{"location":"user-guide/configuration/#subgrid-settings-subgrid","title":"Subgrid Settings (<code>subgrid</code>)","text":"Parameter Type Default Description <code>nr_subgrid_pixels</code> int 5 Number of subgrid pixels per cell <code>lulc_dataset</code> str <code>esa_worldcover_2021</code> Land-use/land-cover dataset <code>reclass_table</code> path null Custom reclassification table CSV <code>manning_land</code> float 0.04 Default Manning coefficient for land <code>manning_sea</code> float 0.02 Default Manning coefficient for sea"},{"location":"user-guide/configuration/#data-catalog-data_catalog","title":"Data Catalog (<code>data_catalog</code>)","text":"Parameter Type Default Description <code>data_libs</code> list <code>[]</code> Additional HydroMT data catalog YAML paths"},{"location":"user-guide/configuration/#nwm-discharge-nwm_discharge-optional","title":"NWM Discharge (<code>nwm_discharge</code>) \u2014 Optional","text":"<p>When configured, adds NWM streamflow discharge source points to the model by intersecting hydrofabric flowpaths with the AOI boundary.</p> <pre><code>nwm_discharge:\n  hydrofabric_gpkg: ./nwm_hydrofabric.gpkg\n  flowpaths_layer: flowpaths\n  flowpath_id_column: feature_id\n  flowpath_ids: [12345, 67890]\n  coastal_domain: atlgulf\n</code></pre> Parameter Type Default Description <code>hydrofabric_gpkg</code> path required Path to NWM hydrofabric GeoPackage <code>flowpaths_layer</code> str required Layer name with flowpath linestrings <code>flowpath_id_column</code> str required Column matching NWM <code>feature_id</code> values <code>flowpath_ids</code> list required NWM feature IDs to extract <code>coastal_domain</code> str <code>conus</code> NWM coastal domain for ID validation"},{"location":"user-guide/configuration/#validation","title":"Validation","text":"<p>Validate your configuration before running:</p> <pre><code>coastal-calibration validate config.yaml\n</code></pre> <p>The validation checks:</p> <ul> <li>All required fields are present</li> <li>Date ranges are valid for selected data sources</li> <li>File paths exist (for required files)</li> <li>Model-specific configuration is consistent (e.g., nscribes &lt; total MPI tasks for     SCHISM)</li> </ul>"},{"location":"user-guide/python-api/","title":"Python API","text":"<p>The Python API provides programmatic access to the coastal calibration workflow, enabling integration with other tools and custom automation.</p>"},{"location":"user-guide/python-api/#basic-usage","title":"Basic Usage","text":"<pre><code>from coastal_calibration import CoastalCalibConfig, CoastalCalibRunner\n\n# Load configuration from YAML\nconfig = CoastalCalibConfig.from_yaml(\"config.yaml\")\n\n# Create a runner\nrunner = CoastalCalibRunner(config)\n\n# Validate configuration\nerrors = runner.validate()\nif errors:\n    for error in errors:\n        print(f\"Error: {error}\")\nelse:\n    # Run the workflow\n    result = runner.run()\n\n    if result.success:\n        print(f\"Completed in {result.duration_seconds:.1f}s\")\n    else:\n        print(f\"Failed: {result.errors}\")\n</code></pre>"},{"location":"user-guide/python-api/#configuration","title":"Configuration","text":""},{"location":"user-guide/python-api/#loading-configuration","title":"Loading Configuration","text":"<pre><code>from coastal_calibration import CoastalCalibConfig\n\n# From YAML file\nconfig = CoastalCalibConfig.from_yaml(\"config.yaml\")\n\n# Access configuration values\nprint(config.simulation.coastal_domain)\nprint(config.paths.work_dir)\nprint(config.model)  # \"schism\" or \"sfincs\"\n</code></pre>"},{"location":"user-guide/python-api/#creating-schism-configuration-programmatically","title":"Creating SCHISM Configuration Programmatically","text":"<pre><code>from datetime import datetime\nfrom pathlib import Path\nfrom coastal_calibration import (\n    CoastalCalibConfig,\n    SimulationConfig,\n    BoundaryConfig,\n    PathConfig,\n    SchismModelConfig,\n    MonitoringConfig,\n    DownloadConfig,\n)\n\nconfig = CoastalCalibConfig(\n    simulation=SimulationConfig(\n        start_date=datetime(2021, 6, 11),\n        duration_hours=24,\n        coastal_domain=\"hawaii\",\n        meteo_source=\"nwm_ana\",\n    ),\n    boundary=BoundaryConfig(source=\"stofs\"),\n    paths=PathConfig(\n        work_dir=Path(\"/ngen-test/coastal/your_username/my_run\"),\n        raw_download_dir=Path(\"/ngen-test/coastal/your_username/downloads\"),\n    ),\n    model_config=SchismModelConfig(\n        nodes=2,\n        ntasks_per_node=18,\n    ),\n)\n\n# Save to YAML\nconfig.to_yaml(\"generated_config.yaml\")\n</code></pre>"},{"location":"user-guide/python-api/#enabling-noaa-observation-station-comparison","title":"Enabling NOAA Observation Station Comparison","text":"<p>To automatically discover NOAA CO-OPS water level stations within the model domain and generate comparison plots after the SCHISM run, set <code>include_noaa_gages=True</code> in the model configuration:</p> <pre><code>config = CoastalCalibConfig(\n    simulation=SimulationConfig(\n        start_date=datetime(2021, 6, 11),\n        duration_hours=24,\n        coastal_domain=\"hawaii\",\n        meteo_source=\"nwm_ana\",\n    ),\n    boundary=BoundaryConfig(source=\"stofs\"),\n    paths=PathConfig(\n        work_dir=Path(\"/ngen-test/coastal/your_username/hawaii_obs\"),\n        raw_download_dir=Path(\"/ngen-test/coastal/your_username/downloads\"),\n    ),\n    model_config=SchismModelConfig(\n        nodes=2,\n        ntasks_per_node=18,\n        include_noaa_gages=True,  # Enable station discovery &amp; comparison plots\n    ),\n)\n</code></pre> <p>This activates two additional stages in the pipeline:</p> <ul> <li><code>schism_obs</code> discovers NOAA CO-OPS stations via the concave hull of the open     boundary nodes and writes <code>station.in</code> and <code>station_noaa_ids.txt</code>.</li> <li><code>schism_plot</code> reads SCHISM station output (<code>staout_1</code>), fetches observations from     the CO-OPS API (with MLLW\u2192MSL datum conversion), and saves comparison plots to     <code>figs/</code>.</li> </ul>"},{"location":"user-guide/python-api/#creating-sfincs-configuration-programmatically","title":"Creating SFINCS Configuration Programmatically","text":"<pre><code>from datetime import datetime\nfrom pathlib import Path\nfrom coastal_calibration import (\n    CoastalCalibConfig,\n    SimulationConfig,\n    BoundaryConfig,\n    PathConfig,\n    SfincsModelConfig,\n)\n\nTEXAS_DIR = Path(\"/path/to/texas/model\")\n\nconfig = CoastalCalibConfig(\n    simulation=SimulationConfig(\n        start_date=datetime(2025, 6, 1),\n        duration_hours=168,\n        coastal_domain=\"atlgulf\",\n        meteo_source=\"nwm_ana\",\n    ),\n    boundary=BoundaryConfig(source=\"stofs\"),\n    paths=PathConfig(\n        work_dir=Path(\"/tmp/sfincs_run\"),\n        raw_download_dir=Path(\"/tmp/sfincs_downloads\"),\n    ),\n    model_config=SfincsModelConfig(\n        prebuilt_dir=TEXAS_DIR,\n        discharge_locations_file=TEXAS_DIR / \"sfincs_nwm.src\",\n        observation_points=[\n            {\"x\": 830344.95, \"y\": 3187383.41, \"name\": \"Sargent\"},\n        ],\n        merge_observations=False,\n        merge_discharge=False,\n        include_noaa_gages=True,\n        include_precip=True,\n        include_wind=True,\n        include_pressure=True,\n        forcing_to_mesh_offset_m=0.0,  # STOFS already in mesh datum\n        vdatum_mesh_to_msl_m=0.171,  # mesh datum (NAVD88) \u2192 MSL for obs comparison\n        meteo_res=2000,  # meteo output resolution in metres (auto-derived if None)\n    ),\n)\n</code></pre>"},{"location":"user-guide/python-api/#configuration-validation","title":"Configuration Validation","text":"<pre><code>config = CoastalCalibConfig.from_yaml(\"config.yaml\")\n\n# Validate and get list of errors\nerrors = config.validate()\n\nif errors:\n    print(\"Configuration errors:\")\n    for error in errors:\n        print(f\"  - {error}\")\nelse:\n    print(\"Configuration is valid\")\n</code></pre>"},{"location":"user-guide/python-api/#workflow-execution","title":"Workflow Execution","text":""},{"location":"user-guide/python-api/#run-the-workflow","title":"Run the Workflow","text":"<pre><code># Run complete workflow\nresult = runner.run()\n\n# Run partial workflow\nresult = runner.run(start_from=\"pre_forcing\", stop_after=\"post_forcing\")\n\n# Run from a specific stage to the end\nresult = runner.run(start_from=\"pre_schism\")\n</code></pre>"},{"location":"user-guide/python-api/#workflow-result","title":"Workflow Result","text":"<p>The <code>WorkflowResult</code> object contains information about the execution:</p> <pre><code>result = runner.run()\n\nprint(f\"Success: {result.success}\")\nprint(f\"Duration: {result.duration_seconds}s\")\n\nif not result.success:\n    for error in result.errors:\n        print(f\"Error: {error}\")\n\n# Stage timing (if enable_timing is True)\nfor stage, duration in result.stage_durations.items():\n    print(f\"  {stage}: {duration:.1f}s\")\n</code></pre>"},{"location":"user-guide/python-api/#data-sources","title":"Data Sources","text":""},{"location":"user-guide/python-api/#check-available-date-ranges","title":"Check Available Date Ranges","text":"<pre><code>from coastal_calibration.downloader import validate_date_ranges\n\n# Validate dates for your configuration\nerrors = validate_date_ranges(\n    start_time=datetime(2021, 6, 11),\n    end_time=datetime(2021, 6, 12),\n    meteo_source=\"nwm_ana\",\n    boundary_source=\"stofs\",\n    coastal_domain=\"hawaii\",\n)\n\nif errors:\n    print(\"Date range errors:\", errors)\n</code></pre>"},{"location":"user-guide/python-api/#supported-data-sources","title":"Supported Data Sources","text":"Source Date Range Description <code>nwm_retro</code> 1979-02-01 to 2023-01-31 NWM Retrospective 3.0 <code>nwm_ana</code> 2018-09-17 to present NWM Analysis <code>stofs</code> 2020-12-30 to present STOFS water levels <code>glofs</code> 2005-09-30 to present Great Lakes OFS"},{"location":"user-guide/python-api/#logging","title":"Logging","text":"<p>Configure logging for the workflow:</p> <pre><code>import logging\nfrom coastal_calibration.utils.logging import setup_logger\n\n# Set up logging\nlogger = setup_logger(log_level=\"DEBUG\", log_file=\"workflow.log\")\n\n# Now run your workflow\nconfig = CoastalCalibConfig.from_yaml(\"config.yaml\")\nrunner = CoastalCalibRunner(config)\nresult = runner.run()\n</code></pre>"},{"location":"user-guide/python-api/#example-batch-processing","title":"Example: Batch Processing","text":"<p>Run multiple simulations with different parameters:</p> <pre><code>from datetime import datetime, timedelta\nfrom coastal_calibration import CoastalCalibConfig, CoastalCalibRunner\n\n# Load base configuration\nbase_config = CoastalCalibConfig.from_yaml(\"base_config.yaml\")\n\n# Run simulations for multiple dates\nstart_dates = [\n    datetime(2021, 6, 1),\n    datetime(2021, 6, 15),\n    datetime(2021, 7, 1),\n]\n\nresults = []\nfor start_date in start_dates:\n    # Modify configuration for this run\n    config = CoastalCalibConfig.from_yaml(\"base_config.yaml\")\n    config.simulation.start_date = start_date\n\n    # Update work directory for this run\n    date_str = start_date.strftime(\"%Y%m%d\")\n    config.paths.work_dir = config.paths.work_dir.parent / f\"run_{date_str}\"\n\n    # Run\n    runner = CoastalCalibRunner(config)\n    result = runner.run()\n    results.append((start_date, result))\n\n# Report results\nfor start_date, result in results:\n    status = \"Success\" if result.success else \"Failed\"\n    print(f\"{start_date}: {status}\")\n</code></pre>"},{"location":"user-guide/python-api/#example-domain-comparison","title":"Example: Domain Comparison","text":"<p>Run the same simulation across multiple domains:</p> <pre><code>domains = [\"hawaii\", \"prvi\", \"atlgulf\", \"pacific\"]\n\nfor domain in domains:\n    config = CoastalCalibConfig.from_yaml(\"base_config.yaml\")\n    config.simulation.coastal_domain = domain\n\n    runner = CoastalCalibRunner(config)\n    errors = runner.validate()\n\n    if errors:\n        print(f\"{domain}: Validation failed - {errors}\")\n        continue\n\n    result = runner.run()\n    print(f\"{domain}: {'Success' if result.success else 'Failed'}\")\n</code></pre>"},{"location":"user-guide/workflow-stages/","title":"Workflow Stages","text":"<p>The coastal calibration workflow consists of sequential stages, each performing a specific task in the simulation pipeline. The stage order depends on the selected model (SCHISM or SFINCS).</p> <p>The <code>run</code> command executes all stages sequentially. Each stage is classified as either Python-only or container (requires Singularity/MPI). All stages execute locally on the allocated compute nodes (e.g., inside an <code>sbatch</code> script).</p>"},{"location":"user-guide/workflow-stages/#schism-stage-overview","title":"SCHISM Stage Overview","text":"<pre><code>flowchart TD\n    A[download] --&gt; B[pre_forcing]\n    B --&gt; C[nwm_forcing]\n    C --&gt; D[post_forcing]\n    D --&gt; E[schism_obs]\n    E --&gt; F[update_params]\n    F --&gt; G[boundary_conditions]\n    G --&gt; H[pre_schism]\n    H --&gt; I[schism_run]\n    I --&gt; J[post_schism]\n    J --&gt; K[schism_plot]</code></pre>"},{"location":"user-guide/workflow-stages/#sfincs-stage-overview","title":"SFINCS Stage Overview","text":"<pre><code>flowchart TD\n    A[download] --&gt; B[sfincs_symlinks]\n    B --&gt; C[sfincs_data_catalog]\n    C --&gt; D[sfincs_init]\n    D --&gt; E[sfincs_timing]\n    E --&gt; F[sfincs_forcing]\n    F --&gt; G[sfincs_obs]\n    G --&gt; H[sfincs_discharge]\n    H --&gt; I[sfincs_precip]\n    I --&gt; J[sfincs_wind]\n    J --&gt; K[sfincs_pressure]\n    K --&gt; L[sfincs_write]\n    L --&gt; M[sfincs_run]\n    M --&gt; N[sfincs_plot]</code></pre>"},{"location":"user-guide/workflow-stages/#schism-stage-details","title":"SCHISM Stage Details","text":""},{"location":"user-guide/workflow-stages/#1-download","title":"1. download","text":"<p>Purpose: Download required input data from remote sources.</p> <p>Data Sources:</p> <ul> <li>NWM meteorological forcing (LDASIN files)</li> <li>NWM streamflow data (CHRTOUT files)</li> <li>STOFS or GLOFS water level data (when applicable)</li> </ul> <p>Runs On: Compute node (Python-only, no container needed)</p> <p>Outputs:</p> <pre><code>raw_download_dir/\n\u251c\u2500\u2500 meteo/nwm_ana/\n\u2502   \u2514\u2500\u2500 *.LDASIN_DOMAIN1.nc\n\u251c\u2500\u2500 hydro/nwm/\n\u2502   \u2514\u2500\u2500 *.CHRTOUT_DOMAIN1.nc\n\u2514\u2500\u2500 coastal/stofs/\n    \u2514\u2500\u2500 *.fields.cwl.nc\n</code></pre>"},{"location":"user-guide/workflow-stages/#2-pre_forcing","title":"2. pre_forcing","text":"<p>Purpose: Prepare NWM forcing data for SCHISM.</p> <p>Tasks:</p> <ul> <li>Copy and organize downloaded NWM files</li> <li>Set up directory structure for forcing generation</li> <li>Validate input data integrity</li> </ul> <p>Runs On: Compute node (inside Singularity)</p>"},{"location":"user-guide/workflow-stages/#3-nwm_forcing","title":"3. nwm_forcing","text":"<p>Purpose: Generate atmospheric forcing files using MPI.</p> <p>Tasks:</p> <ul> <li>Regrid NWM data to SCHISM mesh</li> <li>Interpolate forcing variables</li> <li>Generate SCHISM-compatible forcing files</li> </ul> <p>Runs On: Compute node (MPI parallel, inside Singularity)</p> <p>Outputs:</p> <pre><code>work_dir/\n\u2514\u2500\u2500 sflux/\n    \u251c\u2500\u2500 sflux_air_1.*.nc\n    \u251c\u2500\u2500 sflux_prc_1.*.nc\n    \u2514\u2500\u2500 sflux_rad_1.*.nc\n</code></pre>"},{"location":"user-guide/workflow-stages/#4-post_forcing","title":"4. post_forcing","text":"<p>Purpose: Post-process forcing data.</p> <p>Tasks:</p> <ul> <li>Validate generated forcing files</li> <li>Create forcing summary</li> <li>Clean up temporary files</li> </ul> <p>Runs On: Compute node (inside Singularity)</p>"},{"location":"user-guide/workflow-stages/#5-schism_obs","title":"5. schism_obs","text":"<p>Purpose: Automatically discover NOAA CO-OPS water level stations within the model domain and generate a SCHISM <code>station.in</code> file so that SCHISM writes time-series output at those locations.</p> <p>Enabled by: <code>model_config.include_noaa_gages: true</code> (disabled by default)</p> <p>How it works:</p> <ol> <li>Parses <code>hgrid.gr3</code> to extract the coordinates of all open boundary nodes.</li> <li>Computes a concave hull around the boundary points (using <code>shapely.concave_hull</code> with     <code>ratio=0.05</code>).</li> <li>Queries the NOAA CO-OPS API to find active water level stations that fall inside the     hull polygon.</li> <li>Writes <code>station.in</code> (SCHISM station definition file with elevation-only output flag)     and a companion <code>station_noaa_ids.txt</code> that maps each station index to its NOAA     station ID.</li> </ol> <p>Runs On: Compute node (Python-only). Requires network access for the CO-OPS API call.</p> <p>Outputs:</p> <pre><code>work_dir/\n\u251c\u2500\u2500 station.in             # SCHISM station definition file\n\u2514\u2500\u2500 station_noaa_ids.txt   # Index-to-NOAA-ID mapping\n</code></pre> <p>param.nml patching</p> <p>When <code>station.in</code> exists, the <code>pre_schism</code> stage automatically patches <code>param.nml</code> to set <code>iout_sta = 1</code> (enable station output) and <code>nspool_sta = 18</code> (output interval in time-steps). The value <code>nspool_sta = 18</code> is chosen because it divides all <code>nhot_write</code> values used across domain templates (162, 324, 8640, etc.), satisfying the SCHISM constraint <code>mod(nhot_write, nspool_sta) == 0</code>.</p>"},{"location":"user-guide/workflow-stages/#6-update_params","title":"6. update_params","text":"<p>Purpose: Generate SCHISM parameter file and symlink mesh files.</p> <p>Tasks:</p> <ul> <li>Symlink <code>hgrid.gr3</code> and other mesh files into the work directory</li> <li>Create <code>param.nml</code> with simulation parameters</li> <li>Set time stepping configuration</li> <li>Configure output options</li> </ul> <p>Runs On: Compute node (inside Singularity)</p> <p>Outputs:</p> <pre><code>work_dir/\n\u251c\u2500\u2500 hgrid.gr3 (symlink)\n\u2514\u2500\u2500 param.nml\n</code></pre>"},{"location":"user-guide/workflow-stages/#7-boundary_conditions","title":"7. boundary_conditions","text":"<p>Purpose: Generate boundary conditions from TPXO or STOFS.</p> <p>Tasks (TPXO):</p> <ul> <li>Extract tidal constituents at boundary nodes</li> <li>Generate harmonic boundary files</li> <li>Create <code>bctides.in</code> file</li> </ul> <p>Tasks (STOFS):</p> <ul> <li>Interpolate STOFS water levels to boundary</li> <li>Generate time-varying boundary files</li> <li>Create <code>elev2D.th.nc</code> file</li> </ul> <p>Runs On: Compute node (inside Singularity)</p>"},{"location":"user-guide/workflow-stages/#8-pre_schism","title":"8. pre_schism","text":"<p>Purpose: Final preparation before SCHISM execution.</p> <p>Tasks:</p> <ul> <li>Validate all input files present</li> <li>Set up symbolic links</li> <li>Configure MPI environment</li> </ul> <p>Runs On: Compute node (inside Singularity)</p>"},{"location":"user-guide/workflow-stages/#9-schism_run","title":"9. schism_run","text":"<p>Purpose: Execute the SCHISM model.</p> <p>Tasks:</p> <ul> <li>Run SCHISM with MPI</li> <li>Monitor progress</li> <li>Handle I/O scribes</li> </ul> <p>Runs On: Compute node (MPI parallel, inside Singularity)</p> <p>Configuration:</p> <ul> <li>Uses <code>nscribes</code> I/O processes from model config</li> <li>OpenMP threads configured via <code>omp_num_threads</code></li> <li>Total processes = <code>nodes * ntasks_per_node</code></li> </ul>"},{"location":"user-guide/workflow-stages/#10-post_schism","title":"10. post_schism","text":"<p>Purpose: Post-process SCHISM outputs.</p> <p>Tasks:</p> <ul> <li>Combine output files</li> <li>Generate statistics</li> <li>Create visualization-ready files</li> </ul> <p>Runs On: Compute node (inside Singularity)</p>"},{"location":"user-guide/workflow-stages/#11-schism_plot","title":"11. schism_plot","text":"<p>Purpose: Compare SCHISM-simulated water levels against NOAA CO-OPS observations at every station discovered by the <code>schism_obs</code> stage.</p> <p>Enabled by: <code>model_config.include_noaa_gages: true</code></p> <p>How it works:</p> <ol> <li>Reads the SCHISM station time-series from <code>outputs/staout_1</code>.</li> <li>Loads the station-to-NOAA-ID mapping from <code>station_noaa_ids.txt</code>.</li> <li>Fetches observed water levels from the CO-OPS API in the MLLW datum and converts to     MSL using per-station datum offsets retrieved from the API.</li> <li>Generates 2\u00d72 comparison plots (four stations per figure) showing simulated vs     observed water levels.</li> <li>Saves PNG figures to the <code>figs/</code> subdirectory.</li> </ol> <p>Runs On: Compute node (Python-only). Requires network access for the CO-OPS API call.</p> <p>Outputs:</p> <pre><code>work_dir/\n\u2514\u2500\u2500 figs/\n    \u251c\u2500\u2500 stations_comparison_001.png\n    \u251c\u2500\u2500 stations_comparison_002.png\n    \u2514\u2500\u2500 ...\n</code></pre> <p>Datum conversion</p> <p>Observations are fetched in MLLW (Mean Lower Low Water) and converted to MSL (Mean Sea Level) so they share the same vertical reference as the SCHISM simulation output. The conversion offset is obtained from the CO-OPS datums endpoint for each station.</p>"},{"location":"user-guide/workflow-stages/#sfincs-stage-details","title":"SFINCS Stage Details","text":""},{"location":"user-guide/workflow-stages/#1-download_1","title":"1. download","text":"<p>Same as SCHISM download stage. Downloads NWM meteorological forcing, streamflow, and STOFS water level data.</p>"},{"location":"user-guide/workflow-stages/#2-sfincs_symlinks","title":"2. sfincs_symlinks","text":"<p>Purpose: Create .nc symlinks for NWM data.</p> <p>Tasks:</p> <ul> <li>Create symlinks in the working directory pointing to downloaded NWM files</li> <li>Organize files by type (meteo, hydro)</li> </ul>"},{"location":"user-guide/workflow-stages/#3-sfincs_data_catalog","title":"3. sfincs_data_catalog","text":"<p>Purpose: Generate a HydroMT data catalog.</p> <p>Tasks:</p> <ul> <li>Build YAML data catalog for HydroMT-SFINCS</li> <li>Register NWM meteo, streamflow, and STOFS data sources</li> </ul>"},{"location":"user-guide/workflow-stages/#4-sfincs_init","title":"4. sfincs_init","text":"<p>Purpose: Initialise the SFINCS model from a pre-built template.</p> <p>Tasks:</p> <ul> <li>Copy pre-built SFINCS model from <code>prebuilt_dir</code> into the work directory</li> <li>Remove stale netCDF output files from any previous run (prevents HDF5 segfaults when     <code>write_netcdf_safely</code> encounters files with an incompatible schema)</li> <li>Set up model directory structure</li> </ul>"},{"location":"user-guide/workflow-stages/#5-sfincs_timing","title":"5. sfincs_timing","text":"<p>Purpose: Set SFINCS model timing.</p> <p>Tasks:</p> <ul> <li>Configure simulation start/end times</li> <li>Set output intervals</li> </ul>"},{"location":"user-guide/workflow-stages/#6-sfincs_forcing","title":"6. sfincs_forcing","text":"<p>Purpose: Add water level forcing.</p> <p>Tasks:</p> <ul> <li>Read boundary point locations from <code>sfincs.bnd</code></li> <li>For TPXO: synthesize tidal water levels from TPXO constituents using harmonic     reconstruction</li> <li>For geodataset sources (STOFS): load the geodataset clipped around the boundary     points, spatially interpolate to boundary locations using inverse-distance weighting     (IDW), and inject into the HydroMT model</li> <li>Apply <code>forcing_to_mesh_offset_m</code> to anchor the forcing signal to the correct height on     the mesh datum (see note below)</li> <li>Emit a warning if the adjusted water levels fall outside the \u00b115 m sanity range</li> <li>Write boundary forcing netCDF (<code>sfincs_netbndbzsbzifile.nc</code>) with a zero-filled <code>bzi</code>     (infragravity) variable required by the SFINCS binary</li> </ul> <p>Forcing vertical datum offset</p> <p>Tidal-only sources like TPXO provide oscillations centred on zero (MSL) but carry no information about where MSL sits on the mesh's vertical datum. The <code>forcing_to_mesh_offset_m</code> parameter anchors the tidal signal to the correct geodetic height on the mesh. For sources already in the mesh datum (e.g. STOFS on a NAVD88 mesh), set this to <code>0.0</code>. The offset can be obtained from the NOAA VDatum API.</p>"},{"location":"user-guide/workflow-stages/#7-sfincs_obs","title":"7. sfincs_obs","text":"<p>Purpose: Add observation points.</p> <p>Tasks:</p> <ul> <li>Add tide gauge locations from <code>observation_points</code></li> <li>Configure observation output</li> </ul>"},{"location":"user-guide/workflow-stages/#8-sfincs_discharge","title":"8. sfincs_discharge","text":"<p>Purpose: Add discharge sources.</p> <p>Tasks:</p> <ul> <li>Add NWM streamflow discharge points from <code>discharge_locations_file</code></li> <li>Filter out source points that fall on inactive grid cells (prevents a SFINCS Fortran     segfault caused by out-of-bounds array access)</li> <li>Generate discharge forcing time series</li> </ul>"},{"location":"user-guide/workflow-stages/#9-sfincs_precip","title":"9. sfincs_precip","text":"<p>Purpose: Add precipitation forcing.</p> <p>Tasks:</p> <ul> <li>Add NWM precipitation data as spatially distributed forcing</li> <li>Set the output resolution to <code>meteo_res</code> (or auto-derive from the quadtree grid)</li> <li>Clip the reprojected grid to the model domain to prevent CONUS-scale inflation</li> </ul>"},{"location":"user-guide/workflow-stages/#10-sfincs_wind","title":"10. sfincs_wind","text":"<p>Purpose: Add wind forcing.</p> <p>Tasks:</p> <ul> <li>Add NWM wind data as spatially distributed forcing</li> <li>Set the output resolution to <code>meteo_res</code> (or auto-derive from the quadtree grid)</li> <li>Clip the reprojected grid to the model domain to prevent CONUS-scale inflation</li> </ul> <p>Runs On: Login node (Python-only)</p>"},{"location":"user-guide/workflow-stages/#11-sfincs_pressure","title":"11. sfincs_pressure","text":"<p>Purpose: Add atmospheric pressure forcing.</p> <p>Tasks:</p> <ul> <li>Add NWM atmospheric pressure data as spatially distributed forcing</li> <li>Set the output resolution to <code>meteo_res</code> (or auto-derive from the quadtree grid)</li> <li>Clip the reprojected grid to the model domain to prevent CONUS-scale inflation</li> <li>Enable barometric pressure correction (<code>baro=1</code>)</li> </ul> <p>Runs On: Login node (Python-only)</p>"},{"location":"user-guide/workflow-stages/#12-sfincs_write","title":"12. sfincs_write","text":"<p>Purpose: Write the final SFINCS model.</p> <p>Tasks:</p> <ul> <li>Write all SFINCS input files (<code>sfincs.inp</code>, <code>sfincs.bnd</code>, etc.)</li> <li>Generate boundary and forcing NetCDF files</li> </ul> <p>Runs On: Login node (Python-only)</p>"},{"location":"user-guide/workflow-stages/#13-sfincs_run","title":"13. sfincs_run","text":"<p>Purpose: Execute the SFINCS model.</p> <p>Tasks:</p> <ul> <li>Run SFINCS inside Singularity container</li> <li>Uses single-node OpenMP parallelism (<code>omp_num_threads</code>)</li> </ul> <p>Runs On: Compute node (OpenMP, inside Singularity)</p>"},{"location":"user-guide/workflow-stages/#14-sfincs_plot","title":"14. sfincs_plot","text":"<p>Purpose: Compare simulated water levels against observations.</p> <p>Tasks:</p> <ul> <li>Read SFINCS output at observation points</li> <li>Apply <code>vdatum_mesh_to_msl_m</code> to convert model output from the mesh datum to MSL</li> <li>Fetch NOAA CO-OPS observed water levels (MLLW converted to MSL using per-station datum     offsets from the CO-OPS API)</li> <li>Generate comparison plots (simulated vs observed)</li> <li>Save figures to the <code>figs/</code> directory</li> </ul> <p>Runs On: Login node or compute node (Python, requires network access)</p> <p>Output datum conversion</p> <p>SFINCS output inherits the vertical datum of the mesh (e.g. NAVD88). The <code>vdatum_mesh_to_msl_m</code> offset converts the simulated water levels to MSL so they can be compared with NOAA CO-OPS observations. This value can be obtained from the NOAA VDatum API.</p>"},{"location":"user-guide/workflow-stages/#running-partial-workflows","title":"Running Partial Workflows","text":"<p>The <code>run</code> command supports <code>--start-from</code> and <code>--stop-after</code>.</p>"},{"location":"user-guide/workflow-stages/#cli","title":"CLI","text":"<pre><code># SCHISM examples\ncoastal-calibration run config.yaml --stop-after download\ncoastal-calibration run config.yaml --start-from pre_forcing --stop-after post_forcing\ncoastal-calibration run config.yaml --start-from boundary_conditions\n\n# SFINCS examples\ncoastal-calibration run config.yaml --stop-after sfincs_write\ncoastal-calibration run config.yaml --start-from sfincs_run\n</code></pre>"},{"location":"user-guide/workflow-stages/#python-api","title":"Python API","text":"<pre><code>from coastal_calibration import CoastalCalibConfig, CoastalCalibRunner\n\nconfig = CoastalCalibConfig.from_yaml(\"config.yaml\")\nrunner = CoastalCalibRunner(config)\n\n# Run specific stages\nresult = runner.run(start_from=\"pre_forcing\", stop_after=\"post_forcing\")\n</code></pre>"},{"location":"user-guide/workflow-stages/#error-handling","title":"Error Handling","text":"<p>If a stage fails:</p> <ol> <li>The workflow stops at the failed stage</li> <li>Error details are logged</li> <li>Subsequent stages are not executed</li> <li>Exit code indicates failure</li> </ol> <p>To resume after fixing an issue:</p> <pre><code># Resume from the failed stage\ncoastal-calibration run config.yaml --start-from &lt;failed_stage&gt;\n</code></pre>"},{"location":"user-guide/workflow-stages/#stage-timing","title":"Stage Timing","text":"<p>When <code>enable_timing</code> is true in the monitoring configuration, stage durations are tracked and reported:</p> <pre><code>Stage timing:\n  download: 45.2s\n  pre_forcing: 12.3s\n  nwm_forcing: 234.5s\n  post_forcing: 8.7s\n  schism_obs: 3.8s\n  update_params: 2.1s\n  boundary_conditions: 156.8s\n  pre_schism: 5.4s\n  schism_run: 1823.6s\n  post_schism: 67.2s\n  schism_plot: 15.4s\nTotal: 2375.0s\n</code></pre>"},{"location":"user-guide/workflow-stages/#sfincs-creation-stages","title":"SFINCS Creation Stages","text":"<p>The <code>create</code> command builds a new SFINCS quadtree model from an AOI polygon. It uses a separate configuration schema (<code>SfincsCreateConfig</code>) and a dedicated runner (<code>SfincsCreator</code>) with resumable execution \u2014 completion is tracked in <code>.create_status.json</code> so that interrupted runs can be resumed with <code>--start-from</code>.</p> <pre><code>flowchart TD\n    A[create_grid] --&gt; B[create_fetch_elevation]\n    B --&gt; C[create_elevation]\n    C --&gt; D[create_mask]\n    D --&gt; E[create_boundary]\n    E --&gt; F[create_subgrid]\n    F --&gt; G[create_write]</code></pre>"},{"location":"user-guide/workflow-stages/#1-create_grid","title":"1. create_grid","text":"<p>Purpose: Generate a SFINCS quadtree grid from the AOI polygon.</p> <p>Tasks:</p> <ul> <li>Read the AOI polygon (GeoJSON, Shapefile, etc.)</li> <li>Create the base grid in the specified CRS</li> <li>Apply quadtree refinement based on configured levels and criteria</li> </ul>"},{"location":"user-guide/workflow-stages/#2-create_fetch_elevation","title":"2. create_fetch_elevation","text":"<p>Purpose: Fetch a NOAA coastal DEM covering the AOI.</p> <p>Tasks:</p> <ul> <li>Query the packaged NOAA DEM spatial index to find the best-matching dataset based on     AOI overlap, resolution, and year</li> <li>Download the DEM tiles from the NOAA <code>noaa-nos-coastal-lidar-pds</code> S3 bucket</li> <li>Mosaic tiles and clip to the AOI extent</li> </ul>"},{"location":"user-guide/workflow-stages/#3-create_elevation","title":"3. create_elevation","text":"<p>Purpose: Add elevation and bathymetry data to the grid.</p> <p>Tasks:</p> <ul> <li>Load configured elevation datasets (e.g., fetched NOAA DEM, NWS topobathy)</li> <li>Interpolate elevation values onto the quadtree grid cells</li> <li>Apply <code>zmin</code>/<code>zmax</code> filters per dataset</li> </ul>"},{"location":"user-guide/workflow-stages/#4-create_mask","title":"4. create_mask","text":"<p>Purpose: Create the active cell mask.</p> <p>Tasks:</p> <ul> <li>Determine which grid cells are active based on elevation thresholds</li> <li>Apply land/water masking criteria</li> </ul>"},{"location":"user-guide/workflow-stages/#5-create_boundary","title":"5. create_boundary","text":"<p>Purpose: Create water level boundary cells.</p> <p>Tasks:</p> <ul> <li>Identify grid cells along the open ocean boundary</li> <li>Assign boundary condition flags</li> </ul>"},{"location":"user-guide/workflow-stages/#6-create_subgrid","title":"6. create_subgrid","text":"<p>Purpose: Generate subgrid lookup tables.</p> <p>Tasks:</p> <ul> <li>Compute high-resolution subgrid tables from the DEM</li> <li>These tables allow SFINCS to use coarse computational cells while capturing fine-scale     topographic detail</li> </ul>"},{"location":"user-guide/workflow-stages/#7-create_write","title":"7. create_write","text":"<p>Purpose: Write the complete SFINCS model to disk.</p> <p>Tasks:</p> <ul> <li>Write all SFINCS input files to the configured <code>output_dir</code></li> <li>The output directory can be used as <code>prebuilt_dir</code> in a simulation config</li> </ul>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":""},{"location":"CHANGELOG/#added","title":"Added","text":"<ul> <li><code>create</code> CLI command for building SFINCS quadtree models from an AOI polygon. The     workflow supports grid generation with quadtree refinement, automatic NOAA DEM     discovery and download, elevation/bathymetry, active cell mask, boundary cells,     optional NWM discharge source points, and subgrid table generation.</li> <li><code>prepare-topobathy</code> CLI command for downloading NWS 30 m topobathymetric DEMs from     <code>icechunk</code> (S3) clipped to an AOI bounding box, with HydroMT data catalog output.</li> <li><code>update-dem-index</code> CLI command for rebuilding the NOAA DEM spatial index from S3 STAC     metadata.</li> <li><code>SfincsCreateConfig</code> configuration schema for the <code>create</code> workflow, with sections for     grid, elevation, mask, subgrid, and optional NWM discharge.</li> <li><code>SfincsCreator</code> runner with resumable execution (tracks completion in     <code>.create_status.json</code>) and <code>--start-from</code>/<code>--stop-after</code> support.</li> <li>NOAA DEM auto-discovery utility (<code>noaa_dem.py</code>) that selects the best-matching coastal     DEM based on AOI overlap, resolution, and year.</li> <li>NWS topobathy fetch utility (<code>topobathy.py</code>) for domain-specific DEM downloads via     <code>icechunk</code>.</li> </ul>"},{"location":"CHANGELOG/#changed","title":"Changed","text":"<ul> <li>Remove the <code>submit</code> execution path and <code>SlurmConfig</code> entirely. The <code>run</code> command is     now the sole entry point \u2014 use it inside user-written <code>sbatch</code> scripts instead. Old     YAML configs with a <code>slurm:</code> key are silently ignored for backward compatibility.     The <code>${slurm.user}</code> path-template alias continues to work, resolving from the     <code>$USER</code> environment variable.</li> <li>Replace <code>assert isinstance(...)</code> with <code>typing.cast()</code> for <code>SchismModelConfig</code> type     narrowing in all stage modules (<code>boundary</code>, <code>forcing</code>, <code>schism</code>), since <code>assert</code>     statements are stripped by <code>python -O</code>.</li> <li>Switch type checker from <code>pyright</code> to <code>ty</code>.</li> <li>Switch documentation theme from <code>mkdocs-material</code> to <code>mkdocs-materialx</code>.</li> </ul>"},{"location":"CHANGELOG/#fixed","title":"Fixed","text":"<ul> <li>Resolve all <code>ty</code> type-checker diagnostics across the codebase: invalid-assignment in     <code>coops_api.get_datums</code>, stale <code>type: ignore</code> comments in <code>_hydromt_compat</code>,     <code>download</code>, and <code>runner</code>, a possible <code>None</code> dereference in <code>logging</code> file-handler     stream reconfiguration, and dead <code>_work_dir</code> code path in <code>WorkflowMonitor</code>.</li> </ul>"},{"location":"CHANGELOG/#removed","title":"Removed","text":"<ul> <li><code>SlurmConfig</code>, <code>SlurmManager</code>, <code>JobState</code>, and the <code>submit</code> CLI command.</li> <li><code>utils/slurm.py</code> module.</li> <li>All submit/slurm-related tests and fixtures.</li> </ul>"},{"location":"CHANGELOG/#31100-2026-02-19","title":"3.1.1.0.0 - 2026-02-19","text":""},{"location":"CHANGELOG/#added_1","title":"Added","text":"<ul> <li>Initial release of NWM Coastal</li> <li>SCHISM coastal model workflow support</li> <li>YAML configuration with variable interpolation</li> <li>Configuration inheritance with <code>_base</code> field</li> <li>CLI commands: <code>init</code>, <code>validate</code>, <code>submit</code>, <code>run</code>, <code>stages</code></li> <li>Python API for programmatic workflow control</li> <li>Automatic data download from NWM and STOFS sources</li> <li>Support for TPXO and STOFS boundary conditions</li> <li>Support for four coastal domains: Hawaii, PRVI, Atlantic/Gulf, Pacific</li> <li>Interactive and non-interactive job submission modes</li> <li>Partial workflow execution with <code>--start-from</code> and <code>--stop-after</code></li> <li>Smart default paths with interpolation templates</li> <li>Comprehensive configuration validation</li> <li>MkDocs documentation with Material theme</li> <li>Per-stage completion tracking in the <code>submit</code> path's generated runner script: each     container stage is recorded in <code>.pipeline_status.json</code> as it finishes, enabling     mid-pipeline restarts after a SLURM job failure without re-running expensive stages     (e.g., <code>predict_tide</code>). On resubmit, completed stages are automatically skipped.</li> <li><code>meteo_res</code> option in <code>SfincsModelConfig</code> to control the output resolution (m) of     gridded meteorological forcing (precipitation, wind, pressure). When not set, the     resolution is derived from the SFINCS quadtree grid base cell size.</li> <li>Meteo grid clipping (<code>_clip_meteo_to_domain</code>) that trims reprojected meteo grids to     the model domain extent, preventing the LCC \u2192 UTM reprojection from inflating grids     to CONUS scale \u2014 reducing SFINCS runtime from 15 h+ to under 15 min.</li> <li>Stale netCDF file cleanup in <code>SfincsInitStage</code> to prevent HDF5 segfaults when     re-running a pipeline over an existing model directory.</li> <li><code>GeoDataset</code>-based water-level forcing with IDW interpolation to boundary points,     replacing the built-in <code>model.water_level.create(geodataset=...)</code> which passed all     source stations incompatibly with <code>.bnd</code> files.</li> <li>Active-cell filtering for discharge source points to prevent a SFINCS Fortran segfault     when a source point falls on an inactive grid cell.</li> <li><code>apply_all_patches()</code> convenience function in <code>_hydromt_compat</code> that applies all     <code>hydromt</code>/<code>hydromt-sfincs</code> compatibility patches in one call, with logging.</li> <li><code>quiet</code> parameter on <code>WorkflowMonitor.mark_stage_completed()</code> to control whether a     visible COMPLETED log line is emitted for externally-executed stages.</li> <li>Unified <code>run</code> and <code>submit</code> execution pipelines \u2014 both commands now execute the same     stage pipeline. <code>submit</code> automatically partitions stages into login-node     (Python-only) and SLURM job (container) groups.</li> <li><code>--start-from</code> and <code>--stop-after</code> options for <code>submit</code> command, matching <code>run</code></li> <li><code>requires_container</code> class attribute on <code>WorkflowStage</code> for automatic stage     classification (Python-only vs container)</li> <li><code>schism_obs</code> stage: automatic NOAA CO-OPS water level station discovery via concave     hull of open boundary nodes, writing <code>station.in</code> and <code>station_noaa_ids.txt</code></li> <li><code>schism_plot</code> stage: post-run comparison plots of simulated vs NOAA-observed water     levels with MLLW\u2192MSL datum conversion</li> <li><code>COOPSAPIClient</code> for querying the NOAA CO-OPS API (station metadata, water levels,     datums) with local caching of station metadata</li> <li><code>include_noaa_gages</code> option in <code>SchismModelConfig</code> (defaults to <code>false</code>) that enables     the <code>schism_obs</code> and <code>schism_plot</code> stages</li> <li>Automatic <code>param.nml</code> patching (<code>iout_sta = 1</code>, <code>nspool_sta = 18</code>) when <code>station.in</code>     exists, ensuring <code>mod(nhot_write, nspool_sta) == 0</code> across all domain templates</li> <li><code>forcing_to_mesh_offset_m</code> option in <code>SfincsModelConfig</code> to apply a vertical offset to     boundary-condition water levels before they enter SFINCS. For tidal-only sources     like TPXO, this anchors the tidal signal to the correct geodetic height of MSL on     the mesh.</li> <li><code>vdatum_mesh_to_msl_m</code> option in <code>SfincsModelConfig</code> to convert SFINCS output from the     mesh vertical datum to MSL for comparison with NOAA CO-OPS observations.</li> <li>Sanity-check warning in <code>sfincs_forcing</code> when adjusted boundary water levels fall     outside the \u00b115 m range, indicating a possible sign or magnitude error in     <code>forcing_to_mesh_offset_m</code>.</li> <li><code>sfincs_wind</code>, <code>sfincs_pressure</code>, and <code>sfincs_plot</code> stages to SFINCS workflow</li> <li>SFINCS coastal model workflow with full pipeline (download through <code>sfincs_run</code>)</li> <li>Polymorphic <code>ModelConfig</code> ABC with <code>SchismModelConfig</code> and <code>SfincsModelConfig</code>     concrete implementations</li> <li><code>MODEL_REGISTRY</code> for automatic model dispatch from YAML <code>model:</code> key</li> <li><code>--model</code> option for <code>init</code> and <code>stages</code> CLI commands</li> <li>Model-specific compute parameters (SCHISM: multi-node MPI; SFINCS: single-node OpenMP)</li> <li><code>${model}</code> variable in default path templates for model-aware directory naming</li> </ul>"},{"location":"CHANGELOG/#changed_1","title":"Changed","text":"<ul> <li><code>DownloadStage.description</code> is now a property that derives its text from the     configured data sources (e.g. \"Download input data (NWM, TPXO)\") instead of a static     string.</li> <li><code>hydromt</code> compatibility patches consolidated into <code>apply_all_patches()</code> with per-patch     logging; individual imports replaced by a single call.</li> <li><code>CoastalCalibConfig</code> now takes <code>model_config: ModelConfig</code> instead of separate     <code>model</code>, <code>mpi</code>, and <code>sfincs</code> parameters</li> <li><code>SlurmConfig</code> now contains only scheduling parameters (<code>job_name</code>, <code>partition</code>,     <code>time_limit</code>, <code>account</code>, <code>qos</code>, <code>user</code>); compute resources (<code>nodes</code>,     <code>ntasks_per_node</code>, <code>exclusive</code>) moved to <code>SchismModelConfig</code></li> <li>Default path templates use <code>${model}_</code> prefix instead of hardcoded <code>schism_</code></li> <li>Stage order and stage creation delegated to <code>ModelConfig</code> subclasses</li> <li>SFINCS datum handling split into two separate offsets: the former single     <code>navd88_to_msl_m</code> field is replaced by <code>forcing_to_mesh_offset_m</code> (applied to     boundary forcing before simulation) and <code>vdatum_mesh_to_msl_m</code> (applied to model     output for observation comparison). The two offsets serve fundamentally different     purposes and may have different values depending on the boundary source.</li> <li>SFINCS field renames: <code>model_dir</code> -&gt; <code>prebuilt_dir</code>, <code>obs_points</code> -&gt;     <code>observation_points</code>, <code>obs_merge</code> -&gt; <code>merge_observations</code>, <code>src_locations</code> -&gt;     <code>discharge_locations_file</code>, <code>src_merge</code> -&gt; <code>merge_discharge</code>, <code>docker_tag</code> -&gt;     <code>container_tag</code>, <code>sif_path</code> -&gt; <code>container_image</code></li> </ul>"},{"location":"CHANGELOG/#fixed_1","title":"Fixed","text":"<ul> <li>Call <code>expanduser()</code> before <code>resolve()</code> on all path config fields so that paths     containing <code>~</code> are correctly expanded to the user's home directory.</li> <li>Call <code>monitor.end_workflow()</code> before returning early in no-wait mode (<code>submit</code> with     <code>wait=False</code>), so that the workflow timing summary is always closed.</li> <li>Set <code>HDF5_USE_FILE_LOCKING=FALSE</code> in container environment to prevent     <code>PermissionError</code> on NFS-mounted filesystems.</li> <li>Add conda environment paths (<code>PATH</code>, <code>LD_LIBRARY_PATH</code>) to the <code>run</code> path's     <code>build_environment()</code> so that <code>mpiexec</code> and MPI shared libraries from the conda     environment are found, matching the environment set up by the generated <code>submit</code>     scripts. Without these paths, the <code>run</code> path could not locate <code>mpiexec</code>, causing MPI     stages to hang or fail.</li> <li>Add MPI/EFA fabric tuning variables (<code>MPICH_OFI_STARTUP_CONNECT</code>,     <code>FI_OFI_RXM_SAR_LIMIT</code>, etc.) to the <code>run</code> path's SCHISM environment, matching the     <code>submit</code> path and preventing hangs on AWS <code>c5n</code> nodes.</li> <li>Suppress ESMF diagnostic output from SLURM logs by redirecting stdout to <code>/dev/null</code>     for MPI stages and setting <code>ESMF.Manager(debug=False)</code>.</li> <li>Redirect container stdout/stderr to temporary files instead of pipes to prevent     pipe-buffer deadlocks with MPI process trees (<code>mpiexec</code> \u2192 <code>singularity</code>), where     inherited pipe file-descriptors in child processes can fill the OS pipe buffer (64     KB on Linux) and deadlock the entire tree.</li> <li>Use <code>$COASTAL_DOMAIN</code> instead of hardcoded <code>prvi</code> in <code>make_tpxo_ocean.bash</code> so the     correct open-boundary mesh is used for all domains.</li> <li>Add missing <code>$</code> in <code>${PDY}</code> variable expansion in <code>post_regrid_stofs.bash</code> log     filename.</li> <li>Correct malformed shebangs (<code>#/usr/bin/evn</code>) in <code>pre_nwm_forcing_coastal.bash</code> and     <code>post_nwm_forcing_coastal.bash</code>.</li> <li>Use integer division (<code>//</code>) for the netCDF array index in <code>WrfHydroFECPP/fecpp/app.py</code>     to avoid <code>float</code> index errors.</li> <li>Use numeric comparison (<code>-gt</code>) instead of string comparison (<code>&gt;</code>) for <code>LENGTH_HRS</code> in     <code>update_param.bash</code>.</li> <li>Add missing sub-hourly <code>CHRTOUT</code> symlinks for Hawaii in the last-timestep block of     <code>initial_discharge.bash</code>.</li> <li>Read <code>NSCRIBES</code> from the environment with a fallback default instead of hardcoding it     in <code>pre_schism.bash</code> and <code>run_sing_coastal_workflow_post_schism.bash</code>.</li> <li>Compute <code>LENGTH_HRS</code> in <code>STOFSBoundaryStage</code> directly instead of parsing stdout from     the pre-script, which was silently lost after the <code>Popen.communicate()</code> fix     redirected stdout to <code>/dev/null</code>.</li> <li>Remove duplicate domain-to-inland/geogrid mappings in <code>runner.py</code> and use the     canonical properties from <code>SimulationConfig</code> to prevent the two copies from drifting     out of sync.</li> <li>Correct shebangs (<code>#!/usr/bin/bash</code> \u2192 <code>#!/usr/bin/env bash</code>) in     <code>pre_regrid_stofs.bash</code> and <code>post_regrid_stofs.bash</code> for consistency and     portability.</li> <li>Source inner bash scripts from <code>$SCRIPTS_DIR</code> instead of <code>./</code> in all wrapper scripts,     so that the bind-mounted (package) versions are used rather than the stale copies     baked into the container image.</li> <li>Export <code>COASTAL_SCRIPTS_DIR</code>, <code>WRF_HYDRO_DIR</code>, <code>TPXO_SCRIPTS_DIR</code>, and     <code>FORCINGS_SCRIPTS_DIR</code> in the <code>submit</code> path's generated runner script \u2014 these     variables were only set in the <code>run</code> path, causing     <code>$COASTAL_SCRIPTS_DIR/makeAtmo.py</code> (and similar) to resolve to just <code>/makeAtmo.py</code>     and fail silently.</li> <li>Export date-component variables (<code>FORCING_START_YEAR</code>, <code>FORCING_START_MONTH</code>,     <code>FORCING_START_DAY</code>, <code>FORCING_START_HOUR</code>, <code>PDY</code>, <code>cyc</code>, <code>FORCING_BEGIN_DATE</code>,     <code>FORCING_END_DATE</code>, <code>END_DATETIME</code>) in the <code>submit</code> path header so that     <code>makeAtmo.py</code>, <code>makeDischarge.py</code>, and other Python scripts inside the container     have access to them across all stages.</li> <li>Add <code>set -e</code> to all inner bash scripts (<code>post_nwm_forcing_coastal.bash</code>,     <code>initial_discharge.bash</code>, <code>merge_source_sink.bash</code>, <code>combine_sink_source.bash</code>,     <code>pre_nwm_forcing_coastal.bash</code>, <code>post_regrid_stofs.bash</code>, <code>pre_regrid_stofs.bash</code>,     <code>make_tpxo_ocean.bash</code>, <code>pre_schism.bash</code>, <code>post_schism.bash</code>, <code>update_param.bash</code>)     so that command failures (e.g., <code>python</code> file-not-found or import errors) propagate     instead of being silently swallowed.</li> <li>Correct shebang in <code>make_tpxo_ocean.bash</code> and <code>pre_schism.bash</code> (<code>#!/usr/bin/bash</code> \u2192     <code>#!/usr/bin/env bash</code>).</li> <li>Copy <code>setup_tpxo.txt</code> and <code>Model_tpxo10_atlas</code> from <code>$SCRIPTS_DIR</code> instead of <code>./</code> in     <code>make_tpxo_ocean.bash</code>, so the bind-mounted (package) versions are used rather than     stale copies baked into the container image.</li> <li>Truncate discharge arrays in <code>merge_source_sink.py</code> to match the precipitation     timestep count from <code>precip_source.nc</code>, preventing a shape-mismatch <code>ValueError</code>     when sub-hourly <code>CHRTOUT</code> files (e.g., Hawaii) produce one extra trailing timestep.</li> <li>Export <code>SCHISM_BEGIN_DATE</code> and <code>SCHISM_END_DATE</code> in the <code>submit</code> path header so that     <code>update_param.bash</code> can patch <code>param.nml</code> with the correct simulation start/end     dates \u2014 without these, <code>param.nml</code> retains its template defaults (2000-01-01) and     SCHISM aborts with a time mismatch against <code>sflux</code> forcing files.</li> <li>Report accurately which container stages completed vs failed when a SLURM job ends     with a non-zero exit status, instead of marking all container stages as failed.</li> </ul>"},{"location":"CHANGELOG/#removed_1","title":"Removed","text":"<ul> <li><code>MPIConfig</code> class (fields absorbed into <code>SchismModelConfig</code>)</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#contributing-to-nwm-coastal","title":"Contributing to NWM Coastal","text":"<p>First off, thanks for taking the time to contribute! \u2764\ufe0f</p> <p>All types of contributions are encouraged and valued. See the Table of Contents for different ways to help and details about how this project handles them. Please make sure to read the relevant section before making your contribution. It will make it a lot easier for us maintainers and smooth out the experience for all involved. The community looks forward to your contributions. \ud83c\udf89</p> <p>And if you like the project, but just don't have time to contribute, that's fine. There are other easy ways to support the project and show your appreciation, which we would also be very happy about:</p> <ul> <li>Star the project</li> <li>Tweet about it</li> <li>Refer this project in your project's readme</li> <li>Mention the project at local meetups and tell your friends/colleagues</li> </ul>"},{"location":"CONTRIBUTING/#table-of-contents","title":"Table of Contents","text":"<ul> <li>I Have a Question</li> <li>I Want To Contribute<ul> <li>Reporting Bugs</li> <li>Suggesting Enhancements</li> <li>Your First Code Contribution</li> <li>Improving The Documentation</li> </ul> </li> </ul>"},{"location":"CONTRIBUTING/#i-have-a-question","title":"I Have a Question","text":"<p>If you want to ask a question, we assume that you have read the available Documentation.</p> <p>Before you ask a question, it is best to search for existing Issues that might help you. In case you have found a suitable issue and still need clarification, you can write your question in this issue. It is also advisable to search the internet for answers first.</p> <p>If you then still feel the need to ask a question and need clarification, we recommend the following:</p> <ul> <li>Open an Issue.</li> <li>Provide as much context as you can about what you're running into.</li> <li>Provide project and platform versions (<code>python</code>, <code>pixi</code>, etc), depending on what seems     relevant.</li> </ul> <p>We will then take care of the issue as soon as possible.</p>"},{"location":"CONTRIBUTING/#i-want-to-contribute","title":"I Want To Contribute","text":""},{"location":"CONTRIBUTING/#legal-notice","title":"Legal Notice","text":"<p>When contributing to this project, you must agree that you have authored 100% of the content, that you have the necessary rights to the content and that the content you contribute may be provided under the project license.</p>"},{"location":"CONTRIBUTING/#reporting-bugs","title":"Reporting Bugs","text":""},{"location":"CONTRIBUTING/#before-submitting-a-bug-report","title":"Before Submitting a Bug Report","text":"<p>A good bug report shouldn't leave others needing to chase you up for more information. Therefore, we ask you to investigate carefully, collect information and describe the issue in detail in your report. Please complete the following steps in advance to help us fix any potential bug as fast as possible.</p> <ul> <li>Make sure that you are using the latest version.</li> <li>Determine if your bug is really a bug and not an error on your side e.g. using     incompatible environment components/versions (Make sure that you have read the     documentation. If you are looking for     support, you might want to check this section).</li> <li>To see if other users have experienced (and potentially already solved) the same issue     you are having, check if there is not already a bug report existing for your bug or     error in the     bug tracker.</li> <li>Also make sure to search the internet (including Stack Overflow) to see if users     outside of the GitHub community have discussed the issue.</li> <li>Collect information about the bug:<ul> <li>Stack trace (Traceback)</li> <li>OS, Platform and Version (Windows, Linux, macOS, x86, ARM)</li> <li>Version of the interpreter, compiler, SDK, runtime environment, package manager,     depending on what seems relevant.</li> <li>Possibly your input and the output</li> <li>Can you reliably reproduce the issue? And can you also reproduce it with older     versions?</li> </ul> </li> </ul>"},{"location":"CONTRIBUTING/#how-do-i-submit-a-good-bug-report","title":"How Do I Submit a Good Bug Report?","text":"<p>You must never report security related issues, vulnerabilities or bugs including sensitive information to the issue tracker, or elsewhere in public. Instead sensitive bugs must be sent by email to the project maintainers.</p> <p>We use GitHub issues to track bugs and errors. If you run into an issue with the project:</p> <ul> <li>Open an Issue. (Since we can't be     sure at this point whether it is a bug or not, we ask you not to talk about a bug     yet and not to label the issue.)</li> <li>Explain the behavior you would expect and the actual behavior.</li> <li>Please provide as much context as possible and describe the reproduction steps that     someone else can follow to recreate the issue on their own. This usually includes     your code. For good bug reports you should isolate the problem and create a reduced     test case.</li> <li>Provide the information you collected in the previous section.</li> </ul> <p>Once it's filed:</p> <ul> <li>The project team will label the issue accordingly.</li> <li>A team member will try to reproduce the issue with your provided steps. If there are     no reproduction steps or no obvious way to reproduce the issue, the team will ask     you for those steps and mark the issue as <code>needs-repro</code>. Bugs with the <code>needs-repro</code>     tag will not be addressed until they are reproduced.</li> <li>If the team is able to reproduce the issue, it will be marked <code>needs-fix</code>, as well as     possibly other tags (such as <code>critical</code>), and the issue will be left to be     implemented by someone.</li> </ul>"},{"location":"CONTRIBUTING/#suggesting-enhancements","title":"Suggesting Enhancements","text":"<p>This section guides you through submitting an enhancement suggestion for NWM Coastal, including completely new features and minor improvements to existing functionality. Following these guidelines will help maintainers and the community to understand your suggestion and find related suggestions.</p>"},{"location":"CONTRIBUTING/#before-submitting-an-enhancement","title":"Before Submitting an Enhancement","text":"<ul> <li>Make sure that you are using the latest version.</li> <li>Read the documentation carefully and find out     if the functionality is already covered, maybe by an individual configuration.</li> <li>Perform a search to see if the     enhancement has already been suggested. If it has, add a comment to the existing     issue instead of opening a new one.</li> <li>Find out whether your idea fits with the scope and aims of the project. It's up to you     to make a strong case to convince the project's developers of the merits of this     feature. Keep in mind that we want features that will be useful to the majority of     our users and not just a small subset. If you're just targeting a minority of users,     consider writing an add-on/plugin library.</li> </ul>"},{"location":"CONTRIBUTING/#how-do-i-submit-a-good-enhancement-suggestion","title":"How Do I Submit a Good Enhancement Suggestion?","text":"<p>Enhancement suggestions are tracked as GitHub issues.</p> <ul> <li>Use a clear and descriptive title for the issue to identify the suggestion.</li> <li>Provide a step-by-step description of the suggested enhancement in as many details     as possible.</li> <li>Describe the current behavior and explain which behavior you expected to see     instead and why. At this point you can also tell which alternatives do not work     for you.</li> <li>Explain why this enhancement would be useful to most NWM Coastal users. You may     also want to point out the other projects that solved it better and which could     serve as inspiration.</li> </ul>"},{"location":"CONTRIBUTING/#your-first-code-contribution","title":"Your First Code Contribution","text":"<p>Ready to contribute? Here's how to set up NWM Coastal for local development.</p> <ol> <li> <p>Fork the NWM Coastal repo through the GitHub website.</p> </li> <li> <p>Clone your fork locally and add the main <code>nwm-coastal</code> as the upstream remote:</p> <pre><code>git clone git@github.com:your_name_here/nwm-coastal.git\ngit remote add upstream git@github.com:NGWPC/nwm-coastal.git\n</code></pre> </li> <li> <p>Install Pixi then install the development environments:</p> <pre><code>cd nwm-coastal/\npixi install -e dev\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>git checkout -b bugfix-or-feature/name-of-your-bugfix-or-feature\ngit push\n</code></pre> </li> <li> <p>Now you can make your changes locally, make sure to add a description of the changes     to <code>CHANGELOG.md</code> file based on     Keep a Changelog and add extra tests, if     applicable, to <code>tests</code> folder. Also, make sure to give yourself credit by adding     your name at the end of the item(s) that you add in the history like this     <code>by [Your Name](https://github.com/your_handle)</code>. Then, fetch the latest updates from     the remote and resolve any merge conflicts:</p> <pre><code>git fetch upstream\ngit merge upstream/development\n</code></pre> </li> <li> <p>Then run linting, type checking, and tests:</p> <pre><code>pixi r lint\npixi r -e typecheck typecheck\npixi r -e test311 test\npixi r -e test314 test\n</code></pre> </li> <li> <p>If you are making breaking changes make sure to reflect them in the documentation,     <code>README.md</code>, and tests if necessary.</p> </li> <li> <p>Commit your changes and push your branch to GitHub following     Conventional Commits     specification. For example:</p> <pre><code>git add .\ngit commit -m \"feat: a detailed description of your changes.\"\ngit push origin name-of-your-branch\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"CONTRIBUTING/#improving-the-documentation","title":"Improving The Documentation","text":"<p>NWM Coastal could always use more documentation, whether as part of the official NWM Coastal docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"CONTRIBUTING/#attribution","title":"Attribution","text":"<p>This guide is based on the contributing-gen. Make your own!</p>"},{"location":"DESIGN/","title":"coastal-calibration: Design Documentation","text":""},{"location":"DESIGN/#overview","title":"Overview","text":"<p>The <code>coastal-calibration</code> Python package is a complete redesign and rewrite of the original bash-based SCHISM model calibration workflow. This document details the architectural improvements, design decisions, and substantial enhancements made over the original implementation.</p>"},{"location":"DESIGN/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Executive Summary</li> <li>Original Implementation Analysis</li> <li>New Architecture</li> <li>Key Design Decisions</li> <li>Substantial Improvements</li> <li>API Reference</li> <li>Potential Future Developments</li> </ol>"},{"location":"DESIGN/#executive-summary","title":"Executive Summary","text":"<p>The <code>coastal-calibration</code> package provides a modern Python interface for running SCHISM and SFINCS coastal model calibration workflows on HPC clusters. It wraps the existing operational workflow scripts with a clean, type-safe API while establishing the foundation for incremental improvements.</p>"},{"location":"DESIGN/#design-goals","title":"Design Goals","text":"<p>The primary objectives of this rewrite are to create a workflow that is:</p> <ol> <li>Intuitive and user-friendly - Simple YAML configuration, clear CLI commands,     helpful error messages</li> <li>Less prone to errors - Type-safe configuration, comprehensive validation,     structured logging</li> <li>Extensible - Polymorphic model architecture that supports SCHISM, SFINCS, and     future models via a common <code>ModelConfig</code> ABC</li> </ol>"},{"location":"DESIGN/#architectural-strategy","title":"Architectural Strategy","text":"<p>The package is designed with a stable public API that shields users from internal changes. This enables:</p> <ul> <li>Immediate usability - Users get a clean interface today, even while internals are     being improved</li> <li>Incremental rewriting - Embedded bash scripts can be replaced with pure Python one     stage at a time</li> <li>Safe evolution - Internal rewrites don't break user-facing code or configurations</li> </ul> <p>The long-term goal is to completely rewrite all embedded bash scripts in Python, but doing so incrementally allows the package to be useful immediately while that work proceeds.</p>"},{"location":"DESIGN/#key-features","title":"Key Features","text":"<ul> <li>Type-safe configuration via <code>dataclasses</code> with runtime validation</li> <li>Modular stage-based architecture for maintainability and extensibility</li> <li>Native Python datetime handling replacing fragile shell date arithmetic</li> <li>Async data downloading with built-in source validation</li> <li>CLI and programmatic APIs for both interactive and automated use</li> <li>Progress tracking and structured logging</li> <li>Configuration inheritance for DRY multi-run setups</li> <li>Smart default paths with variable interpolation</li> </ul>"},{"location":"DESIGN/#original-implementation-analysis","title":"Original Implementation Analysis","text":""},{"location":"DESIGN/#file-structure-20-scripts","title":"File Structure (20+ scripts)","text":"<pre><code>calib_org/\n\u251c\u2500\u2500 sing_run.bash                     # Main entry point (258 lines)\n\u251c\u2500\u2500 schism_calib.cfg                  # Configuration file\n\u251c\u2500\u2500 pre_nwm_forcing_coastal.bash      # Forcing preparation\n\u251c\u2500\u2500 post_nwm_forcing_coastal.bash     # Forcing post-processing\n\u251c\u2500\u2500 make_tpxo_ocean.bash              # TPXO boundary conditions\n\u251c\u2500\u2500 pre_regrid_stofs.bash             # STOFS pre-processing\n\u251c\u2500\u2500 post_regrid_stofs.bash            # STOFS post-processing\n\u251c\u2500\u2500 update_param.bash                 # Parameter file updates (249 lines)\n\u251c\u2500\u2500 pre_schism.bash                   # SCHISM input preparation\n\u251c\u2500\u2500 post_schism.bash                  # SCHISM output processing\n\u251c\u2500\u2500 merge_source_sink.bash            # Discharge file merging\n\u251c\u2500\u2500 initial_discharge.bash            # Initial discharge creation\n\u251c\u2500\u2500 combine_sink_source.bash          # Sink/source combination\n\u2514\u2500\u2500 run_sing_coastal_workflow_*.bash  # 8+ Singularity wrappers\n</code></pre>"},{"location":"DESIGN/#critical-issues-in-original-implementation","title":"Critical Issues in Original Implementation","text":""},{"location":"DESIGN/#1-fragile-date-arithmetic","title":"1. Fragile Date Arithmetic","text":"<p>The original workflow relied on external scripts for date calculations:</p> <pre><code># Original: External script calls for every date operation\nexport FORCING_END_DATE=$(${USHnwm}/utils/advance_time.sh $PDY$cyc $LENGTH_HRS)'00'\npdycyc=$(${USHnwm}/utils/advance_time.sh $PDY$cyc $hr)\n</code></pre> <p>This approach had several problems:</p> <ul> <li>Required external <code>advance_time.sh</code> and <code>advance_cymdh.pl</code> scripts</li> <li>Shell spawning overhead for each date operation</li> <li>Inconsistent handling of edge cases (leap years, month boundaries)</li> <li>No error handling for invalid dates</li> </ul>"},{"location":"DESIGN/#2-environment-variable-pitfalls","title":"2. Environment Variable Pitfalls","text":"<p>The original scripts passed dozens of environment variables between scripts:</p> <pre><code># Original configuration (schism_calib.cfg)\nexport STARTPDY=20230611\nexport STARTCYC=00\nexport FCST_LENGTH_HRS=3.0\nexport HOT_START_FILE=''\nexport USE_TPXO=\"NO\"\nexport COASTAL_DOMAIN=pacific\nexport METEO_SOURCE=NWM_RETRO\nexport COASTAL_WORK_DIR=/efs/schism_use_case/...\n\n# Plus 40+ more in sing_run.bash\nexport NGWPC_COASTAL_PARM_DIR=/ngen-test/coastal/ngwpc-coastal\nexport NGEN_APP_DIR=/ngen-app\nexport FCST_TIMESTEP_LENGTH_SECS=3600\nexport OTPSDIR=$NGEN_APP_DIR/OTPSnc\n# ... etc\n</code></pre> <p>Problems:</p> <ul> <li>No validation of variable values</li> <li>Easy to have typos that fail silently</li> <li>Difficult to track variable dependencies</li> <li>No documentation of which variables are required vs optional</li> </ul>"},{"location":"DESIGN/#3-string-based-domain-mapping","title":"3. String-Based Domain Mapping","text":"<pre><code># Original: Repeated in multiple files\ndeclare -A coastal_domain_to_inland_domain=( \\\n    [prvi]=\"domain_puertorico\" \\\n    [hawaii]=\"domain_hawaii\" \\\n    [atlgulf]=\"domain\" \\\n    [pacific]=\"domain\" )\n\ndeclare -A coastal_domain_to_nwm_domain=( \\\n    [prvi]=\"prvi\" \\\n    [hawaii]=\"hawaii\" \\\n    [atlgulf]=\"conus\" \\\n    [pacific]=\"conus\" )\n\ndeclare -A coastal_domain_to_geo_grid=( \\\n    [prvi]=\"geo_em_PRVI.nc\" \\\n    [hawaii]=\"geo_em_HI.nc\" \\\n    [atlgulf]=\"geo_em_CONUS.nc\" \\\n    [pacific]=\"geo_em_CONUS.nc\" )\n</code></pre> <p>Problems:</p> <ul> <li>Duplicated across multiple scripts</li> <li>No compile-time type checking</li> <li>Silent failures on unknown domains</li> </ul>"},{"location":"DESIGN/#4-no-data-download-integration","title":"4. No Data Download Integration","text":"<p>The original workflow required manual data downloading via a separate workflow. That workflow had no date validation, no source awareness, and no progress tracking.</p>"},{"location":"DESIGN/#5-minimal-error-handling","title":"5. Minimal Error Handling","text":"<pre><code># Original: Scripts would continue on failure\nsingularity exec -B $BINDINGS --pwd ${work_dir} $SIF_PATH \\\n    ./run_sing_coastal_workflow_pre_forcing_coastal.bash\n# No error check here\n\n${MPICOMMAND3} singularity exec -B $BINDINGS \\\n    --pwd ${work_dir} \\\n    $SIF_PATH \\\n    $CONDA_ENVS_PATH/$CONDA_ENV_NAME/bin/python \\\n    $USHnwm/wrf_hydro_workflow_dev/forcings/WrfHydroFECPP/workflow_driver.py\n# No error check here either\n</code></pre>"},{"location":"DESIGN/#new-architecture","title":"New Architecture","text":""},{"location":"DESIGN/#package-structure","title":"Package Structure","text":"<pre><code>src/coastal_calibration/\n\u251c\u2500\u2500 __init__.py                  # Package exports\n\u251c\u2500\u2500 cli.py                       # Command-line interface\n\u251c\u2500\u2500 runner.py                    # Main workflow orchestrator\n\u251c\u2500\u2500 downloader.py                # Async data downloading\n\u251c\u2500\u2500 scripts_path.py              # Script path management\n\u2502\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 schema.py                # YAML config dataclasses + ModelConfig ABC\n\u2502\n\u251c\u2500\u2500 stages/                      # Workflow stages\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 base.py                  # Abstract WorkflowStage base class\n\u2502   \u251c\u2500\u2500 download.py              # Data download stage\n\u2502   \u251c\u2500\u2500 forcing.py               # NWM forcing stages\n\u2502   \u251c\u2500\u2500 boundary.py              # Boundary condition stages\n\u2502   \u251c\u2500\u2500 schism.py                # SCHISM execution stages\n\u2502   \u251c\u2500\u2500 sfincs.py                # SFINCS data catalog &amp; symlinks\n\u2502   \u251c\u2500\u2500 sfincs_build.py          # SFINCS model build stages (HydroMT)\n\u2502   \u2514\u2500\u2500 _hydromt_compat.py       # Compatibility patches for hydromt bugs\n\u2502\n\u251c\u2500\u2500 scripts/                     # Embedded bash scripts\n\u2502   \u251c\u2500\u2500 tpxo_to_open_bnds_hgrid/ # TPXO Python utilities\n\u2502   \u2514\u2500\u2500 wrf_hydro_workflow_dev/  # WRF-Hydro forcing code\n\u2502\n\u2514\u2500\u2500 utils/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 logging.py               # Workflow monitoring\n    \u251c\u2500\u2500 time.py                  # Datetime utilities\n    \u2514\u2500\u2500 workflow.py              # Workflow helper functions\n</code></pre>"},{"location":"DESIGN/#core-components","title":"Core Components","text":""},{"location":"DESIGN/#1-configuration-system-configschemapy","title":"1. Configuration System (<code>config/schema.py</code>)","text":"<p>The new configuration system uses Python <code>dataclasses</code> with full type hints:</p> <pre><code>from dataclasses import dataclass\nfrom typing import Literal\n\nCoastalDomain = Literal[\"prvi\", \"hawaii\", \"atlgulf\", \"pacific\"]\nMeteoSource = Literal[\"nwm_retro\", \"nwm_ana\"]\nBoundarySource = Literal[\"tpxo\", \"stofs\"]\n\n\n@dataclass\nclass SimulationConfig:\n    \"\"\"Simulation time and domain configuration.\"\"\"\n\n    start_date: datetime\n    duration_hours: int\n    coastal_domain: CoastalDomain\n    meteo_source: MeteoSource\n    timestep_seconds: int = 3600\n\n    # Domain mappings as class variables\n    _INLAND_DOMAIN: ClassVar[dict[str, str]] = {\n        \"prvi\": \"domain_puertorico\",\n        \"hawaii\": \"domain_hawaii\",\n        \"atlgulf\": \"domain\",\n        \"pacific\": \"domain\",\n    }\n\n    @property\n    def start_pdy(self) -&gt; str:\n        \"\"\"Return start date as YYYYMMDD string.\"\"\"\n        return self.start_date.strftime(\"%Y%m%d\")\n\n    @property\n    def inland_domain(self) -&gt; str:\n        \"\"\"Inland domain directory name for this coastal domain.\"\"\"\n        return self._INLAND_DOMAIN[self.coastal_domain]\n</code></pre> <p>Benefits:</p> <ul> <li>Type safety: IDE autocompletion, static analysis with <code>pyright</code></li> <li>Self-documenting: Property names and docstrings explain purpose</li> <li>Validation: Runtime checks with helpful error messages</li> <li>DRY: Domain mappings defined once</li> </ul>"},{"location":"DESIGN/#2-yaml-configuration-with-inheritance","title":"2. YAML Configuration with Inheritance","text":"<pre><code># base.yaml - Shared defaults\npaths:\n  nfs_mount: /ngen-test\n\n---\n# hawaii_run.yaml - Inherits from base\n_base: base.yaml\n\nsimulation:\n  start_date: '2023-06-11T00:00:00'\n  duration_hours: 24\n  coastal_domain: hawaii\n  meteo_source: nwm_retro\n\npaths:\n  work_dir: /ngen-test/coastal_runs/${simulation.coastal_domain}\n</code></pre> <p>Features:</p> <ul> <li>Variable interpolation: <code>${section.key}</code> syntax</li> <li>Inheritance: <code>_base</code> field for configuration reuse</li> <li>Deep merging: Override only what changes</li> <li>Smart defaults: Minimal configuration required</li> </ul> <p>When paths are not specified, they are automatically generated using templates that include the <code>${model}</code> variable for model-aware directory naming:</p> <pre><code>DEFAULT_WORK_DIR_TEMPLATE = (\n    \"/ngen-test/coastal/${user}/\"\n    \"${model}_${simulation.coastal_domain}_${boundary.source}_${simulation.meteo_source}/\"\n    \"${model}_${simulation.start_date}\"\n)\n\nDEFAULT_RAW_DOWNLOAD_DIR_TEMPLATE = (\n    \"/ngen-test/coastal/${user}/\"\n    \"${model}_${simulation.coastal_domain}_${boundary.source}_${simulation.meteo_source}/\"\n    \"raw_data\"\n)\n</code></pre> <pre><code>flowchart TD\n    base[base.yaml] --&gt; hawaii[hawaii_run.yaml]\n    base --&gt; pacific[pacific_run.yaml]\n    base --&gt; prvi[prvi_run.yaml]</code></pre>"},{"location":"DESIGN/#3-stage-based-workflow-architecture","title":"3. Stage-Based Workflow Architecture","text":"<p>The stage pipeline is model-specific. Each <code>ModelConfig</code> subclass defines its own <code>stage_order</code> and <code>create_stages()</code>.</p> <p>SCHISM pipeline:</p> <pre><code>flowchart TD\n    A[download] --&gt; B[pre_forcing]\n    B --&gt; C[nwm_forcing]\n    C --&gt; D[post_forcing]\n    D --&gt; E[update_params]\n    E --&gt; F[schism_obs]\n    F --&gt; G[boundary_conditions]\n    G --&gt; H[pre_schism]\n    H --&gt; I[schism_run]\n    I --&gt; J[post_schism]\n    J --&gt; K[schism_plot]</code></pre> <p>SFINCS pipeline:</p> <pre><code>flowchart TD\n    A[download] --&gt; B[sfincs_symlinks]\n    B --&gt; C[sfincs_data_catalog]\n    C --&gt; D[sfincs_init]\n    D --&gt; E[sfincs_timing]\n    E --&gt; F[sfincs_forcing]\n    F --&gt; G[sfincs_obs]\n    G --&gt; H[sfincs_discharge]\n    H --&gt; I[sfincs_precip]\n    I --&gt; J[sfincs_wind]\n    J --&gt; K[sfincs_pressure]\n    K --&gt; L[sfincs_write]\n    L --&gt; M[sfincs_run]</code></pre> <p>Each stage is a Python class inheriting from <code>WorkflowStage</code>:</p> <pre><code>classDiagram\n    class WorkflowStage {\n        &lt;&lt;abstract&gt;&gt;\n        +run() dict\n        +validate() list\n    }\n    WorkflowStage &lt;|-- DownloadStage\n    WorkflowStage &lt;|-- ForcingStage\n    WorkflowStage &lt;|-- BoundaryStage\n    WorkflowStage &lt;|-- SCHISMStage\n    WorkflowStage &lt;|-- SFINCSBuildStage</code></pre> <p>The base class implementation:</p> <pre><code>class WorkflowStage(ABC):\n    \"\"\"Abstract base class for workflow stages.\"\"\"\n\n    name: str = \"base\"\n    description: str = \"Base workflow stage\"\n\n    def __init__(self, config: CoastalCalibConfig, monitor: WorkflowMonitor | None):\n        self.config = config\n        self.monitor = monitor\n\n    def build_environment(self) -&gt; dict[str, str]:\n        \"\"\"Build environment variables for the stage.\"\"\"\n        # Converts config to env vars for bash scripts\n        env = os.environ.copy()\n        env[\"STARTPDY\"] = self.config.simulation.start_pdy\n        env[\"STARTCYC\"] = self.config.simulation.start_cyc\n        # ... all precomputed, no shell date arithmetic needed\n        return env\n\n    def run_singularity_command(\n        self,\n        command: list[str],\n        use_mpi: bool = False,\n        mpi_tasks: int | None = None,\n    ) -&gt; subprocess.CompletedProcess[str]:\n        \"\"\"Run a command inside the Singularity container.\"\"\"\n        # Handles all Singularity setup, bindings, error checking\n        pass\n\n    @abstractmethod\n    def run(self) -&gt; dict[str, Any]:\n        \"\"\"Execute the stage and return results.\"\"\"\n        pass\n\n    def validate(self) -&gt; list[str]:\n        \"\"\"Validate stage prerequisites. Return list of errors.\"\"\"\n        return []\n</code></pre>"},{"location":"DESIGN/#4-workflow-runner-orchestration","title":"4. Workflow Runner Orchestration","text":"<pre><code>class CoastalCalibRunner:\n    \"\"\"Main workflow runner for coastal model calibration.\"\"\"\n\n    @property\n    def STAGE_ORDER(self) -&gt; list[str]:\n        \"\"\"Stage order is delegated to the model config.\"\"\"\n        return self.config.model_config.stage_order\n\n    def run(\n        self,\n        start_from: str | None = None,\n        stop_after: str | None = None,\n        dry_run: bool = False,\n    ) -&gt; WorkflowResult:\n        \"\"\"Execute the calibration workflow.\"\"\"\n        # Validation, stage sequencing, error handling, result collection\n        pass\n</code></pre>"},{"location":"DESIGN/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"DESIGN/#1-python-native-date-arithmetic","title":"1. Python-Native Date Arithmetic","text":"<p>Decision: Replace all bash/Perl date scripts with Python <code>datetime</code>.</p> <p>Rationale:</p> <ul> <li>Python's <code>datetime</code> and <code>timedelta</code> handle all edge cases correctly</li> <li>No external dependencies or shell spawning</li> <li>Type-safe with IDE support</li> </ul> <p>Implementation (<code>utils/time.py</code>):</p> <pre><code>_DATE_RE = re.compile(r\"^\\d{10}$\")\n\n\ndef _parse_date(date_string: str) -&gt; datetime:\n    \"\"\"Parse a YYYYMMDDHH string into a datetime, with strict validation.\"\"\"\n    if not isinstance(date_string, str) or not _DATE_RE.match(date_string):\n        raise ValueError(\n            f\"date_string must be exactly 10 digits in YYYYMMDDHH format, got {date_string!r}\"\n        )\n    return datetime.strptime(date_string, \"%Y%m%d%H\")\n\n\ndef advance_time(date_string: str, hours: int) -&gt; str:\n    \"\"\"Advance a date string by a specified number of hours.\n\n    Replaces advance_time.sh and advance_cymdh.pl with native Python.\n    Handles leap years, month boundaries, DST, etc.\n    \"\"\"\n    dt = _parse_date(date_string) + timedelta(hours=hours)\n    return dt.strftime(\"%Y%m%d%H\")\n</code></pre> <p>The module also consolidates <code>parse_datetime()</code> (flexible datetime parsing, previously duplicated in <code>config.schema</code> and <code>downloader</code>) and <code>iter_hours()</code> (hour-range iteration, previously in <code>downloader</code>).</p> <p>Impact: The <code>build_environment()</code> method precomputes shared date-derived values, then delegates model-specific env vars to <code>model_config.build_environment()</code>:</p> <pre><code># Shared dates computed once in Python, passed to bash scripts\nenv[\"FORCING_BEGIN_DATE\"] = f\"{pdycyc}00\"\nenv[\"FORCING_END_DATE\"] = forcing_end_dt.strftime(\"%Y%m%d%H00\")\nenv[\"END_DATETIME\"] = forcing_end_dt.strftime(\"%Y%m%d%H\")\n\n# Model-specific env vars (e.g., SCHISM_BEGIN_DATE, OMP_NUM_THREADS)\nenv = self.config.model_config.build_environment(env, self.config)\n</code></pre>"},{"location":"DESIGN/#2-integrated-data-downloading-with-validation","title":"2. Integrated Data Downloading with Validation","text":"<p>Decision: Build a comprehensive downloader with source awareness and date range validation.</p> <p>Rationale:</p> <ul> <li>Different data sources have different availability windows</li> <li>Users shouldn't waste time on downloads that will fail</li> <li>Async downloading is faster than sequential</li> </ul> <p>Implementation (<code>downloader.py</code>):</p> <pre><code>DATA_SOURCE_DATE_RANGES: dict[str, dict[str, DateRange]] = {\n    \"nwm_retro\": {\n        \"conus\": DateRange(\n            start=datetime(1979, 2, 1),\n            end=datetime(2023, 1, 31),\n            description=\"NWM Retrospective 3.0 (CONUS)\",\n        ),\n        \"hawaii\": DateRange(\n            start=datetime(1994, 1, 1),\n            end=datetime(2013, 12, 31),\n            description=\"NWM Retrospective 3.0 (Hawaii)\",\n        ),\n        # ...\n    },\n    \"stofs\": {\n        \"_default\": DateRange(\n            start=datetime(2020, 12, 30),\n            end=None,  # operational, no end date\n            description=\"STOFS (operational)\",\n        ),\n    },\n}\n\n\ndef download_data(\n    start_time: datetime,\n    end_time: datetime,\n    output_dir: Path,\n    domain: Domain,\n    meteo_source: MeteoSource = \"nwm_retro\",\n    coastal_source: CoastalSource = \"stofs\",\n) -&gt; DownloadResults:\n    \"\"\"Download with validation and progress tracking.\"\"\"\n    # Validates dates before downloading\n    errors = _validate_date_ranges(start, end, meteo_source, coastal_source, domain)\n    if errors:\n        raise ValueError(\"Date range validation failed:\\n\" + \"\\n\".join(errors))\n\n    # Uses tiny_retriever for async parallel downloads\n    download(urls, paths, timeout=timeout)\n</code></pre>"},{"location":"DESIGN/#3-configuration-over-convention","title":"3. Configuration Over Convention","text":"<p>Decision: Use explicit YAML configuration with sensible defaults.</p> <p>Rationale:</p> <ul> <li>Original relied on implicit conventions (file locations, naming patterns)</li> <li>Explicit configuration is self-documenting</li> <li>Easier to version control and share</li> </ul> <p>Example SCHISM configuration:</p> <pre><code>simulation:\n  start_date: '2023-06-11T00:00:00'\n  duration_hours: 24\n  coastal_domain: pacific\n  meteo_source: nwm_retro\n\nboundary:\n  source: tpxo  # or: source: stofs\n\npaths:\n  work_dir: /ngen-test/coastal_runs/my_run\n  raw_download_dir: /ngen-test/data/downloads\n\n# SCHISM compute parameters (model_config defaults to SchismModelConfig)\nmodel_config:\n  nodes: 2\n  ntasks_per_node: 18\n  nscribes: 2\n  omp_num_threads: 2\n\ndownload:\n  enabled: true\n  skip_existing: true\n</code></pre> <p>Example SFINCS configuration:</p> <pre><code>model: sfincs\n\nsimulation:\n  start_date: 2025-06-01\n  duration_hours: 168\n  coastal_domain: atlgulf\n  meteo_source: nwm_ana\n\nboundary:\n  source: stofs\n\nmodel_config:\n  prebuilt_dir: /path/to/texas/model\n  include_noaa_gages: true\n  forcing_to_mesh_offset_m: 0.0    # STOFS already in mesh datum\n  vdatum_mesh_to_msl_m: 0.171      # mesh datum \u2192 MSL for obs comparison\n  omp_num_threads: 36\n\ndownload:\n  enabled: true\n  skip_existing: true\n</code></pre>"},{"location":"DESIGN/#4-direct-execution-inside-slurm-jobs-run-command","title":"4. Direct Execution Inside SLURM Jobs (<code>run</code> Command)","text":"<p>Decision: Provide a <code>run</code> command for direct, in-process execution inside user-written <code>sbatch</code> scripts.</p> <p>Rationale:</p> <p>Users need full control over SLURM resource allocation\u2014for example when using non-default partitions, requesting specific hardware, or embedding the workflow in a larger pipeline. The <code>run</code> command executes all stages locally on whatever resources are already allocated, making it ideal for use inside manually written <code>sbatch</code> scripts.</p> <p>Usage pattern (preferred on clusters):</p> <p>The recommended approach on clusters is to write an <code>sbatch</code> script that creates a YAML configuration inline using a heredoc and passes it to <code>coastal-calibration run</code>. This is the preferred method because:</p> <ul> <li>The SLURM directives in the <code>sbatch</code> script control resource allocation, while the     YAML controls workflow configuration</li> <li>Everything is contained in a single file that can be submitted with <code>sbatch</code></li> <li>No separate YAML file needs to be managed or kept in sync with SLURM settings</li> <li>The heredoc is self-documenting \u2014 reviewers can see the exact configuration used</li> </ul> <pre><code>#!/usr/bin/env bash\n#SBATCH --job-name=coastal_schism\n#SBATCH --partition=c5n-18xlarge\n#SBATCH -N 2\n#SBATCH --ntasks-per-node=18\n#SBATCH --exclusive\n#SBATCH --output=slurm-%j.out\n\nCONFIG_FILE=\"/tmp/coastal_config_${SLURM_JOB_ID}.yaml\"\n\ncat &gt; \"${CONFIG_FILE}\" &lt;&lt;'EOF'\nmodel: schism\n\nsimulation:\n  start_date: 2021-01-01\n  duration_hours: 12\n  coastal_domain: hawaii\n  meteo_source: nwm_retro\n\nboundary:\n  source: tpxo\n\nmodel_config:\n  include_noaa_gages: true\nEOF\n\n/ngen-test/coastal-calibration/coastal-calibration run \"${CONFIG_FILE}\"\nrm -f \"${CONFIG_FILE}\"\n</code></pre> <p>Design choices:</p> <ul> <li>The config filename includes <code>$SLURM_JOB_ID</code> to avoid collisions when multiple jobs     run concurrently</li> <li>Single-quoted heredoc (<code>&lt;&lt;'EOF'</code>) prevents accidental shell variable expansion inside     the YAML</li> <li>Complete examples for both SCHISM and SFINCS are provided in <code>docs/examples/</code></li> </ul>"},{"location":"DESIGN/#5-stable-public-api-with-incremental-internal-rewrite","title":"5. Stable Public API with Incremental Internal Rewrite","text":"<p>Decision: Establish a clean, stable public API while embedding existing scripts as a transitional measure.</p> <p>Rationale:</p> <p>The primary goal of this rewrite is to create an intuitive, user-friendly, and extensible workflow system. The existing bash and Python scripts are difficult to maintain and not performant. However, rewriting everything at once would:</p> <ul> <li>Delay delivery of a usable tool to users</li> <li>Risk introducing regressions without a baseline</li> <li>Require extensive testing before any release</li> </ul> <p>Strategy:</p> <p>The architecture deliberately separates public API from private implementation:</p> Layer Components Stability Public API <code>CoastalCalibConfig</code>, <code>CoastalCalibRunner</code>, CLI Stable Stage Interface <code>WorkflowStage.run()</code>, <code>.validate()</code>, <code>.build_environment()</code> Stable Private Implementation Bash scripts \u2192 Pure Python Evolving <p>This allows:</p> <ol> <li>Users get a stable interface today - The CLI and Python API won't change as     internals evolve</li> <li>Incremental rewriting - Each stage can be rewritten independently without     affecting others</li> <li>Testing baseline first - Establish test coverage against current behavior before     changes</li> <li>Performance optimization - Replace bash subprocess calls with native Python as     needed</li> </ol> <p>Current State:</p> <ul> <li>Package includes <code>scripts/</code> directory with embedded bash scripts</li> <li><code>WorkflowStage.run_singularity_command()</code> provides abstraction layer</li> <li>Python precomputes all environment variables, minimizing bash complexity</li> </ul> <p>Future Direction:</p> <ol> <li>Add comprehensive integration tests capturing current behavior</li> <li>Incrementally rewrite stages in pure Python (starting with simpler stages)</li> <li>Deprecate bash scripts as Python replacements are validated</li> <li>Optimize performance-critical paths (file I/O, data processing)</li> </ol>"},{"location":"DESIGN/#6-strict-type-checking-with-pyright","title":"6. Strict Type Checking with <code>pyright</code>","text":"<p>Decision: Use strict <code>pyright</code> mode for static type analysis.</p> <p>Rationale:</p> <ul> <li>Catches errors before runtime</li> <li>Enables IDE features (autocomplete, refactoring)</li> <li>Self-documents function signatures</li> </ul> <p>Configuration (<code>pyproject.toml</code>):</p> <pre><code>[tool.pyright]\ntypeCheckingMode = \"strict\"\ninclude = [\"src/coastal_calibration\"]\n</code></pre>"},{"location":"DESIGN/#substantial-improvements","title":"Substantial Improvements","text":""},{"location":"DESIGN/#1-error-handling-and-validation","title":"1. Error Handling and Validation","text":"Aspect Original New Configuration validation None 12+ checks in <code>CoastalCalibConfig.validate()</code> Stage validation None Each stage has <code>validate()</code> method Error messages Exit codes only Detailed, actionable messages Recovery Manual restart Partial workflow execution with <code>--start-from</code> <p>Validation examples:</p> <pre><code>def validate(self) -&gt; list[str]:\n    errors = []\n\n    # Shared validation\n    if self.simulation.duration_hours &lt;= 0:\n        errors.append(\"simulation.duration_hours must be positive\")\n\n    if (\n        self.boundary.source == \"stofs\"\n        and not self.boundary.stofs_file\n        and not self.download.enabled\n    ):\n        errors.append(\n            \"boundary.stofs_file required when using STOFS source and download is disabled\"\n        )\n\n    # Model-specific validation (delegated to ModelConfig subclass)\n    errors.extend(self.model_config.validate(self))\n\n    return errors\n</code></pre>"},{"location":"DESIGN/#2-progress-tracking-and-monitoring","title":"2. Progress Tracking and Monitoring","text":"<p>Original: No progress tracking, just log messages scattered in bash scripts.</p> <p>New: Structured monitoring with stage context:</p> <pre><code>class WorkflowMonitor:\n    \"\"\"Monitors and logs workflow execution progress.\"\"\"\n\n    def register_stages(self, stages: list[str]) -&gt; None:\n        \"\"\"Register stages for progress tracking.\"\"\"\n\n    @contextmanager\n    def stage_context(self, stage_name: str, description: str):\n        \"\"\"Context manager for stage execution with timing.\"\"\"\n        self.info(f\"Starting stage: {stage_name} - {description}\")\n        start = time.perf_counter()\n        try:\n            yield\n            duration = time.perf_counter() - start\n            self.info(f\"Completed stage: {stage_name} in {duration:.1f}s\")\n            self.progress[stage_name] = \"completed\"\n        except Exception as e:\n            self.progress[stage_name] = \"failed\"\n            raise\n\n    def save_progress(self, path: Path) -&gt; None:\n        \"\"\"Save progress to JSON for resumption.\"\"\"\n</code></pre>"},{"location":"DESIGN/#3-cli-with-multiple-entry-points","title":"3. CLI with Multiple Entry Points","text":"<pre><code># Initialize configuration for a domain\ncoastal-calibration init config.yaml --domain hawaii\n\n# Validate configuration\ncoastal-calibration validate config.yaml\n\n# Run workflow (inside an sbatch script or locally)\ncoastal-calibration run config.yaml\n\n# Dry-run to validate without executing\ncoastal-calibration run config.yaml --dry-run\n\n# Run partial workflow\ncoastal-calibration run config.yaml --start-from update_params --stop-after boundary_conditions\n\n# List available stages\ncoastal-calibration stages\n</code></pre>"},{"location":"DESIGN/#4-dual-api-cli-and-programmatic","title":"4. Dual API: CLI and Programmatic","text":"<pre><code># Python API\nfrom coastal_calibration import CoastalCalibConfig, CoastalCalibRunner\n\nconfig = CoastalCalibConfig.from_yaml(\"config.yaml\")\nrunner = CoastalCalibRunner(config)\n\n# Validate first\nerrors = runner.validate()\nif errors:\n    print(\"Validation failed:\", errors)\nelse:\n    result = runner.run()\n    print(f\"Success: {result.success}\")\n</code></pre>"},{"location":"DESIGN/#5-comprehensive-downloader","title":"5. Comprehensive Downloader","text":"Feature Original New Data sources Manual AWS CLI NWM Retro, NWM Ana, STOFS, GLOFS Date validation None Checks against known availability Parallel download None Async with <code>tiny_retriever</code> Skip existing None <code>skip_existing=True</code> option Progress tracking None Success/failure counts Domain awareness Manual Automatic URL building"},{"location":"DESIGN/#6-results-serialization","title":"6. Results Serialization","text":"<pre><code>@dataclass\nclass WorkflowResult:\n    success: bool\n    job_id: str | None\n    start_time: datetime\n    end_time: datetime | None\n    stages_completed: list[str]\n    stages_failed: list[str]\n    outputs: dict[str, Any]\n    errors: list[str]\n\n    @property\n    def duration_seconds(self) -&gt; float | None:\n        if self.end_time:\n            return (self.end_time - self.start_time).total_seconds()\n        return None\n\n    def save(self, path: Path) -&gt; None:\n        \"\"\"Save result to JSON for post-processing.\"\"\"\n</code></pre>"},{"location":"DESIGN/#api-reference","title":"API Reference","text":""},{"location":"DESIGN/#configuration-classes","title":"Configuration Classes","text":"Class Purpose <code>CoastalCalibConfig</code> Root configuration container <code>SimulationConfig</code> Time, domain, and source settings <code>BoundaryConfig</code> TPXO vs STOFS selection <code>PathConfig</code> All file and directory paths <code>ModelConfig</code> ABC for model-specific configuration <code>SchismModelConfig</code> SCHISM compute, MPI, and stage settings <code>SfincsModelConfig</code> SFINCS model paths, OpenMP, and stage settings <code>MonitoringConfig</code> Logging and progress tracking <code>DownloadConfig</code> Data download settings"},{"location":"DESIGN/#schism-workflow-stages","title":"SCHISM Workflow Stages","text":"Stage Class Description <code>download</code> <code>DownloadStage</code> Download NWM/STOFS/GLOFS data <code>pre_forcing</code> <code>PreForcingStage</code> Prepare forcing directories and symlinks <code>nwm_forcing</code> <code>NWMForcingStage</code> Run WRF-Hydro forcing engine (MPI) <code>post_forcing</code> <code>PostForcingStage</code> Post-process forcing files <code>update_params</code> <code>UpdateParamsStage</code> Generate SCHISM <code>param.nml</code> <code>schism_obs</code> <code>SchismObsStage</code> Discover NOAA stations and write <code>station.in</code> <code>boundary_conditions</code> <code>BoundaryConditionStage</code> TPXO or STOFS boundary generation <code>pre_schism</code> <code>PreSCHISMStage</code> Prepare SCHISM inputs <code>schism_run</code> <code>SCHISMRunStage</code> Execute <code>pschism</code> binary (MPI) <code>post_schism</code> <code>PostSCHISMStage</code> Validate and post-process outputs <code>schism_plot</code> <code>SchismPlotStage</code> Plot simulated vs observed water levels (with datum conversion)"},{"location":"DESIGN/#sfincs-workflow-stages","title":"SFINCS Workflow Stages","text":"Stage Class Description <code>download</code> <code>DownloadStage</code> Download NWM/STOFS data <code>sfincs_symlinks</code> <code>SFINCSSymlinksStage</code> Create <code>.nc</code> symlinks for NWM data <code>sfincs_data_catalog</code> <code>SFINCSDataCatalogStage</code> Generate HydroMT data catalog <code>sfincs_init</code> <code>SfincsInitStage</code> Initialize SFINCS model + clean stale files <code>sfincs_timing</code> <code>SfincsTimingStage</code> Set SFINCS timing <code>sfincs_forcing</code> <code>SfincsForcingStage</code> Add water level forcing (IDW interpolation) <code>sfincs_obs</code> <code>SfincsObsStage</code> Add observation points <code>sfincs_discharge</code> <code>SfincsDischargeStage</code> Add discharge sources (active-cell filter) <code>sfincs_precip</code> <code>SfincsPrecipitationStage</code> Add precipitation forcing + clip meteo grid <code>sfincs_wind</code> <code>SfincsWindStage</code> Add wind forcing + clip meteo grid <code>sfincs_pressure</code> <code>SfincsPressureStage</code> Add pressure forcing + clip meteo grid <code>sfincs_write</code> <code>SfincsWriteStage</code> Write SFINCS model <code>sfincs_run</code> <code>SfincsRunStage</code> Run SFINCS (Singularity/OpenMP) <code>sfincs_plot</code> <code>SfincsPlotStage</code> Plot simulated vs observed water levels (with datum conversion)"},{"location":"DESIGN/#potential-future-developments","title":"Potential Future Developments","text":""},{"location":"DESIGN/#vision-unified-workflow-architecture","title":"Vision: Unified Workflow Architecture","text":"<p>The overarching goal is to make the SCHISM and SFINCS workflows architecturally consistent. Every coastal model workflow is conceptually the same four-phase pipeline:</p> <pre><code>Model Creation \u2500\u2500\u25ba Model Preparation \u2500\u2500\u25ba Model Execution \u2500\u2500\u25ba Evaluation\n(mesh, config)   (forcing, boundaries)   (run the solver)   (obs vs sim)\n</code></pre> <p>The SFINCS workflow already follows this pattern cleanly: users provide a pre-built model (<code>prebuilt_dir</code>), the Python pipeline adds forcing/boundaries/observations, then a single container call runs the solver. The SCHISM workflow, by contrast, conflates model creation and preparation inside monolithic bash scripts with hardcoded paths to a pre-built model on the cluster. The future direction is to bring SCHISM in line with SFINCS.</p> <p>The end state is three purpose-built containers, one per concern:</p> Container Purpose Invocation SFINCS SFINCS solver (OpenMP, single-node) <code>singularity run</code> (entrypoint-based) SCHISM SCHISM solver + mesh partitioning (MPI, multi-node) <code>singularity exec</code> (single call) ESMF regridding NWM forcing + STOFS boundary regridding (MPI Python + ESMPy) <code>singularity exec</code> (single call)"},{"location":"DESIGN/#current-state-schism-vs-sfincs-architectural-gap","title":"Current State: SCHISM vs SFINCS Architectural Gap","text":"Aspect SFINCS (target pattern) SCHISM (current) Model input <code>prebuilt_dir</code> (user-provided) Hardcoded paths in <code>/ngwpc-coastal/parm/</code> Model manipulation Python (HydroMT-SFINCS library) Bash scripts inside Singularity Forcing generation Pure Python (<code>xarray</code>, <code>rasterio</code>) MPI Python + bash wrappers (container) Boundary conditions Pure Python (IDW interpolation) Fortran binary (<code>predict_tide</code>) or MPI Python Configuration <code>sfincs.inp</code> read/written by Python <code>param.nml</code> generated by 230-line bash Container usage Single call (<code>singularity run</code>) 9 separate <code>singularity exec</code> calls Pre-run stages 12 stages, 11 pure Python 9 stages, only 2 pure Python Bash dependency 0 bash scripts 15 bash scripts (~1,000 lines) Embedded Python 0 (all in package proper) 6 scripts (~1,100 lines) in <code>scripts/</code> <p>What the current monolithic Singularity container bundles:</p> <ul> <li><code>pschism</code> binary (the SCHISM solver, compiled with MPI + ParMETIS)</li> <li><code>metis_prep</code> and <code>gpmetis</code> binaries (mesh partitioning for parallel execution)</li> <li><code>combine_hotstart7</code> binary (hot-start file post-processing)</li> <li><code>predict_tide</code> (OTPS Fortran binary for TPXO tidal prediction)</li> <li>Conda environments with ESMF-based Python scripts</li> <li>All of the above run via 9 separate <code>singularity exec</code> calls per workflow</li> </ul>"},{"location":"DESIGN/#phase-1-pre-built-schism-model-prebuilt_dir","title":"Phase 1: Pre-Built SCHISM Model (<code>prebuilt_dir</code>)","text":"<p>Goal: Accept a pre-built SCHISM model directory, just like SFINCS.</p> <p>A pre-built SCHISM model directory would contain the mesh and static configuration:</p> <pre><code>prebuilt_dir/\n  hgrid.gr3           # Unstructured mesh (required)\n  hgrid.nc            # Same mesh in NetCDF (required for ESMF regridding)\n  vgrid.in            # Vertical grid specification\n  param.nml.template  # Namelist template (dates/paths filled at runtime)\n  bctides.in          # Tidal boundary setup (optional, for TPXO)\n  station.in          # Observation stations (optional, auto-generated if absent)\n</code></pre> <p>Currently, these files live at hardcoded cluster paths (<code>${parm_dir}/parm/coastal/{domain}/</code>) and are symlinked into the work directory by <code>update_param.bash</code>. The refactoring moves them into a user-provided directory:</p> <p>Changes required:</p> <ol> <li>Add <code>prebuilt_dir: Path</code> to <code>SchismModelConfig</code> (mirrors <code>SfincsModelConfig</code>)</li> <li>Add validation that <code>prebuilt_dir</code> contains required files</li> <li>Replace the symlink logic in <code>update_params</code> bash with a Python init stage that     copies/symlinks from <code>prebuilt_dir</code> to <code>work_dir</code></li> <li>Replace <code>param.nml</code> generation (currently 230 lines of bash in <code>update_param.bash</code>)     with Python using <code>f90nml</code> to read the template and fill runtime values</li> <li>Remove the <code>parm_dir</code> and <code>nwm_dir</code> path dependencies from <code>PathConfig</code></li> </ol> <p>Backward compatibility: The current <code>parm_dir</code>-based paths can be preserved as a fallback: if <code>prebuilt_dir</code> is not set, construct it from <code>${parm_dir}/parm/coastal/{domain}/</code> to maintain cluster compatibility during transition.</p>"},{"location":"DESIGN/#phase-2-pure-python-model-preparation","title":"Phase 2: Pure-Python Model Preparation","text":"<p>Goal: Rewrite all pre-run SCHISM stages in pure Python, eliminating the bash scripts and the need to run preparation stages inside the Singularity container.</p> <p>Stage-by-stage rewrite plan:</p> Stage Current Replacement <code>update_params</code> <code>update_param.bash</code> (230 LOC) <code>f90nml</code> template fill <code>pre_forcing</code> <code>pre_nwm_forcing_coastal.bash</code> <code>pathlib</code> + <code>shutil</code> (symlinks) <code>post_forcing</code> <code>makeAtmo.py</code> (232 LOC) Absorb into package as module <code>boundary (TPXO)</code> <code>predict_tide</code> Fortran binary pure Python TPXO <code>pre_schism</code> <code>pre_schism.bash</code> (56 LOC) <code>makeDischarge.py</code> already Python <code>post_schism</code> <code>post_schism.bash</code> (37 LOC) <code>pathlib</code> + NetCDF4 checks <p>Scripts that can be absorbed immediately (already Python, just need to move out of <code>scripts/</code> into proper package modules):</p> <ul> <li><code>makeAtmo.py</code> (232 LOC) - atmospheric post-processing</li> <li><code>makeDischarge.py</code> (139 LOC) - discharge source generation</li> <li><code>merge_source_sink.py</code> (166 LOC) - source/sink merging</li> <li><code>correct_elevation.py</code> (32 LOC) - elevation correction</li> <li><code>otps_to_open_bnds_hgrid.py</code> (108 LOC) - TPXO output parsing</li> </ul> <p>These 5 Python files total ~677 lines and are already functional Python code that just needs to be brought under the package's type checking, testing, and import system.</p>"},{"location":"DESIGN/#phase-3-three-purpose-built-containers","title":"Phase 3: Three Purpose-Built Containers","text":"<p>Goal: Replace the current monolithic container with three focused containers, each with a single responsibility.</p>"},{"location":"DESIGN/#sfincs-container-already-exists","title":"SFINCS Container (already exists)","text":"<p>The SFINCS container already follows the target pattern. It contains only the SFINCS solver binary with OpenMP support and is invoked via a single <code>singularity run</code> call with the model directory bind-mounted at <code>/data</code>.</p>"},{"location":"DESIGN/#schism-container-new-solver-only","title":"SCHISM Container (new, solver only)","text":"<p>A minimal container with only the binaries needed to run and partition a SCHISM model:</p> <ul> <li><code>pschism</code> (the SCHISM solver, compiled with MPI + NetCDF)</li> <li><code>metis_prep</code> + <code>gpmetis</code> (mesh partitioning for parallel execution)</li> <li><code>combine_hotstart7</code> (combines distributed hot-start files after a run)</li> <li>OpenMPI runtime and InfiniBand/network libraries for multi-node MPI</li> <li>HDF5/NetCDF4 Fortran libraries</li> </ul> <p>This container is invoked twice: once for mesh partitioning (<code>metis_prep</code> + <code>gpmetis</code>) and once for the solver (<code>mpiexec pschism</code>). Both are simple <code>singularity exec</code> calls.</p> <p>Removed from container (moved to host-side Python):</p> <ul> <li>All bash wrapper scripts</li> <li>Conda environments</li> <li><code>predict_tide</code> (replaced by pure Python TPXO)</li> <li>NWM USH/EXEC scripts</li> </ul>"},{"location":"DESIGN/#esmf-regridding-container-new-mpi-python-esmpy","title":"ESMF Regridding Container (new, MPI Python + ESMPy)","text":"<p>The NWM forcing engine (<code>workflow_driver.py</code>) and STOFS boundary regridding (<code>regrid_estofs.py</code>) both depend on ESMPy (<code>import ESMF</code>), which is the Python interface to the ESMF (Earth System Modeling Framework) regridding library. ESMPy itself requires MPI and performs parallel regridding of NWM meteorological fields (wind, pressure, precipitation) and STOFS water levels onto the SCHISM unstructured mesh.</p> <p>These ESMF dependencies are heavyweight (MPI-aware C/Fortran libraries + Python bindings) and do not belong in either the SCHISM solver container or the host Python environment. A dedicated ESMF container isolates this concern:</p> <ul> <li>ESMPy (<code>ESMF</code> Python module) with MPI support</li> <li><code>workflow_driver.py</code> - regrids NWM forcing fields to SCHISM mesh via ESMF</li> <li><code>regrid_estofs.py</code> - regrids STOFS water levels to SCHISM open boundaries via ESMF</li> <li>Python scientific stack (<code>numpy</code>, <code>netCDF4</code>, <code>xarray</code>)</li> <li>OpenMPI runtime matching the cluster</li> </ul> <p>This container is invoked via <code>singularity exec</code> with <code>mpiexec</code> for the two ESMF-based stages (<code>nwm_forcing</code> and STOFS <code>boundary_conditions</code>). This is the last container to address since both scripts are already functional MPI Python programs.</p>"},{"location":"DESIGN/#target-architecture","title":"Target Architecture","text":"<pre><code>Host (Python)                    Containers\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndownload          \u2500\u2500\u2500 pure Python (no container)\npre_forcing       \u2500\u2500\u2500 pure Python (no container)\nnwm_forcing       \u2500\u2500\u2500 ESMF container (mpiexec + workflow_driver.py)\npost_forcing      \u2500\u2500\u2500 pure Python (no container)\nupdate_params     \u2500\u2500\u2500 pure Python (no container)\nschism_obs        \u2500\u2500\u2500 pure Python (no container)\nboundary (TPXO)   \u2500\u2500\u2500 pure Python (no container)\nboundary (STOFS)  \u2500\u2500\u2500 ESMF container (mpiexec + regrid_estofs.py)\npre_schism        \u2500\u2500\u2500 SCHISM container (metis_prep + gpmetis)\nschism_run        \u2500\u2500\u2500 SCHISM container (mpiexec + pschism)\npost_schism       \u2500\u2500\u2500 pure Python (no container)\nschism_plot       \u2500\u2500\u2500 pure Python (no container)\n</code></pre> <p>Bind mounts simplified from 15+ to 2-3 per container (work_dir + MPI libs).</p>"},{"location":"DESIGN/#phase-4-evaluation-and-visualization","title":"Phase 4: Evaluation and Visualization","text":"<p>Goal: Expand the existing <code>schism_plot</code> and <code>sfincs_plot</code> stages into a unified evaluation framework.</p> <p>Both <code>SchismPlotStage</code> and <code>SfincsPlotStage</code> are already pure Python, query NOAA CO-OPS observations, and generate comparison plots. Future enhancements:</p> <ol> <li>Unified <code>EvaluationStage</code> base class for both models</li> <li>Statistical metrics (RMSE, bias, correlation, skill scores)</li> <li>Multi-station summary dashboards</li> <li>Time series export (CSV/Parquet) for downstream analysis</li> </ol>"},{"location":"DESIGN/#near-term-priorities","title":"Near-Term Priorities","text":"<ol> <li> <p>Pure-Python TPXO - Replaces the <code>predict_tide</code> Fortran binary and eliminates the     SFINCS workflow's only Singularity dependency for boundary conditions.</p> </li> <li> <p>Absorb embedded Python scripts - The 5 Python files in <code>scripts/</code> (677 LOC) are     already functional Python. Moving them into the package proper brings them under     type checking, testing, and import hygiene with minimal risk.</p> </li> <li> <p><code>f90nml</code>-based <code>param.nml</code> generation - Replaces the largest bash script     (<code>update_param.bash</code>, 230 LOC) and unblocks the <code>prebuilt_dir</code> pattern.</p> </li> </ol>"},{"location":"DESIGN/#feature-expansion","title":"Feature Expansion","text":"<ol> <li> <p>Hot Start Chain Automation</p> <ul> <li>Automatic hot-start file discovery</li> <li>Multi-run chaining for long simulations</li> </ul> </li> <li> <p>Ensemble Runs</p> <ul> <li>Multiple configurations from single base</li> <li>Parallel SLURM array jobs</li> </ul> </li> <li> <p>Cloud-Native Deployment</p> <ul> <li>AWS Batch support</li> <li>Container-native execution (no Singularity)</li> </ul> </li> </ol>"},{"location":"DESIGN/#conclusion","title":"Conclusion","text":"<p>The <code>coastal-calibration</code> package represents a substantial modernization of the original bash-based workflow:</p> Metric Original New Improvement Lines of bash ~2,500 ~500 (embedded) 80% reduction Lines of Python ~200 (scattered) ~4,000 (structured) Full rewrite Configuration Environment variables Typed YAML Type-safe Error handling Exit codes Exceptions + validation Comprehensive Testing None <code>pytest</code> + <code>pyright</code> CI-ready Documentation Comments only Docstrings + types Self-documenting Extensibility Copy &amp; modify scripts Inherit <code>WorkflowStage</code> Object-oriented Model support SCHISM only SCHISM + SFINCS Polymorphic <p>The architecture is designed for maintainability, extensibility, and correctness while supporting multiple coastal models (SCHISM and SFINCS) through a polymorphic <code>ModelConfig</code> ABC and preserving compatibility with the existing HPC infrastructure.</p>"}]}